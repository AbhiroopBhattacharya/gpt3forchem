{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Helper functions that make it easier to use the OpenAI API\n",
    "output-file: api_wrappers.html\n",
    "title: API wrappers\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L32){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_ft_model_name\n",
       "\n",
       ">      get_ft_model_name (ft_id, sleep=60)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L32){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_ft_model_name\n",
       "\n",
       ">      get_ft_model_name (ft_id, sleep=60)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(get_ft_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ada:ft-lsmoepfl-2022-08-18-00-13-00'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ft_model_name('ft-jK0ziTZX6y5d2DXbB3kdct4w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ada:ft-lsmoepfl-2022-09-02-14-15-28'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ft_model_name('ft-f5xtILIGM6yjvLrj0J1GH5FQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L40){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### fine_tune\n",
       "\n",
       ">      fine_tune (train_file, valid_file, model:str='ada', n_epochs:int=4,\n",
       ">                 sleep:int=120)\n",
       "\n",
       "Run the fine tuning of a GPT-3 model via the OpenAI API.\n",
       "\n",
       "There is some logic here to wait until the fine-tuning task is complete.\n",
       "Often, the job might end up in the queue and we do not have the model id yet. \n",
       "In this case, we will ask for the status of the job regularly and wait until it is complete.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| train_file |  |  | path to json file with training prompts (column names \"prompt\" and \"completion\") |\n",
       "| valid_file |  |  | path to json file with validation prompts (column names \"prompt\" and \"completion\") |\n",
       "| model | str | ada | model type to use. One of \"ada\", \"babbage\", \"curie\", \"davinci\". \"ada\" is the default (and cheapest). |\n",
       "| n_epochs | int | 4 | number of epochs to fine-tune for |\n",
       "| sleep | int | 120 | number of seconds to wait between checking the status of the fine-tuning task |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L40){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### fine_tune\n",
       "\n",
       ">      fine_tune (train_file, valid_file, model:str='ada', n_epochs:int=4,\n",
       ">                 sleep:int=120)\n",
       "\n",
       "Run the fine tuning of a GPT-3 model via the OpenAI API.\n",
       "\n",
       "There is some logic here to wait until the fine-tuning task is complete.\n",
       "Often, the job might end up in the queue and we do not have the model id yet. \n",
       "In this case, we will ask for the status of the job regularly and wait until it is complete.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| train_file |  |  | path to json file with training prompts (column names \"prompt\" and \"completion\") |\n",
       "| valid_file |  |  | path to json file with validation prompts (column names \"prompt\" and \"completion\") |\n",
       "| model | str | ada | model type to use. One of \"ada\", \"babbage\", \"curie\", \"davinci\". \"ada\" is the default (and cheapest). |\n",
       "| n_epochs | int | 4 | number of epochs to fine-tune for |\n",
       "| sleep | int | 120 | number of seconds to wait between checking the status of the fine-tuning task |"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(fine_tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helpers to make it easiers to get completions from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L77){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### query_gpt3\n",
       "\n",
       ">      query_gpt3 (model:str, df:pandas.core.frame.DataFrame,\n",
       ">                  temperature:float=0, max_tokens:int=10, sleep:float=5,\n",
       ">                  one_by_one:bool=False, parallel_max:int=20)\n",
       "\n",
       "Get completions for all prompts in a dataframe.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| model | str |  | name of the model to use, e.g. \"ada:ft-personal-2022-08-24-10-41-29\" |\n",
       "| df | DataFrame |  | hashable dataframe with prompts and expected completions (column names \"prompt\" and \"completion\") |\n",
       "| temperature | float | 0 | temperature, 0 is the default and corresponds to argmax |\n",
       "| max_tokens | int | 10 | maximum number of tokens to generate |\n",
       "| sleep | float | 5 | number of seconds to wait between queries |\n",
       "| one_by_one | bool | False | if True, generate one completion at a time (i.e., due to submit the maximum number of prompts per request) |\n",
       "| parallel_max | int | 20 | maximum number of prompts that can be sent per request |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L77){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### query_gpt3\n",
       "\n",
       ">      query_gpt3 (model:str, df:pandas.core.frame.DataFrame,\n",
       ">                  temperature:float=0, max_tokens:int=10, sleep:float=5,\n",
       ">                  one_by_one:bool=False, parallel_max:int=20)\n",
       "\n",
       "Get completions for all prompts in a dataframe.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| model | str |  | name of the model to use, e.g. \"ada:ft-personal-2022-08-24-10-41-29\" |\n",
       "| df | DataFrame |  | hashable dataframe with prompts and expected completions (column names \"prompt\" and \"completion\") |\n",
       "| temperature | float | 0 | temperature, 0 is the default and corresponds to argmax |\n",
       "| max_tokens | int | 10 | maximum number of tokens to generate |\n",
       "| sleep | float | 5 | number of seconds to wait between queries |\n",
       "| one_by_one | bool | False | if True, generate one completion at a time (i.e., due to submit the maximum number of prompts per request) |\n",
       "| parallel_max | int | 20 | maximum number of prompts that can be sent per request |"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(query_gpt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L130){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### extract_prediction\n",
       "\n",
       ">      extract_prediction (completion, i:int=0)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| completion |  |  | dictionary with \"choices\" key returned by the API |\n",
       "| i | int | 0 | index of the \"choice\" (relevant if multiple completions have been returned) |\n",
       "| **Returns** | **str** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L130){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### extract_prediction\n",
       "\n",
       ">      extract_prediction (completion, i:int=0)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| completion |  |  | dictionary with \"choices\" key returned by the API |\n",
       "| i | int | 0 | index of the \"choice\" (relevant if multiple completions have been returned) |\n",
       "| **Returns** | **str** |  |  |"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(extract_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "example_pred = {\n",
    "    \"choices\": [{\"finish_reason\": \"length\", \"index\": 0, \"text\": \" 0@@@@@@@\"}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_prediction(example_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L138){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### extract_regression_prediction\n",
       "\n",
       ">      extract_regression_prediction (completion, i:int=0)\n",
       "\n",
       "Similar to `extract_prediction`, but returns a float.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| completion |  |  | dictionary with \"choices\" key returned by the API |\n",
       "| i | int | 0 | index of the \"choice\" (relevant if multiple completions have been returned) |\n",
       "| **Returns** | **float** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L138){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### extract_regression_prediction\n",
       "\n",
       ">      extract_regression_prediction (completion, i:int=0)\n",
       "\n",
       "Similar to `extract_prediction`, but returns a float.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| completion |  |  | dictionary with \"choices\" key returned by the API |\n",
       "| i | int | 0 | index of the \"choice\" (relevant if multiple completions have been returned) |\n",
       "| **Returns** | **float** |  |  |"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(extract_regression_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "example_pred = {\n",
    "    \"choices\": [{\"finish_reason\": \"length\", \"index\": 0, \"text\": \" -8.2@@@@@@@\"}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_regression_prediction(example_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L150){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### extract_inverse_prediction\n",
       "\n",
       ">      extract_inverse_prediction (completion, i=0)\n",
       "\n",
       "Extracts the prediction of a molecule/material generative task."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L150){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### extract_inverse_prediction\n",
       "\n",
       ">      extract_inverse_prediction (completion, i=0)\n",
       "\n",
       "Extracts the prediction of a molecule/material generative task."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(extract_inverse_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC1=C(C(C)=NN1)/N=N/C2=CC=C(C(F)(F)F)C=C2',\n",
       " 'CC(C=C(N(CCC#N)CCO)C=C1)=C1/N=N/C2=CC=CC=C2',\n",
       " 'CC(C=C(N(CCC#N)CCO)C=C1)=C1/N=N/C2=CC=CC=C2']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_inverse_predictions = {\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"finish_reason\": \"length\",\n",
    "            \"index\": 0,\n",
    "            \"text\": \"CC1=C(C(C)=NN1)/N=N/C2=CC=C(C(F)(F)F)C=C2@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\",\n",
    "        },\n",
    "        {\n",
    "            \"finish_reason\": \"length\",\n",
    "            \"index\": 1,\n",
    "            \"text\": \"CC(C=C(N(CCC#N)CCO)C=C1)=C1/N=N/C2=CC=CC=C2@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\",\n",
    "        },\n",
    "        {\n",
    "            \"finish_reason\": \"length\",\n",
    "            \"index\": 2,\n",
    "            \"text\": \"CC(C=C(N(CCC#N)CCO)C=C1)=C1/N=N/C2=CC=CC=C2@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\",\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "[extract_inverse_prediction(example_inverse_predictions, i) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L158){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### train_test_loop\n",
       "\n",
       ">      train_test_loop (df:pandas.core.frame.DataFrame, train_size:int,\n",
       ">                       prompt_create_fn:<built-infunctioncallable>,\n",
       ">                       random_state:int, stratify:Optional[str]=None,\n",
       ">                       test_subset:Optional[int]=None)\n",
       "\n",
       "Run the full training and testing process for the classification task.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DataFrame |  | dataframe with prompts and expected completions (column names \"prompt\" and \"completion\"). Split will be performed within this function. |\n",
       "| train_size | int |  | number of rows to use for training |\n",
       "| prompt_create_fn | callable |  | function to create a prompt from a row of the dataframe |\n",
       "| random_state | int |  | random state for splitting the dataframe |\n",
       "| stratify | typing.Optional[str] | None | column name to use for stratification |\n",
       "| test_subset | typing.Optional[int] | None |  |\n",
       "| **Returns** | **dict** |  | **number of rows to use for testing. If None, use the remainder of the dataframe.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L158){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### train_test_loop\n",
       "\n",
       ">      train_test_loop (df:pandas.core.frame.DataFrame, train_size:int,\n",
       ">                       prompt_create_fn:<built-infunctioncallable>,\n",
       ">                       random_state:int, stratify:Optional[str]=None,\n",
       ">                       test_subset:Optional[int]=None)\n",
       "\n",
       "Run the full training and testing process for the classification task.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DataFrame |  | dataframe with prompts and expected completions (column names \"prompt\" and \"completion\"). Split will be performed within this function. |\n",
       "| train_size | int |  | number of rows to use for training |\n",
       "| prompt_create_fn | callable |  | function to create a prompt from a row of the dataframe |\n",
       "| random_state | int |  | random state for splitting the dataframe |\n",
       "| stratify | typing.Optional[str] | None | column name to use for stratification |\n",
       "| test_subset | typing.Optional[int] | None |  |\n",
       "| **Returns** | **dict** |  | **number of rows to use for testing. If None, use the remainder of the dataframe.** |"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(train_test_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Deep ensembles](https://cims.nyu.edu/~andrewgw/deepensembles/) are a powerful technique to make neural networks \"Bayesian\". It can make them more robust and also be used to obtain uncertainty estimates.\n",
    "\n",
    "Typically, they rely on the fact that there is some inherent randomness in training of a model due to the random intialization. However, when we fine-tune a model, we always start from the same weights. Hence we anticipate that we'll need to sample the data to achieve enough randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L218){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multiple_fine_tunes\n",
       "\n",
       ">      multiple_fine_tunes (train_files, valid_files)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L218){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multiple_fine_tunes\n",
       "\n",
       ">      multiple_fine_tunes (train_files, valid_files)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(multiple_fine_tunes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L227){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ensemble_fine_tune\n",
       "\n",
       ">      ensemble_fine_tune (train_frame, valid_frame, num_models:int=10,\n",
       ">                          subsample:float=0.8, run_file_dir:str='run_files',\n",
       ">                          filename_base_string:str='')"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L227){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ensemble_fine_tune\n",
       "\n",
       ">      ensemble_fine_tune (train_frame, valid_frame, num_models:int=10,\n",
       ">                          subsample:float=0.8, run_file_dir:str='run_files',\n",
       ">                          filename_base_string:str='')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(ensemble_fine_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L261){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multiple_query_gpt3\n",
       "\n",
       ">      multiple_query_gpt3 (models:List[str], df:pandas.core.frame.DataFrame,\n",
       ">                           temperature:float=0, max_tokens:int=10,\n",
       ">                           sleep:float=5, one_by_one:bool=False,\n",
       ">                           parallel_max:int=20)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| models | typing.List[str] |  | names of the models to use, e.g. \"ada:ft-personal-2022-08-24-10-41-29\" |\n",
       "| df | DataFrame |  | dataframe with prompts and expected completions (column names \"prompt\" and \"completion\") |\n",
       "| temperature | float | 0 | temperature, 0 is the default and corresponds to argmax |\n",
       "| max_tokens | int | 10 | maximum number of tokens to generate |\n",
       "| sleep | float | 5 | number of seconds to wait between queries |\n",
       "| one_by_one | bool | False | if True, generate one completion at a time (i.e., due to submit the maximum number of prompts per request) |\n",
       "| parallel_max | int | 20 | maximum number of prompts that can be sent per request |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kjappelbaum/gpt3forchem/blob/main/gpt3forchem/api_wrappers.py#L261){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multiple_query_gpt3\n",
       "\n",
       ">      multiple_query_gpt3 (models:List[str], df:pandas.core.frame.DataFrame,\n",
       ">                           temperature:float=0, max_tokens:int=10,\n",
       ">                           sleep:float=5, one_by_one:bool=False,\n",
       ">                           parallel_max:int=20)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| models | typing.List[str] |  | names of the models to use, e.g. \"ada:ft-personal-2022-08-24-10-41-29\" |\n",
       "| df | DataFrame |  | dataframe with prompts and expected completions (column names \"prompt\" and \"completion\") |\n",
       "| temperature | float | 0 | temperature, 0 is the default and corresponds to argmax |\n",
       "| max_tokens | int | 10 | maximum number of tokens to generate |\n",
       "| sleep | float | 5 | number of seconds to wait between queries |\n",
       "| one_by_one | bool | False | if True, generate one completion at a time (i.e., due to submit the maximum number of prompts per request) |\n",
       "| parallel_max | int | 20 | maximum number of prompts that can be sent per request |"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(multiple_query_gpt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not useful/not used as the OpenAI API currently does not allow to retrieve the internals of custom models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gpt3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a4fa60962de90e73b5da8d67a44b01d2de04630d82b94b8db1f727a73d31e61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
