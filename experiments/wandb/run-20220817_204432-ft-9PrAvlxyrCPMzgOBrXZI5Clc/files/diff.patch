diff --git a/experiments/01_polymer_forward.ipynb b/experiments/01_polymer_forward.ipynb
index ab8ac01..0497381 100644
--- a/experiments/01_polymer_forward.ipynb
+++ b/experiments/01_polymer_forward.ipynb
@@ -1,5 +1,294 @@
 {
  "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Forward predictions of polymer adsorption energies\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 23,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import time\n",
+    "\n",
+    "from sklearn.model_selection import train_test_split\n",
+    "\n",
+    "from gpt3forchem.data import get_polymer_data\n",
+    "from gpt3forchem.input import create_single_property_forward_prompts\n",
+    "from gpt3forchem.api_wrappers import fine_tune, query_gpt3, extract_prediction\n",
+    "\n",
+    "from pycm import ConfusionMatrix"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Let's run one fine-tuning and inference for sanity check and then do it a coule of times for statistics.\n"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Sanity check\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 26,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "df = get_polymer_data()\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 27,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "df_train, df_test = train_test_split(df, train_size=200, random_state=42)\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 28,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "train_prompts = create_single_property_forward_prompts(\n",
+    "    df_train, \"deltaGmin_cat\", {\"deltaGmin_cat\": \"adsorption energy\"}\n",
+    ")\n",
+    "\n",
+    "test_prompts = create_single_property_forward_prompts(\n",
+    "    df_test, \"deltaGmin_cat\", {\"deltaGmin_cat\": \"adsorption energy\"}\n",
+    ")\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 29,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "train_size  = len(train_prompts)\n",
+    "test_size = len(test_prompts)\n",
+    "\n",
+    "filename_base = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
+    "train_filename = f\"run_files/{filename_base}_train_prompts_polymers_{train_size}.jsonl\"\n",
+    "valid_filename = f\"run_files/{filename_base}_valid_prompts_polymers_{test_size}.jsonl\"\n",
+    "\n",
+    "train_prompts.to_json(train_filename, orient=\"records\", lines=True)\n",
+    "test_prompts.to_json(valid_filename, orient=\"records\", lines=True)\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 30,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "modelname = fine_tune(train_filename, valid_filename)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "test_prompt_subset = test_prompts.iloc[:100]\n",
+    "completions = query_gpt3(modelname, test_prompt_subset)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "predictions = [extract_prediction(completion) for completion in completions]\n",
+    "true = [t.split('@')[0] for t in test_prompt_subset['completion']]"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "cm = ConfusionMatrix(true, predictions)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Predict   0        2        3        4       3        \n",
+      "Actual\n",
+      " 0       0        0        0        0        4        \n",
+      "\n",
+      " 2       0        0        0        0        1        \n",
+      "\n",
+      " 3       0        0        0        0        1        \n",
+      "\n",
+      " 4       0        0        0        0        4        \n",
+      "\n",
+      "3        0        0        0        0        0        \n",
+      "\n",
+      "\n",
+      "\n",
+      "\n",
+      "\n",
+      "Overall Statistics : \n",
+      "\n",
+      "95% CI                                                            (0.0,0.0)\n",
+      "ACC Macro                                                         0.6\n",
+      "ARI                                                               0.0\n",
+      "AUNP                                                              None\n",
+      "AUNU                                                              None\n",
+      "Bangdiwala B                                                      None\n",
+      "Bennett S                                                         -0.25\n",
+      "CBA                                                               0.0\n",
+      "CSI                                                               None\n",
+      "Chi-Squared                                                       None\n",
+      "Chi-Squared DF                                                    16\n",
+      "Conditional Entropy                                               -0.0\n",
+      "Cramer V                                                          None\n",
+      "Cross Entropy                                                     0\n",
+      "F1 Macro                                                          0.0\n",
+      "F1 Micro                                                          0.0\n",
+      "FNR Macro                                                         None\n",
+      "FNR Micro                                                         1.0\n",
+      "FPR Macro                                                         0.2\n",
+      "FPR Micro                                                         0.25\n",
+      "Gwet AC1                                                          -0.1994\n",
+      "Hamming Loss                                                      1.0\n",
+      "Joint Entropy                                                     1.72193\n",
+      "KL Divergence                                                     None\n",
+      "Kappa                                                             0.0\n",
+      "Kappa 95% CI                                                      (0.0,0.0)\n",
+      "Kappa No Prevalence                                               -1.0\n",
+      "Kappa Standard Error                                              0.0\n",
+      "Kappa Unbiased                                                    -0.50376\n",
+      "Krippendorff Alpha                                                -0.42857\n",
+      "Lambda A                                                          0.0\n",
+      "Lambda B                                                          None\n",
+      "Mutual Information                                                0.0\n",
+      "NIR                                                               0.4\n",
+      "Overall ACC                                                       0.0\n",
+      "Overall CEN                                                       0.28699\n",
+      "Overall J                                                         (0.0,0.0)\n",
+      "Overall MCC                                                       None\n",
+      "Overall MCEN                                                      0.28699\n",
+      "Overall RACC                                                      0.0\n",
+      "Overall RACCU                                                     0.335\n",
+      "P-Value                                                           1\n",
+      "PPV Macro                                                         None\n",
+      "PPV Micro                                                         0.0\n",
+      "Pearson C                                                         None\n",
+      "Phi-Squared                                                       None\n",
+      "RCI                                                               0.0\n",
+      "RR                                                                2.0\n",
+      "Reference Entropy                                                 1.72193\n",
+      "Response Entropy                                                  -0.0\n",
+      "SOA1(Landis & Koch)                                               Slight\n",
+      "SOA2(Fleiss)                                                      Poor\n",
+      "SOA3(Altman)                                                      Poor\n",
+      "SOA4(Cicchetti)                                                   Poor\n",
+      "SOA5(Cramer)                                                      None\n",
+      "SOA6(Matthews)                                                    None\n",
+      "Scott PI                                                          -0.50376\n",
+      "Standard Error                                                    0.0\n",
+      "TNR Macro                                                         0.8\n",
+      "TNR Micro                                                         0.75\n",
+      "TPR Macro                                                         None\n",
+      "TPR Micro                                                         0.0\n",
+      "Zero-one Loss                                                     10\n",
+      "\n",
+      "Class Statistics :\n",
+      "\n",
+      "Classes                                                            0             2             3             4            3             \n",
+      "ACC(Accuracy)                                                     0.6           0.9           0.9           0.6           0.0           \n",
+      "AGF(Adjusted F-score)                                             0.0           0.0           0.0           0.0           0.0           \n",
+      "AGM(Adjusted geometric mean)                                      0             0             0             0             None          \n",
+      "AM(Difference between automatic and manual classification)        -4            -1            -1            -4            10            \n",
+      "AUC(Area under the ROC curve)                                     0.5           0.5           0.5           0.5           None          \n",
+      "AUCI(AUC value interpretation)                                    Poor          Poor          Poor          Poor          None          \n",
+      "AUPR(Area under the PR curve)                                     None          None          None          None          None          \n",
+      "BCD(Bray-Curtis dissimilarity)                                    0.2           0.05          0.05          0.2           0.5           \n",
+      "BM(Informedness or bookmaker informedness)                        0.0           0.0           0.0           0.0           None          \n",
+      "CEN(Confusion entropy)                                            0.0           0.0           0.0           0.0           0.57398       \n",
+      "DOR(Diagnostic odds ratio)                                        None          None          None          None          None          \n",
+      "DP(Discriminant power)                                            None          None          None          None          None          \n",
+      "DPI(Discriminant power interpretation)                            None          None          None          None          None          \n",
+      "ERR(Error rate)                                                   0.4           0.1           0.1           0.4           1.0           \n",
+      "F0.5(F0.5 score)                                                  0.0           0.0           0.0           0.0           0.0           \n",
+      "F1(F1 score - harmonic mean of precision and sensitivity)         0.0           0.0           0.0           0.0           0.0           \n",
+      "F2(F2 score)                                                      0.0           0.0           0.0           0.0           0.0           \n",
+      "FDR(False discovery rate)                                         None          None          None          None          1.0           \n",
+      "FN(False negative/miss/type 2 error)                              4             1             1             4             0             \n",
+      "FNR(Miss rate or false negative rate)                             1.0           1.0           1.0           1.0           None          \n",
+      "FOR(False omission rate)                                          0.4           0.1           0.1           0.4           None          \n",
+      "FP(False positive/type 1 error/false alarm)                       0             0             0             0             10            \n",
+      "FPR(Fall-out or false positive rate)                              0.0           0.0           0.0           0.0           1.0           \n",
+      "G(G-measure geometric mean of precision and sensitivity)          None          None          None          None          None          \n",
+      "GI(Gini index)                                                    0.0           0.0           0.0           0.0           None          \n",
+      "GM(G-mean geometric mean of specificity and sensitivity)          0.0           0.0           0.0           0.0           None          \n",
+      "IBA(Index of balanced accuracy)                                   0.0           0.0           0.0           0.0           None          \n",
+      "ICSI(Individual classification success index)                     None          None          None          None          None          \n",
+      "IS(Information score)                                             None          None          None          None          None          \n",
+      "J(Jaccard index)                                                  0.0           0.0           0.0           0.0           0.0           \n",
+      "LS(Lift score)                                                    None          None          None          None          None          \n",
+      "MCC(Matthews correlation coefficient)                             None          None          None          None          None          \n",
+      "MCCI(Matthews correlation coefficient interpretation)             None          None          None          None          None          \n",
+      "MCEN(Modified confusion entropy)                                  0.0           0.0           0.0           0.0           0.57398       \n",
+      "MK(Markedness)                                                    None          None          None          None          None          \n",
+      "N(Condition negative)                                             6             9             9             6             10            \n",
+      "NLR(Negative likelihood ratio)                                    1.0           1.0           1.0           1.0           None          \n",
+      "NLRI(Negative likelihood ratio interpretation)                    Negligible    Negligible    Negligible    Negligible    None          \n",
+      "NPV(Negative predictive value)                                    0.6           0.9           0.9           0.6           None          \n",
+      "OC(Overlap coefficient)                                           None          None          None          None          None          \n",
+      "OOC(Otsuka-Ochiai coefficient)                                    None          None          None          None          None          \n",
+      "OP(Optimized precision)                                           -0.4          -0.1          -0.1          -0.4          None          \n",
+      "P(Condition positive or support)                                  4             1             1             4             0             \n",
+      "PLR(Positive likelihood ratio)                                    None          None          None          None          None          \n",
+      "PLRI(Positive likelihood ratio interpretation)                    None          None          None          None          None          \n",
+      "POP(Population)                                                   10            10            10            10            10            \n",
+      "PPV(Precision or positive predictive value)                       None          None          None          None          0.0           \n",
+      "PRE(Prevalence)                                                   0.4           0.1           0.1           0.4           0.0           \n",
+      "Q(Yule Q - coefficient of colligation)                            None          None          None          None          None          \n",
+      "QI(Yule Q interpretation)                                         None          None          None          None          None          \n",
+      "RACC(Random accuracy)                                             0.0           0.0           0.0           0.0           0.0           \n",
+      "RACCU(Random accuracy unbiased)                                   0.04          0.0025        0.0025        0.04          0.25          \n",
+      "TN(True negative/correct rejection)                               6             9             9             6             0             \n",
+      "TNR(Specificity or true negative rate)                            1.0           1.0           1.0           1.0           0.0           \n",
+      "TON(Test outcome negative)                                        10            10            10            10            0             \n",
+      "TOP(Test outcome positive)                                        0             0             0             0             10            \n",
+      "TP(True positive/hit)                                             0             0             0             0             0             \n",
+      "TPR(Sensitivity, recall, hit rate, or true positive rate)         0.0           0.0           0.0           0.0           None          \n",
+      "Y(Youden index)                                                   0.0           0.0           0.0           0.0           None          \n",
+      "dInd(Distance index)                                              1.0           1.0           1.0           1.0           None          \n",
+      "sInd(Similarity index)                                            0.29289       0.29289       0.29289       0.29289       None          \n",
+      "\n"
+     ]
+    }
+   ],
+   "source": [
+    "print(cm)"
+   ]
+  },
   {
    "cell_type": "code",
    "execution_count": null,
@@ -10,18 +299,26 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "Python 3.9.13 64-bit",
+   "display_name": "Python 3.8.13 ('gpt3')",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
    "name": "python",
-   "version": "3.9.13"
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.9.12"
   },
   "orig_nbformat": 4,
   "vscode": {
    "interpreter": {
-    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
+    "hash": "9a4fa60962de90e73b5da8d67a44b01d2de04630d82b94b8db1f727a73d31e61"
    }
   }
  },
diff --git a/gpt3forchem/_modidx.py b/gpt3forchem/_modidx.py
index ff8e67c..fb48397 100644
--- a/gpt3forchem/_modidx.py
+++ b/gpt3forchem/_modidx.py
@@ -55,6 +55,7 @@ d = { 'settings': { 'allowed_cell_metadata_keys': '',
                                    'gpt3forchem.input.POLYMER_ONE_PROPERTY_INVERSE_COMPLETION_TEMPLATE_CAT': 'https://kjappelbaum.github.io/gpt3forchem/input.html#polymer_one_property_inverse_completion_template_cat',
                                    'gpt3forchem.input.POLYMER_ONE_PROPERTY_INVERSE_PROMPT_TEMPLATE_CAT': 'https://kjappelbaum.github.io/gpt3forchem/input.html#polymer_one_property_inverse_prompt_template_cat',
                                    'gpt3forchem.input.POLYMER_ONE_PROPERTY_INVERSE_PROMPT_TEMPLATE_CAT_W_COMPOSITION': 'https://kjappelbaum.github.io/gpt3forchem/input.html#polymer_one_property_inverse_prompt_template_cat_w_composition',
+                                   'gpt3forchem.input.create_single_property_forward_prompts': 'https://kjappelbaum.github.io/gpt3forchem/input.html#create_single_property_forward_prompts',
                                    'gpt3forchem.input.decode_categorical_value': 'https://kjappelbaum.github.io/gpt3forchem/input.html#decode_categorical_value',
                                    'gpt3forchem.input.encode_categorical_value': 'https://kjappelbaum.github.io/gpt3forchem/input.html#encode_categorical_value',
                                    'gpt3forchem.input.get_polymer_composition_dict': 'https://kjappelbaum.github.io/gpt3forchem/input.html#get_polymer_composition_dict'},
diff --git a/gpt3forchem/api_wrappers.py b/gpt3forchem/api_wrappers.py
index 912a0eb..539d0e4 100644
--- a/gpt3forchem/api_wrappers.py
+++ b/gpt3forchem/api_wrappers.py
@@ -3,18 +3,27 @@
 # %% auto 0
 __all__ = ['fine_tune', 'query_gpt3', 'extract_prediction']
 
-# %% ../notebooks/01_api_wrappers.ipynb 4
+# %% ../notebooks/01_api_wrappers.ipynb 2
+import subprocess
+
+import openai
+import time
+import re
+
+# %% ../notebooks/01_api_wrappers.ipynb 5
 def fine_tune(train_file, valid_file, model: str = "ada"):
     # run the fine tuning
-    subprocess.run(
+    result = subprocess.run(
         f"openai api fine_tunes.create -t {train_file} -v {valid_file} -m {model}",
         shell=True,
+        stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
     )
+    modelname = re.findall(r'completions.create -m ([\w\d:-]+) -p', result.stdout)[0]
     # sync runs with wandb
     subprocess.run("openai wandb sync -n 1", shell=True)
+    return modelname
 
-
-# %% ../notebooks/01_api_wrappers.ipynb 6
+# %% ../notebooks/01_api_wrappers.ipynb 8
 def query_gpt3(model, df, temperature=0, max_tokens=10):
     completions = []
     for i, row in df.iterrows():
@@ -29,7 +38,7 @@ def query_gpt3(model, df, temperature=0, max_tokens=10):
 
     return completions
 
-# %% ../notebooks/01_api_wrappers.ipynb 7
+# %% ../notebooks/01_api_wrappers.ipynb 9
 def extract_prediction(completion):
     return completion["choices"][0]["text"].split("@")[0].strip()
 
diff --git a/gpt3forchem/input.py b/gpt3forchem/input.py
index 02c8b37..f075dc7 100644
--- a/gpt3forchem/input.py
+++ b/gpt3forchem/input.py
@@ -4,9 +4,12 @@
 __all__ = ['ONE_PROPERTY_FORWARD_PROMPT_TEMPLATE', 'ONE_PROPERTY_FORWARD_COMPLETION_TEMPLATE',
            'POLYMER_ONE_PROPERTY_INVERSE_PROMPT_TEMPLATE_CAT', 'POLYMER_ONE_PROPERTY_INVERSE_COMPLETION_TEMPLATE_CAT',
            'POLYMER_ONE_PROPERTY_INVERSE_PROMPT_TEMPLATE_CAT_W_COMPOSITION', 'encode_categorical_value',
-           'decode_categorical_value', 'get_polymer_composition_dict']
+           'decode_categorical_value', 'create_single_property_forward_prompts', 'get_polymer_composition_dict']
 
 # %% ../notebooks/03_input.ipynb 2
+import pandas as pd
+from collections import Counter
+
 _DEFAULT_ENCODING_DICT = {
     "very small": 0,
     "small": 1,
@@ -18,14 +21,14 @@ _DEFAULT_ENCODING_DICT = {
 _DEFAULT_DECODING_DICT = {v: k for k, v in _DEFAULT_ENCODING_DICT.items()}
 
 
-def encode_categorical_value(value, encoding_dict):
+def encode_categorical_value(value, encoding_dict=_DEFAULT_DECODING_DICT):
     try:
         return encoding_dict[value]
     except KeyError:
         raise ValueError("Unknown value: %s" % value)
 
 
-def decode_categorical_value(value, decoding_dict):
+def decode_categorical_value(value, decoding_dict=_DEFAULT_DECODING_DICT):
     try:
         return decoding_dict[value]
     except KeyError:
@@ -37,7 +40,43 @@ ONE_PROPERTY_FORWARD_PROMPT_TEMPLATE = "what is the {property} of {text}###"
 ONE_PROPERTY_FORWARD_COMPLETION_TEMPLATE = " {value}@@@"
 
 
-# %% ../notebooks/03_input.ipynb 8
+# %% ../notebooks/03_input.ipynb 4
+def create_single_property_forward_prompts(
+    df, # input data
+    target, # target property
+    target_rename_dict, # dict to rename target property from the column name in df to the target property name in the prompt
+    encode_value=True, # whether to encode the value of the target property categorically
+    encoding_dict=_DEFAULT_ENCODING_DICT, # mapping from numerical categories to string
+    prompt_prefix="", # prefix to add to the prompt, e.g. "I am an expert chemist"
+):
+    prompts = []
+
+    target_name = target
+    for key, value in target_rename_dict.items():
+        target_name = target_name.replace(key, value)
+
+    for _, row in df.iterrows():
+        if encode_value:
+            value = encode_categorical_value(row[target], encoding_dict=encoding_dict)
+        else:
+            value = row[target]
+
+        prompts.append(
+            {
+                "prompt": prompt_prefix
+                + ONE_PROPERTY_FORWARD_PROMPT_TEMPLATE.format(
+                    property=target_name, text=row["string"]
+                ),
+                "completion": ONE_PROPERTY_FORWARD_COMPLETION_TEMPLATE.format(
+                    value=value
+                ),
+            }
+        )
+
+    return pd.DataFrame(prompts)
+
+
+# %% ../notebooks/03_input.ipynb 9
 POLYMER_ONE_PROPERTY_INVERSE_PROMPT_TEMPLATE_CAT = (
     "what is a polymer with {class_name} {property}?###"
 )
@@ -46,7 +85,7 @@ POLYMER_ONE_PROPERTY_INVERSE_COMPLETION_TEMPLATE_CAT = " {text}@@@"
 POLYMER_ONE_PROPERTY_INVERSE_PROMPT_TEMPLATE_CAT_W_COMPOSITION = "what is a polymer with {class_name} {property} and {num_A} A, {num_B} B, {num_W} W, and {num_R} R?###"
 
 
-# %% ../notebooks/03_input.ipynb 9
+# %% ../notebooks/03_input.ipynb 10
 def get_polymer_composition_dict(row):
     composition = Counter(row["string"].split("-"))
     comp_dict = {}
diff --git a/notebooks/01_api_wrappers.ipynb b/notebooks/01_api_wrappers.ipynb
index 0be2429..dfe6f05 100644
--- a/notebooks/01_api_wrappers.ipynb
+++ b/notebooks/01_api_wrappers.ipynb
@@ -16,11 +16,22 @@
    "outputs": [],
    "source": [
     "# |hide\n",
-    "from nbdev.showdoc import *\n",
+    "from nbdev.showdoc import *"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 3,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# |export\n",
+    "\n",
     "import subprocess\n",
     "\n",
     "import openai\n",
-    "import time\n"
+    "import time\n",
+    "import re"
    ]
   },
   {
@@ -41,19 +52,31 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 12,
    "metadata": {},
    "outputs": [],
    "source": [
     "# |export\n",
     "def fine_tune(train_file, valid_file, model: str = \"ada\"):\n",
     "    # run the fine tuning\n",
-    "    subprocess.run(\n",
+    "    result = subprocess.run(\n",
     "        f\"openai api fine_tunes.create -t {train_file} -v {valid_file} -m {model}\",\n",
     "        shell=True,\n",
+    "        stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
     "    )\n",
+    "    modelname = re.findall(r'completions.create -m ([\\w\\d:-]+) -p', result.stdout)[0]\n",
     "    # sync runs with wandb\n",
-    "    subprocess.run(\"openai wandb sync -n 1\", shell=True)\n"
+    "    subprocess.run(\"openai wandb sync -n 1\", shell=True)\n",
+    "    return modelname"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 4,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "_stdout_fragment = \"openai api completions.create -m ada:ft-epfl-2022-06-23-09-10-58 -p <YOUR_PROMPT>\""
    ]
   },
   {
diff --git a/notebooks/03_input.ipynb b/notebooks/03_input.ipynb
index d5a5a0d..9bc4122 100644
--- a/notebooks/03_input.ipynb
+++ b/notebooks/03_input.ipynb
@@ -6,9 +6,7 @@
    "metadata": {},
    "outputs": [],
    "source": [
-    "# | default_exp input\n",
-    "import pandas as pd\n",
-    "from collections import Counter\n"
+    "# | default_exp input"
    ]
   },
   {
@@ -20,11 +18,14 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 21,
    "metadata": {},
    "outputs": [],
    "source": [
     "# | export\n",
+    "import pandas as pd\n",
+    "from collections import Counter\n",
+    "\n",
     "_DEFAULT_ENCODING_DICT = {\n",
     "    \"very small\": 0,\n",
     "    \"small\": 1,\n",
@@ -67,6 +68,7 @@
    "metadata": {},
    "outputs": [],
    "source": [
+    "# | export\n",
     "def create_single_property_forward_prompts(\n",
     "    df, # input data\n",
     "    target, # target property\n",
diff --git a/notebooks/index.ipynb b/notebooks/index.ipynb
index 7a85238..a7c559c 100644
--- a/notebooks/index.ipynb
+++ b/notebooks/index.ipynb
@@ -39,8 +39,18 @@
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "## How to use"
+    "## How to use\n",
+    "\n",
+    "\n",
+    "* the `legacy` directory contains code from initial exploration. The relevant parts have been migrated into notebooks. \n",
+    "\n",
+    "* the `experiments` directory contain code for the actual fine-tuning experiments"
    ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": []
   }
  ],
  "metadata": {
