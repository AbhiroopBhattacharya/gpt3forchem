diff --git a/experiments/02_polymer_learning_curves.py b/experiments/02_polymer_learning_curves.py
deleted file mode 100644
index 60bf606..0000000
--- a/experiments/02_polymer_learning_curves.py
+++ /dev/null
@@ -1,95 +0,0 @@
-import time
-
-from fastcore.utils import save_pickle
-from pycm import ConfusionMatrix
-from sklearn.model_selection import train_test_split
-
-from gpt3forchem.api_wrappers import extract_prediction, fine_tune, query_gpt3
-from gpt3forchem.data import get_polymer_data
-from gpt3forchem.input import create_single_property_forward_prompts
-
-TRAIN_SET_SIZE = [10, 50, 100, 200, 500, 1000, 2000, 3000]
-REPEATS = 10
-MODEL_TYPES = ["davinci", "ada"]
-PREFIXES = ["", "I'm an expert polymer chemist "]
-
-DF = get_polymer_data()
-RANDOM_STATE = None
-MAX_TEST_SIZE = 200  # upper limit to speed it up, this will still require 10 requests
-
-
-def learning_curve_point(model_type, train_set_size, prefix):
-    df_train, df_test = train_test_split(
-        DF, train_size=train_set_size, random_state=None
-    )
-    train_prompts = create_single_property_forward_prompts(
-        df_train,
-        "deltaGmin_cat",
-        {"deltaGmin_cat": "adsorption energy"},
-        prompt_prefix=prefix,
-    )
-
-    test_prompts = create_single_property_forward_prompts(
-        df_test,
-        "deltaGmin_cat",
-        {"deltaGmin_cat": "adsorption energy"},
-        prompt_prefix=prefix,
-    )
-
-    train_size = len(train_prompts)
-    test_size = len(test_prompts)
-
-    filename_base = time.strftime("%Y-%m-%d-%H-%M-%S", time.localtime())
-    train_filename = (
-        f"run_files/{filename_base}_train_prompts_polymers_{train_size}.jsonl"
-    )
-    valid_filename = (
-        f"run_files/{filename_base}_valid_prompts_polymers_{test_size}.jsonl"
-    )
-
-    train_prompts.to_json(train_filename, orient="records", lines=True)
-    test_prompts.to_json(valid_filename, orient="records", lines=True)
-
-    print(f"Training {model_type} model on {train_size} training examples")
-    modelname = fine_tune(train_filename, valid_filename, model_type)
-    test_prompts = test_prompts.iloc[:40]
-    completions = query_gpt3(modelname, test_prompts)
-    predictions = [
-        extract_prediction(completions, i)
-        for i, completion in enumerate(completions["choices"][0])
-    ]
-    true = [
-        int(test_prompts.iloc[i]["completion"].split("@")[0])
-        for i in range(len(predictions))
-    ]
-    cm = ConfusionMatrix(true, predictions)
-
-    results = {
-        "model_type": model_type,
-        "train_set_size": train_set_size,
-        "prefix": prefix,
-        "train_size": train_size,
-        "test_size": test_size,
-        "cm": cm,
-        "accuracy": cm.ACC_Macro,
-        "completions": completions,
-        "train_filename": train_filename,
-        "valid_filename": valid_filename,
-        "MAX_TEST_SIZE": MAX_TEST_SIZE,
-    }
-
-    outname = f"results/{filename_base}_results_polymers_{train_size}_{prefix}_{model_type}.pkl"
-
-    save_pickle(outname, results)
-
-
-if __name__ == "__main__":
-    for repeat in range(REPEATS):
-        for model_type in MODEL_TYPES:
-            for train_set_size in TRAIN_SET_SIZE:
-                for prefix in PREFIXES:
-                    try:
-                        learning_curve_point(model_type, train_set_size, prefix)
-                    except Exception as e:
-                        time.sleep(10)
-                        print(e)
diff --git a/experiments/wandb/latest-run b/experiments/wandb/latest-run
index 3286af4..62628b7 120000
--- a/experiments/wandb/latest-run
+++ b/experiments/wandb/latest-run
@@ -1 +1 @@
-run-20220822_104328-ft-fARz0PHXfc4Dp43XiAh173PQ
\ No newline at end of file
+run-20220822_142836-ft-SsRkqzwvIxWmNTVXF3JX1Cls
\ No newline at end of file
diff --git a/notebooks/04_output.ipynb b/notebooks/04_output.ipynb
index cd3bc11..6798260 100644
--- a/notebooks/04_output.ipynb
+++ b/notebooks/04_output.ipynb
@@ -11,20 +11,23 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 1,
    "metadata": {},
    "outputs": [],
    "source": [
     "# |hide\n",
     "import re\n",
-    "from collections import defaultdict, Counter\n",
+    "from collections import Counter, defaultdict\n",
     "from typing import Iterable\n",
     "\n",
     "import numpy as np\n",
     "from nbdev.showdoc import *\n",
     "from strsimpy.levenshtein import Levenshtein\n",
     "from strsimpy.longest_common_subsequence import LongestCommonSubsequence\n",
-    "from strsimpy.normalized_levenshtein import NormalizedLevenshtein\n"
+    "from strsimpy.normalized_levenshtein import NormalizedLevenshtein\n",
+    "\n",
+    "\n",
+    "from sklearn.metrics import r2_score, max_error, mean_absolute_error, mean_squared_error"
    ]
   },
   {
@@ -108,10 +111,12 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [],
    "source": [
+    "# |export\n",
+    "\n",
     "def convert2smiles(string):\n",
     "    new_encoding = {\"A\": \"[Ta]\", \"B\": \"[Tr]\", \"W\": \"[W]\", \"R\": \"[R]\"}\n",
     "\n",
@@ -159,10 +164,11 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 3,
    "metadata": {},
    "outputs": [],
    "source": [
+    "# |export\n",
     "def get_num_monomer(string, monomer):\n",
     "    num = re.findall(f\"([\\d+]) {monomer}\", string)\n",
     "    try:\n",
@@ -198,6 +204,7 @@
    "metadata": {},
    "outputs": [],
    "source": [
+    "# |export\n",
     "def get_prompt_compostion(prompt):\n",
     "    composition = {}\n",
     "\n",
@@ -213,6 +220,8 @@
    "metadata": {},
    "outputs": [],
    "source": [
+    "# |export\n",
+    "\n",
     "def get_target(string, target_name=\"adsorption\"):\n",
     "    num = re.findall(f\"([\\d+]) {target_name}\", string)\n",
     "    return int(num[0])\n"
@@ -224,6 +233,8 @@
    "metadata": {},
    "outputs": [],
    "source": [
+    "# |export\n",
+    "\n",
     "def get_prompt_data(prompt):\n",
     "    composition = get_prompt_compostion(prompt)\n",
     "\n",
@@ -236,6 +247,8 @@
    "metadata": {},
    "outputs": [],
    "source": [
+    "# |export\n",
+    "\n",
     "def get_completion_composition(string):\n",
     "    parts = string.split(\"-\")\n",
     "    counts = Counter(parts)\n",
@@ -248,6 +261,8 @@
    "metadata": {},
    "outputs": [],
    "source": [
+    "# |export\n",
+    "\n",
     "def string2performance(string):\n",
     "    # we need to perform a bunch of tasks here:\n",
     "    # 1) Featurize\n",
@@ -274,6 +289,8 @@
    "metadata": {},
    "outputs": [],
    "source": [
+    "# |export\n",
+    "\n",
     "def composition_mismatch(composition: dict, found: dict):\n",
     "    distances = []\n",
     "\n",
@@ -309,6 +326,48 @@
     "    }\n"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": 4,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# |export\n",
+    "\n",
+    "def get_regression_metrics(y_true, y_pred):\n",
+    "    return {\n",
+    "        \"r2\": r2_score(y_true, y_pred),\n",
+    "        \"max_error\": max_error(y_true, y_pred),\n",
+    "        \"mean_absolute_error\": mean_absolute_error(y_true, y_pred),\n",
+    "        \"mean_squared_error\": mean_squared_error(y_true, y_pred),\n",
+    "    }"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "{'r2': 1.0,\n",
+       " 'max_error': 0,\n",
+       " 'mean_absolute_error': 0.0,\n",
+       " 'mean_squared_error': 0.0}"
+      ]
+     },
+     "execution_count": 5,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "get_regression_metrics(\n",
+    "    [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]\n",
+    ")"
+   ]
+  },
   {
    "cell_type": "code",
    "execution_count": null,
@@ -319,9 +378,26 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "Python 3.8.13 ('gpt3')",
+   "display_name": "Python 3.9.12 ('gpt3')",
    "language": "python",
    "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.9.13"
+  },
+  "vscode": {
+   "interpreter": {
+    "hash": "9a4fa60962de90e73b5da8d67a44b01d2de04630d82b94b8db1f727a73d31e61"
+   }
   }
  },
  "nbformat": 4,
