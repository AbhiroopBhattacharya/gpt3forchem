diff --git a/.gitignore b/.gitignore
index 97f0b83..0c141df 100644
--- a/.gitignore
+++ b/.gitignore
@@ -143,3 +143,4 @@ checklink/cookies.txt
 .gitconfig
 legacy
 
+dev/
\ No newline at end of file
diff --git a/experiments/09_photoswitch_learning_curve.py b/experiments/09_photoswitch_learning_curve.py
index 6284bab..e16ebf8 100644
--- a/experiments/09_photoswitch_learning_curve.py
+++ b/experiments/09_photoswitch_learning_curve.py
@@ -1,4 +1,3 @@
-from doctest import DocFileCase
 import time
 
 from fastcore.xtras import save_pickle
@@ -51,7 +50,7 @@ def learning_curve_point(representation, model_type, train_set_size):
     test_prompts.to_json(valid_filename, orient="records", lines=True)
 
     print(f"Training {model_type} model on {train_size} training examples")
-    modelname = fine_tune(train_filename, valid_filename, model_type)
+    modelname = fine_tune(train_filename, valid_filename, model_type, n_epochs=2)
 
     completions = query_gpt3(modelname, test_prompts)
     predictions = [
@@ -84,7 +83,7 @@ def learning_curve_point(representation, model_type, train_set_size):
         "baseline_accuracy": baseline["cm"].ACC_Macro,
     }
 
-    outname = f"results/photoswitch/{filename_base}_results_photoswitch_{train_size}_{model_type}_{representation}.pkl"
+    outname = f"results/photoswitch_2epoch/{filename_base}_results_photoswitch_{train_size}_{model_type}_{representation}.pkl"
 
     save_pickle(outname, results)
     return results
diff --git a/experiments/wandb/latest-run b/experiments/wandb/latest-run
index 21b72cb..157af13 120000
--- a/experiments/wandb/latest-run
+++ b/experiments/wandb/latest-run
@@ -1 +1 @@
-run-20220831_152656-ft-tMaOO5fT4Y8mfa1Q6KOhmuP8
\ No newline at end of file
+run-20220831_232416-ft-mJeuSXztKVS9hWI45LPQ7Yus
\ No newline at end of file
diff --git a/gpt3forchem/api_wrappers.py b/gpt3forchem/api_wrappers.py
index e96d4a3..4f632d2 100644
--- a/gpt3forchem/api_wrappers.py
+++ b/gpt3forchem/api_wrappers.py
@@ -17,11 +17,12 @@ from sklearn.model_selection import train_test_split
 def fine_tune(
     train_file,  # path to json file with training prompts (column names "prompt" and "completion")
     valid_file,  # path to json file with validation prompts (column names "prompt" and "completion")
-    model: str = "ada",  # model type to use. One of "ada", "davinci". "ada" is the default (and cheapest).
+    model: str = "ada",  # model type to use. One of "ada", "babbage", "curie", "davinci". "ada" is the default (and cheapest).
+    n_epochs: int = 4,  # number of epochs to fine-tune for
 ):
     """Run the fine tuning of a GPT-3 model via the OpenAI API."""
     result = subprocess.run(
-        f"openai api fine_tunes.create -t {train_file} -v {valid_file} -m {model}",
+        f"openai api fine_tunes.create -t {train_file} -v {valid_file} -m {model} --n_epochs {n_epochs}",
         shell=True,
         stdout=subprocess.PIPE,
         stderr=subprocess.PIPE,
diff --git a/gpt3forchem/data.py b/gpt3forchem/data.py
index 307fc04..fb0bba1 100644
--- a/gpt3forchem/data.py
+++ b/gpt3forchem/data.py
@@ -1,7 +1,7 @@
 # AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/00_data.ipynb.
 
 # %% auto 0
-__all__ = ['get_polymer_data', 'get_photoswitch_data']
+__all__ = ['POLYMER_FEATURES', 'get_polymer_data', 'get_photoswitch_data']
 
 # %% ../notebooks/00_data.ipynb 2
 import os
@@ -12,13 +12,32 @@ _THIS_DIR = os.path.abspath(os.path.dirname(os.path.abspath("")))
 
 
 # %% ../notebooks/00_data.ipynb 5
+POLYMER_FEATURES = [
+    "num_[W]",
+    "max_[W]",
+    "num_[Tr]",
+    "max_[Tr]",
+    "num_[Ta]",
+    "max_[Ta]",
+    "num_[R]",
+    "max_[R]",
+    "[W]",
+    "[Tr]",
+    "[Ta]",
+    "[R]",
+    "rel_shannon",
+    "length",
+]
+
+
+# %% ../notebooks/00_data.ipynb 6
 def get_polymer_data(
     datadir="../data" # path to folder with data files
 ):
     return pd.read_csv(os.path.join(datadir, "polymers.csv"))
 
 
-# %% ../notebooks/00_data.ipynb 9
+# %% ../notebooks/00_data.ipynb 10
 def get_photoswitch_data(
     datadir="../data" # path to folder with data files
 ):
diff --git a/legacy/experiments/polymers/20220622_fine_tune_forward_regression.ipynb b/legacy/experiments/polymers/20220622_fine_tune_forward_regression.ipynb
index df9aa1e..426d27d 100644
--- a/legacy/experiments/polymers/20220622_fine_tune_forward_regression.ipynb
+++ b/legacy/experiments/polymers/20220622_fine_tune_forward_regression.ipynb
@@ -357156,7 +357156,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.8.13"
+   "version": "3.9.13"
   },
   "orig_nbformat": 4
  },
diff --git a/notebooks/00_data.ipynb b/notebooks/00_data.ipynb
index 66aee18..26937ea 100644
--- a/notebooks/00_data.ipynb
+++ b/notebooks/00_data.ipynb
@@ -51,6 +51,32 @@
     "## Polymers\n"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# |export\n",
+    "\n",
+    "POLYMER_FEATURES = [\n",
+    "    \"num_[W]\",\n",
+    "    \"max_[W]\",\n",
+    "    \"num_[Tr]\",\n",
+    "    \"max_[Tr]\",\n",
+    "    \"num_[Ta]\",\n",
+    "    \"max_[Ta]\",\n",
+    "    \"num_[R]\",\n",
+    "    \"max_[R]\",\n",
+    "    \"[W]\",\n",
+    "    \"[Tr]\",\n",
+    "    \"[Ta]\",\n",
+    "    \"[R]\",\n",
+    "    \"rel_shannon\",\n",
+    "    \"length\",\n",
+    "]\n"
+   ]
+  },
   {
    "cell_type": "code",
    "execution_count": null,
diff --git a/notebooks/01_api_wrappers.ipynb b/notebooks/01_api_wrappers.ipynb
index dcc45dc..815ebf4 100644
--- a/notebooks/01_api_wrappers.ipynb
+++ b/notebooks/01_api_wrappers.ipynb
@@ -62,11 +62,12 @@
     "def fine_tune(\n",
     "    train_file,  # path to json file with training prompts (column names \"prompt\" and \"completion\")\n",
     "    valid_file,  # path to json file with validation prompts (column names \"prompt\" and \"completion\")\n",
-    "    model: str = \"ada\",  # model type to use. One of \"ada\", \"davinci\". \"ada\" is the default (and cheapest).\n",
+    "    model: str = \"ada\",  # model type to use. One of \"ada\", \"babbage\", \"curie\", \"davinci\". \"ada\" is the default (and cheapest).\n",
+    "    n_epochs: int = 4,  # number of epochs to fine-tune for\n",
     "):\n",
     "    \"\"\"Run the fine tuning of a GPT-3 model via the OpenAI API.\"\"\"\n",
     "    result = subprocess.run(\n",
-    "        f\"openai api fine_tunes.create -t {train_file} -v {valid_file} -m {model}\",\n",
+    "        f\"openai api fine_tunes.create -t {train_file} -v {valid_file} -m {model} --n_epochs {n_epochs}\",\n",
     "        shell=True,\n",
     "        stdout=subprocess.PIPE,\n",
     "        stderr=subprocess.PIPE,\n",
@@ -324,6 +325,15 @@
    "display_name": "Python 3.9.13 64-bit",
    "language": "python",
    "name": "python3"
+  },
+  "language_info": {
+   "name": "python",
+   "version": "3.9.13"
+  },
+  "vscode": {
+   "interpreter": {
+    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
+   }
   }
  },
  "nbformat": 4,
