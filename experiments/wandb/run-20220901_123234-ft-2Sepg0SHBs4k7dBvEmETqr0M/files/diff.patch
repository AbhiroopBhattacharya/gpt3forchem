diff --git a/experiments/wandb/latest-run b/experiments/wandb/latest-run
index 2f66126..884420d 120000
--- a/experiments/wandb/latest-run
+++ b/experiments/wandb/latest-run
@@ -1 +1 @@
-run-20220901_100804-ft-GGKPUYWhGO3GQGlpSHYVzwbV
\ No newline at end of file
+run-20220901_123234-ft-2Sepg0SHBs4k7dBvEmETqr0M
\ No newline at end of file
diff --git a/gpt3forchem/_modidx.py b/gpt3forchem/_modidx.py
index 83b01a6..ffa71d9 100644
--- a/gpt3forchem/_modidx.py
+++ b/gpt3forchem/_modidx.py
@@ -5,12 +5,20 @@ d = { 'settings': { 'branch': 'main',
                 'doc_host': 'https://kjappelbaum.github.io',
                 'git_url': 'https://github.com/kjappelbaum/gpt3forchem/',
                 'lib_path': 'gpt3forchem'},
-  'syms': { 'gpt3forchem.api_wrappers': { 'gpt3forchem.api_wrappers.extract_prediction': ( 'api_wrappers.html#extract_prediction',
+  'syms': { 'gpt3forchem.api_wrappers': { 'gpt3forchem.api_wrappers._fine_tune': ( 'api_wrappers.html#_fine_tune',
+                                                                                   'gpt3forchem/api_wrappers.py'),
+                                          'gpt3forchem.api_wrappers.ensemble_fine_tune': ( 'api_wrappers.html#ensemble_fine_tune',
+                                                                                           'gpt3forchem/api_wrappers.py'),
+                                          'gpt3forchem.api_wrappers.extract_prediction': ( 'api_wrappers.html#extract_prediction',
                                                                                            'gpt3forchem/api_wrappers.py'),
                                           'gpt3forchem.api_wrappers.extract_regression_prediction': ( 'api_wrappers.html#extract_regression_prediction',
                                                                                                       'gpt3forchem/api_wrappers.py'),
                                           'gpt3forchem.api_wrappers.fine_tune': ( 'api_wrappers.html#fine_tune',
                                                                                   'gpt3forchem/api_wrappers.py'),
+                                          'gpt3forchem.api_wrappers.multiple_fine_tunes': ( 'api_wrappers.html#multiple_fine_tunes',
+                                                                                            'gpt3forchem/api_wrappers.py'),
+                                          'gpt3forchem.api_wrappers.multiple_query_gpt3': ( 'api_wrappers.html#multiple_query_gpt3',
+                                                                                            'gpt3forchem/api_wrappers.py'),
                                           'gpt3forchem.api_wrappers.query_gpt3': ( 'api_wrappers.html#query_gpt3',
                                                                                    'gpt3forchem/api_wrappers.py'),
                                           'gpt3forchem.api_wrappers.train_test_loop': ( 'api_wrappers.html#train_test_loop',
diff --git a/gpt3forchem/api_wrappers.py b/gpt3forchem/api_wrappers.py
index df9c151..75a87d4 100644
--- a/gpt3forchem/api_wrappers.py
+++ b/gpt3forchem/api_wrappers.py
@@ -1,7 +1,8 @@
 # AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/01_api_wrappers.ipynb.
 
 # %% auto 0
-__all__ = ['fine_tune', 'query_gpt3', 'extract_prediction', 'extract_regression_prediction', 'train_test_loop']
+__all__ = ['fine_tune', 'query_gpt3', 'extract_prediction', 'extract_regression_prediction', 'train_test_loop',
+           'multiple_fine_tunes', 'ensemble_fine_tune', 'multiple_query_gpt3']
 
 # %% ../notebooks/01_api_wrappers.ipynb 2
 import re
@@ -172,3 +173,65 @@ def train_test_loop(
 
     return out
 
+
+# %% ../notebooks/01_api_wrappers.ipynb 18
+def _fine_tune(file_tuple): return fine_tune(*file_tuple)
+
+def multiple_fine_tunes(
+    train_files, 
+    valid_files,
+):
+    print('Fine tuning on {} train files and {} valid files'.format(len(train_files), len(valid_files)))
+    models = parallel(_fine_tune, [(train_file, valid_file) for train_file, valid_file in zip(train_files, valid_files)])
+    return models
+
+# %% ../notebooks/01_api_wrappers.ipynb 19
+def ensemble_fine_tune(
+    train_frame, 
+    valid_frame, 
+    num_models: int = 10,
+    subsample: float = 0.8, 
+    run_file_dir: str = "run_files",
+    filename_base_string: str = ""
+): 
+    train_frames = [train_frame.sample(frac=subsample) for _ in range(num_models)]
+    valid_frames = [valid_frame]
+
+    train_filenames = []
+    for i, train_frame in enumerate(train_frames):
+        train_size  = len(train_frame)
+
+        filename_base = time.strftime("%Y-%m-%d-%H-%M-%S", time.localtime())
+        train_filename = f"run_files/{filename_base}_train_{filename_base_string}_ensemble_{i}_{train_size}.jsonl"
+        train_filenames.append(train_filename)
+        train_frame.to_json(train_filename, orient="records", lines=True)
+
+    valid_filenames = []
+    for i, valid_frame in enumerate(valid_frames):
+        valid_size  = len(valid_frame)
+
+        filename_base = time.strftime("%Y-%m-%d-%H-%M-%S", time.localtime())
+        valid_filename = os.path.join(run_file_dir, f"{filename_base}_valid_{filename_base_string}_ensemble_{i}_{valid_size}.jsonl")
+        valid_filenames.append(valid_filename)
+        valid_frame.to_json(valid_filename, orient="records", lines=True)
+
+    models = multiple_fine_tunes(train_filenames, valid_filenames)
+
+    return models
+
+# %% ../notebooks/01_api_wrappers.ipynb 21
+def multiple_query_gpt3(
+    models: List[str],  # names of the models to use, e.g. "ada:ft-personal-2022-08-24-10-41-29"
+    df: pd.DataFrame,  # dataframe with prompts and expected completions (column names "prompt" and "completion")
+    temperature: float = 0,  # temperature, 0 is the default and corresponds to argmax
+    max_tokens: int = 10,  # maximum number of tokens to generate
+    sleep: float = 5,  # number of seconds to wait between queries
+    one_by_one: bool = False,  # if True, generate one completion at a time (i.e., due to submit the maximum number of prompts per request)
+    parallel_max: int = 20,  # maximum number of prompts that can be sent per request
+):
+    curried_query = partial(query_gpt3, df=df, temperature=temperature, max_tokens=max_tokens, sleep=sleep, one_by_one=one_by_one, parallel_max=parallel_max)
+
+    completions = parallel(curried_query, models)
+
+    return completions
+
diff --git a/notebooks/01_api_wrappers.ipynb b/notebooks/01_api_wrappers.ipynb
index 0ef1267..2782093 100644
--- a/notebooks/01_api_wrappers.ipynb
+++ b/notebooks/01_api_wrappers.ipynb
@@ -343,11 +343,15 @@
    "metadata": {},
    "outputs": [],
    "source": [
+    "# |export\n",
+    "def _fine_tune(file_tuple): return fine_tune(*file_tuple)\n",
+    "\n",
     "def multiple_fine_tunes(\n",
     "    train_files, \n",
     "    valid_files,\n",
     "):\n",
-    "    models = parallel(fine_tune, [(train_file, valid_file) for train_file, valid_file in zip(train_files, valid_files)])\n",
+    "    print('Fine tuning on {} train files and {} valid files'.format(len(train_files), len(valid_files)))\n",
+    "    models = parallel(_fine_tune, [(train_file, valid_file) for train_file, valid_file in zip(train_files, valid_files)])\n",
     "    return models"
    ]
   },
@@ -357,6 +361,7 @@
    "metadata": {},
    "outputs": [],
    "source": [
+    "# |export\n",
     "def ensemble_fine_tune(\n",
     "    train_frame, \n",
     "    valid_frame, \n",
@@ -369,7 +374,7 @@
     "    valid_frames = [valid_frame]\n",
     "\n",
     "    train_filenames = []\n",
-    "    for i, train_frame in train_frames:\n",
+    "    for i, train_frame in enumerate(train_frames):\n",
     "        train_size  = len(train_frame)\n",
     "\n",
     "        filename_base = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
@@ -378,7 +383,7 @@
     "        train_frame.to_json(train_filename, orient=\"records\", lines=True)\n",
     "\n",
     "    valid_filenames = []\n",
-    "    for i, valid_frame in valid_frames:\n",
+    "    for i, valid_frame in enumerate(valid_frames):\n",
     "        valid_size  = len(valid_frame)\n",
     "\n",
     "        filename_base = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
@@ -402,6 +407,7 @@
    "metadata": {},
    "outputs": [],
    "source": [
+    "# |export\n",
     "def multiple_query_gpt3(\n",
     "    models: List[str],  # names of the models to use, e.g. \"ada:ft-personal-2022-08-24-10-41-29\"\n",
     "    df: pd.DataFrame,  # dataframe with prompts and expected completions (column names \"prompt\" and \"completion\")\n",
