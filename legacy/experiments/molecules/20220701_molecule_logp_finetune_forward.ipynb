{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single output tuning for molecules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycm import ConfusionMatrix\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from gpt3forchem.molecules.data import get_data\n",
    "from gpt3forchem.molecules.baseline.classification import LogisticClassifierBaseline\n",
    "from gpt3forchem.molecules.constants import TARGETS, FEATURES, CAT_TARGETS, TEXT\n",
    "from gpt3forchem.molecules.create_prompts import create_single_property_forward_prompts\n",
    "from gpt3forchem.fine_tune import fine_tune\n",
    "from gpt3forchem.query_model import query_gpt3, extract_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(train_df, test_df):\n",
    "    train_size = len(train_df)\n",
    "    test_size = len(test_df)\n",
    "    run = wandb.init(\n",
    "        project=\"gpt-3\",\n",
    "        job_type=\"train-baseline-model\",\n",
    "        config={\n",
    "            \"train_size\": train_size,\n",
    "            \"test_size\": test_size,\n",
    "            \"features\": FEATURES,\n",
    "            \"targets\": [CAT_TARGETS[0]],\n",
    "            \"ds\": \"molecule\",\n",
    "        },\n",
    "    )\n",
    "    baseline = LogisticClassifierBaseline(43)\n",
    "    baseline.fit(train_df[FEATURES], train_df[CAT_TARGETS[0]])\n",
    "    predictions = baseline.predict(test_df[FEATURES].values)\n",
    "\n",
    "    run.log(\n",
    "        {\n",
    "            \"distribution confusion matrix\": wandb.plot.confusion_matrix(\n",
    "                y_true=test_df[CAT_TARGETS[0]].values,\n",
    "                preds=predictions,\n",
    "                class_names=np.arange(5),\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "    cm = ConfusionMatrix(\n",
    "        test_df[CAT_TARGETS[0]].values,\n",
    "        predictions,\n",
    "    )\n",
    "\n",
    "    run.log(\n",
    "        {\"ACC\": cm.ACC, \"ACC_macro\": cm.ACC_Macro, \"F1_micro\": cm.F1_Micro, \"F1_macro\": cm.F1_Macro}\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        {\"ACC\": cm.ACC, \"ACC_macro\": cm.ACC_Macro, \"F1_micro\": cm.F1_Micro, \"F1_macro\": cm.F1_Macro}\n",
    "    )\n",
    "\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_fine_tune(train_df, test_df, text=0):\n",
    "    print(f\"Creating fine tune prompts for {TEXT[text]}\")\n",
    "    train_prompts = create_single_property_forward_prompts(train_df, CAT_TARGETS[0], TEXT[text])\n",
    "    valid_prompts = create_single_property_forward_prompts(test_df, CAT_TARGETS[0], TEXT[text])\n",
    "\n",
    "    filename_base = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "    train_filename = f\"run_files/{filename_base}_train_prompts_molecule_{train_size}.jsonl\"\n",
    "    valid_filename = f\"run_files/{filename_base}_valid_prompts_molecule_{test_size}.jsonl\"\n",
    "    # to save money, just run a small valid frame\n",
    "    valid_small_filename = (\n",
    "        f\"run_files/{filename_base}_validsmall_prompts_molecule_{test_size}.jsonl\"\n",
    "    )\n",
    "    train_prompts.to_json(train_filename, orient=\"records\", lines=True)\n",
    "    valid_prompts.to_json(valid_filename, orient=\"records\", lines=True)\n",
    "    valid_prompts.sample(100).to_json(valid_small_filename, orient=\"records\", lines=True)\n",
    "\n",
    "    fine_tune(train_filename, valid_small_filename)\n",
    "    return train_prompts, valid_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gpt3(model_id, finetune_id, valid_prompts, test_df, max_samples: int = 100):\n",
    "    completions = query_gpt3(model_id, valid_prompts.iloc[:max_samples])\n",
    "    predictions = [int(extract_prediction(completion)) for completion in completions]\n",
    "    true = test_df.iloc[:max_samples][CAT_TARGETS[0]].values\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"gpt-3\",\n",
    "        job_type=\"test-finetuned-model\",\n",
    "        config={\n",
    "            \"train_size\": train_size,\n",
    "            \"test_size\": test_size,\n",
    "            \"features\": FEATURES,\n",
    "            \"targets\": [CAT_TARGETS[0]],\n",
    "            \"ds\": \"molecule\",\n",
    "            \"modelname\": model_id,\n",
    "            \"finetune_run_id\": finetune_id,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    run.log(\n",
    "        {\n",
    "            \"distribution confusion matrix\": wandb.plot.confusion_matrix(\n",
    "                y_true=true,\n",
    "                preds=predictions,\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "    cm = ConfusionMatrix(\n",
    "        true,\n",
    "        predictions,\n",
    "    )\n",
    "\n",
    "    run.log(\n",
    "        {\"ACC\": cm.ACC, \"ACC_macro\": cm.ACC_Macro, \"F1_micro\": cm.F1_Micro, \"F1_macro\": cm.F1_Macro}\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        {\"ACC\": cm.ACC, \"ACC_macro\": cm.ACC_Macro, \"F1_micro\": cm.F1_Micro, \"F1_macro\": cm.F1_Macro}\n",
    "    )\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small data limit ($N = 100$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 train points and 18409 test points\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, train_size=100, stratify=df[CAT_TARGETS[0]])\n",
    "train_size = len(train_df)\n",
    "test_size = len(test_df)\n",
    "print(f\"{len(train_df)} train points and {len(test_df)} test points\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.12.20 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_143137-1npe6jcy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/1npe6jcy\" target=\"_blank\">denim-armadillo-120</a></strong> to <a href=\"https://wandb.ai/kjappelbaum/GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegressionCV was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': {0: 0.8861969688739204, 1: 0.8027051985441903, 2: 0.7447444184909555, 3: 0.761855614101798, 4: 0.8527893964908468}, 'ACC_macro': 0.8096583193003422, 'F1_micro': 0.5241457982508556, 'F1_macro': 0.5179678782595067}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>▁</td></tr><tr><td>F1_macro</td><td>▁</td></tr><tr><td>F1_micro</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>0.80966</td></tr><tr><td>F1_macro</td><td>0.51797</td></tr><tr><td>F1_micro</td><td>0.52415</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">denim-armadillo-120</strong>: <a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/1npe6jcy\" target=\"_blank\">https://wandb.ai/kjappelbaum/GPT-3/runs/1npe6jcy</a><br/>Synced 7 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220703_143137-1npe6jcy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_baseline(train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT3 on SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fine tune prompts for smiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 9.91k/9.91k [00:00<00:00, 4.50Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-03-14-39-19_train_prompts_molecule_100.jsonl: file-PrmjBFTjZeEvYABIV9x22Fbv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 11.1k/11.1k [00:00<00:00, 14.5Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-03-14-39-19_validsmall_prompts_molecule_18409.jsonl: file-A818SFA8AyhMkTXi445mRFdI\n",
      "Created fine-tune: ft-qHqcHsf69wW24yFoXKR1CqyZ\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2022-07-03 14:39:26] Created fine-tune: ft-qHqcHsf69wW24yFoXKR1CqyZ\n",
      "[2022-07-03 14:39:32] Fine-tune costs $0.01\n",
      "[2022-07-03 14:39:33] Fine-tune enqueued. Queue number: 0\n",
      "[2022-07-03 14:39:37] Fine-tune started\n",
      "[2022-07-03 14:40:08] Completed epoch 1/4\n",
      "[2022-07-03 14:40:23] Completed epoch 2/4\n",
      "[2022-07-03 14:40:38] Completed epoch 3/4\n",
      "[2022-07-03 14:40:53] Completed epoch 4/4\n",
      "[2022-07-03 14:41:16] Uploaded model: ada:ft-epfl-2022-07-03-12-41-14\n",
      "[2022-07-03 14:41:19] Uploaded result file: file-K9yKddDBFuiwGVJREkCEcm8A\n",
      "[2022-07-03 14:41:20] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded 🎉\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m ada:ft-epfl-2022-07-03-12-41-14 -p <YOUR_PROMPT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: kjappelbaum. Use `wandb login --relogin` to force relogin\n",
      "wandb: wandb version 0.12.20 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.12.19\n",
      "wandb: Run data is saved locally in /Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_144130-ft-qHqcHsf69wW24yFoXKR1CqyZ\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run ft-qHqcHsf69wW24yFoXKR1CqyZ\n",
      "wandb: ⭐️ View project at https://wandb.ai/kjappelbaum/GPT-3\n",
      "wandb: 🚀 View run at https://wandb.ai/kjappelbaum/GPT-3/runs/ft-qHqcHsf69wW24yFoXKR1CqyZ\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb:                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 wandb sync completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:             elapsed_examples ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "wandb:               elapsed_tokens ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "wandb:                training_loss █▅▃▄▄▂▂▃▄▃▄▂▂▄▂▃▃▂▂▂▁▃▄▂▂▁▂▂▂▂▃▁▂▂▁▂▁▁▁▂\n",
      "wandb:   training_sequence_accuracy ▁▁█▁██▁▁▁▁▁▁█▁█████▁█▁▁▁███████▁████████\n",
      "wandb:      training_token_accuracy ▁▆█▆██▆▆▆▆▆▆█▆█████▆█▆▆▆███████▆████████\n",
      "wandb:              validation_loss █▄▃▄▄▄▂▂▃▃▄▃▂▂▃▃▄▂▂▂▃▂▂▂▂▃▂▃▂▂▃▃▂▁▃▂▂▃▃\n",
      "wandb: validation_sequence_accuracy ▁█▁▁▁▁▁█▁▁▁▁█▁▁█▁▁█▁███▁▁▁▁▁▁█▁▁▁█▁▁██▁\n",
      "wandb:    validation_token_accuracy ▁█▆▆▆▆▆█▆▆▆▆█▆▆█▆▆█▆███▆▆▆▆▆▆█▆▆▆█▆▆██▆\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:             elapsed_examples 402.0\n",
      "wandb:               elapsed_tokens 18842.0\n",
      "wandb:             fine_tuned_model ada:ft-epfl-2022-07-...\n",
      "wandb:                       status succeeded\n",
      "wandb:                training_loss 0.17082\n",
      "wandb:   training_sequence_accuracy 1.0\n",
      "wandb:      training_token_accuracy 1.0\n",
      "wandb:              validation_loss 0.17614\n",
      "wandb: validation_sequence_accuracy 0.0\n",
      "wandb:    validation_token_accuracy 0.66667\n",
      "wandb: \n",
      "wandb: Synced ft-qHqcHsf69wW24yFoXKR1CqyZ: https://wandb.ai/kjappelbaum/GPT-3/runs/ft-qHqcHsf69wW24yFoXKR1CqyZ\n",
      "wandb: Synced 6 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20220703_144130-ft-qHqcHsf69wW24yFoXKR1CqyZ/logs\n"
     ]
    }
   ],
   "source": [
    "train_prompts, valid_prompts = create_prompt_fine_tune(train_df, test_df, text=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.12.20 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_145132-2ltbl0t7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/2ltbl0t7\" target=\"_blank\">happy-rain-124</a></strong> to <a href=\"https://wandb.ai/kjappelbaum/GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': {0: 0.88, 1: 0.79, 2: 0.59, 3: 0.75, 4: 0.79}, 'ACC_macro': 0.76, 'F1_micro': 0.4, 'F1_macro': 0.41704646676486484}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>▁</td></tr><tr><td>F1_macro</td><td>▁</td></tr><tr><td>F1_micro</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>0.76</td></tr><tr><td>F1_macro</td><td>0.41705</td></tr><tr><td>F1_micro</td><td>0.4</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">happy-rain-124</strong>: <a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/2ltbl0t7\" target=\"_blank\">https://wandb.ai/kjappelbaum/GPT-3/runs/2ltbl0t7</a><br/>Synced 7 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220703_145132-2ltbl0t7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_gpt3(\"ada:ft-epfl-2022-07-03-12-41-14\", \"ft-qHqcHsf69wW24yFoXKR1CqyZ\", valid_prompts, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT3 on SELFIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fine tune prompts for selfies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 21.9k/21.9k [00:00<00:00, 13.2Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-03-14-52-24_train_prompts_molecule_100.jsonl: file-7WuTIHOhMIhmtS3NQd4yVYA6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 22.6k/22.6k [00:00<00:00, 33.4Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-03-14-52-24_validsmall_prompts_molecule_18409.jsonl: file-oZIcVqmi5NPNU4gERt4EKbHI\n",
      "Created fine-tune: ft-KSOPxV6MYzu5chJGYdMDfV2j\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2022-07-03 14:52:32] Created fine-tune: ft-KSOPxV6MYzu5chJGYdMDfV2j\n",
      "[2022-07-03 14:52:45] Fine-tune costs $0.02\n",
      "[2022-07-03 14:52:45] Fine-tune enqueued. Queue number: 0\n",
      "[2022-07-03 14:52:48] Fine-tune started\n",
      "[2022-07-03 14:53:19] Completed epoch 1/4\n",
      "[2022-07-03 14:53:35] Completed epoch 2/4\n",
      "[2022-07-03 14:53:50] Completed epoch 3/4\n",
      "[2022-07-03 14:54:05] Completed epoch 4/4\n",
      "[2022-07-03 14:54:26] Uploaded model: ada:ft-epfl-2022-07-03-12-54-24\n",
      "[2022-07-03 14:54:29] Uploaded result file: file-Q9oJJijPyGXbkokfGNAd3k6o\n",
      "[2022-07-03 14:54:30] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded 🎉\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m ada:ft-epfl-2022-07-03-12-54-24 -p <YOUR_PROMPT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: kjappelbaum. Use `wandb login --relogin` to force relogin\n",
      "wandb: wandb version 0.12.20 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.12.19\n",
      "wandb: Run data is saved locally in /Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_145441-ft-KSOPxV6MYzu5chJGYdMDfV2j\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run ft-KSOPxV6MYzu5chJGYdMDfV2j\n",
      "wandb: ⭐️ View project at https://wandb.ai/kjappelbaum/GPT-3\n",
      "wandb: 🚀 View run at https://wandb.ai/kjappelbaum/GPT-3/runs/ft-KSOPxV6MYzu5chJGYdMDfV2j\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb:                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 wandb sync completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:             elapsed_examples ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "wandb:               elapsed_tokens ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "wandb:                training_loss █▃▃▃▄▂▂▂▃▄▂▂▂▃▂▂▃▂▃▁▂▃▂▁▂▂▁▂▂▂▂▁▁▁▂▂▂▂▂▁\n",
      "wandb:   training_sequence_accuracy ▁▁█▁█▁▁▁▁▁▁███▁▁▁▁▁██▁█▁█▁█▁▁█▁█████▁███\n",
      "wandb:      training_token_accuracy ▁▅█▅█▅▅▅▅▅▅███▅▅▅▅▅██▅█▅█▅█▅▅█▅█████▅███\n",
      "wandb:              validation_loss █▄▃▃▄▃▅▃▃▃▃▂▄▃▂▂▂▂▂▃▃▂▂▃▂▂▂▂▂▂▂▂▃▁▂▃▃▂\n",
      "wandb: validation_sequence_accuracy ▁▁▁▁█▁█▁▁██▁▁▁▁█▁███▁█▁▁██▁▁█▁▁▁▁██▁▁█\n",
      "wandb:    validation_token_accuracy ▁▆▆▆█▆█▆▆██▆▆▆▆█▆███▆█▆▆██▆▆█▆▆▆▆██▆▆█\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:             elapsed_examples 401.0\n",
      "wandb:               elapsed_tokens 42801.0\n",
      "wandb:             fine_tuned_model ada:ft-epfl-2022-07-...\n",
      "wandb:                       status succeeded\n",
      "wandb:                training_loss 0.04701\n",
      "wandb:   training_sequence_accuracy 1.0\n",
      "wandb:      training_token_accuracy 1.0\n",
      "wandb:              validation_loss 0.0726\n",
      "wandb: validation_sequence_accuracy 1.0\n",
      "wandb:    validation_token_accuracy 1.0\n",
      "wandb: \n",
      "wandb: Synced ft-KSOPxV6MYzu5chJGYdMDfV2j: https://wandb.ai/kjappelbaum/GPT-3/runs/ft-KSOPxV6MYzu5chJGYdMDfV2j\n",
      "wandb: Synced 6 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20220703_145441-ft-KSOPxV6MYzu5chJGYdMDfV2j/logs\n"
     ]
    }
   ],
   "source": [
    "train_prompts, valid_prompts = create_prompt_fine_tune(train_df, test_df, text=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.12.20 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_150614-2i1tkth6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/2i1tkth6\" target=\"_blank\">crimson-valley-129</a></strong> to <a href=\"https://wandb.ai/kjappelbaum/GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': {0: 0.73, 1: 0.7, 2: 0.73, 3: 0.74, 4: 0.56}, 'ACC_macro': 0.6920000000000001, 'F1_micro': 0.23, 'F1_macro': 0.217015093015093}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>▁</td></tr><tr><td>F1_macro</td><td>▁</td></tr><tr><td>F1_micro</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>0.692</td></tr><tr><td>F1_macro</td><td>0.21702</td></tr><tr><td>F1_micro</td><td>0.23</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">crimson-valley-129</strong>: <a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/2i1tkth6\" target=\"_blank\">https://wandb.ai/kjappelbaum/GPT-3/runs/2i1tkth6</a><br/>Synced 7 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220703_150614-2i1tkth6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_gpt3(\"ada:ft-epfl-2022-07-03-12-54-24\", \"ft-KSOPxV6MYzu5chJGYdMDfV2j\", valid_prompts, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT-3 on names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fine tune prompts for iupac_name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 12.9k/12.9k [00:00<00:00, 6.40Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-04-06-37-58_train_prompts_molecule_100.jsonl: file-oa3j7kpChV2POBalhMHfiJ40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 13.7k/13.7k [00:00<00:00, 20.1Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-04-06-37-58_validsmall_prompts_molecule_18409.jsonl: file-Q6E4sTvlks4JFWAQV21pJqCz\n",
      "Created fine-tune: ft-9Xa22xxtYKI1sF7gs3DVdmxC\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2022-07-04 06:38:07] Created fine-tune: ft-9Xa22xxtYKI1sF7gs3DVdmxC\n",
      "[2022-07-04 06:38:13] Fine-tune costs $0.01\n",
      "[2022-07-04 06:38:14] Fine-tune enqueued. Queue number: 0\n",
      "[2022-07-04 06:38:17] Fine-tune started\n",
      "[2022-07-04 06:38:49] Completed epoch 1/4\n",
      "[2022-07-04 06:39:04] Completed epoch 2/4\n",
      "[2022-07-04 06:39:19] Completed epoch 3/4\n",
      "[2022-07-04 06:39:34] Completed epoch 4/4\n",
      "[2022-07-04 06:39:56] Uploaded model: ada:ft-epfl-2022-07-04-04-39-54\n",
      "[2022-07-04 06:39:59] Uploaded result file: file-AHWlG7oBPPfWWMK0mDCCWGS1\n",
      "[2022-07-04 06:40:00] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded 🎉\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m ada:ft-epfl-2022-07-04-04-39-54 -p <YOUR_PROMPT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: kjappelbaum. Use `wandb login --relogin` to force relogin\n",
      "wandb: wandb version 0.12.20 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.12.19\n",
      "wandb: Run data is saved locally in /Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220704_064014-ft-9Xa22xxtYKI1sF7gs3DVdmxC\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run ft-9Xa22xxtYKI1sF7gs3DVdmxC\n",
      "wandb: ⭐️ View project at https://wandb.ai/kjappelbaum/GPT-3\n",
      "wandb: 🚀 View run at https://wandb.ai/kjappelbaum/GPT-3/runs/ft-9Xa22xxtYKI1sF7gs3DVdmxC\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb:                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 wandb sync completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:             elapsed_examples ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "wandb:               elapsed_tokens ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "wandb:                training_loss ▇▅█▄▄▄▄▃▄▄▄▃▃▂▅▃▃▂▃▃▃▂▂▂▂▂▃▃▃▃▂▁▂▁▂▂▂▁▁▁\n",
      "wandb:   training_sequence_accuracy ▁▁▁▁▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁█▁▁██▁█▁▁██▁██▁████\n",
      "wandb:      training_token_accuracy ▁▅▅▅▅▅▅▅▅▅█▅▅█▅▅▅▅▅▅▅█▅▅██▅█▅▅██▅██▅████\n",
      "wandb:              validation_loss █▃▃▆▂▃▂▂▃▅▃▃▂▂▂▅▂▂▂▂▃▁▂▂▄▆▄▃▁▃▂▁▂▂▃▂▃▃\n",
      "wandb: validation_sequence_accuracy ▁▁▁▁▁▁▁█▁▁▁▁███▁▁▁███▁▁▁▁▁▁▁█▁▁██▁▁▁▁█\n",
      "wandb:    validation_token_accuracy ▁▆▆▆▆▆▆█▆▆▆▆███▆▆▆███▆▆▆▆▆▆▆█▆▆██▆▆▆▆█\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:             elapsed_examples 401.0\n",
      "wandb:               elapsed_tokens 21665.0\n",
      "wandb:             fine_tuned_model ada:ft-epfl-2022-07-...\n",
      "wandb:                       status succeeded\n",
      "wandb:                training_loss 0.11967\n",
      "wandb:   training_sequence_accuracy 0.0\n",
      "wandb:      training_token_accuracy 0.66667\n",
      "wandb:              validation_loss 0.17705\n",
      "wandb: validation_sequence_accuracy 1.0\n",
      "wandb:    validation_token_accuracy 1.0\n",
      "wandb: \n",
      "wandb: Synced ft-9Xa22xxtYKI1sF7gs3DVdmxC: https://wandb.ai/kjappelbaum/GPT-3/runs/ft-9Xa22xxtYKI1sF7gs3DVdmxC\n",
      "wandb: Synced 6 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20220704_064014-ft-9Xa22xxtYKI1sF7gs3DVdmxC/logs\n"
     ]
    }
   ],
   "source": [
    "train_prompts, valid_prompts = create_prompt_fine_tune(train_df, test_df, text=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkjappelbaum\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.20 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220704_065121-3vtqlunq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/3vtqlunq\" target=\"_blank\">pleasant-planet-160</a></strong> to <a href=\"https://wandb.ai/kjappelbaum/GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': {0: 0.82, 1: 0.7, 2: 0.75, 3: 0.69, 4: 0.84}, 'ACC_macro': 0.76, 'F1_micro': 0.4, 'F1_macro': 0.39273675934735647}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>▁</td></tr><tr><td>F1_macro</td><td>▁</td></tr><tr><td>F1_micro</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>0.76</td></tr><tr><td>F1_macro</td><td>0.39274</td></tr><tr><td>F1_micro</td><td>0.4</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pleasant-planet-160</strong>: <a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/3vtqlunq\" target=\"_blank\">https://wandb.ai/kjappelbaum/GPT-3/runs/3vtqlunq</a><br/>Synced 7 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220704_065121-3vtqlunq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_gpt3(\"ada:ft-epfl-2022-07-04-04-39-54\", \"ft-9Xa22xxtYKI1sF7gs3DVdmxC\", valid_prompts, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medium size dataset ($N=500$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 train points and 18009 test points\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, train_size=500, stratify=df[CAT_TARGETS[0]])\n",
    "train_size = len(train_df)\n",
    "test_size = len(test_df)\n",
    "print(f\"{len(train_df)} train points and {len(test_df)} test points\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkjappelbaum\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.20 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_153441-3m8htstc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/3m8htstc\" target=\"_blank\">resilient-wildflower-133</a></strong> to <a href=\"https://wandb.ai/kjappelbaum/GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegressionCV was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': {0: 0.9411405408406908, 1: 0.8687323005164085, 2: 0.8462990726858792, 3: 0.855072463768116, 4: 0.9262590926758842}, 'ACC_macro': 0.8875006940973957, 'F1_micro': 0.7187517352434893, 'F1_macro': 0.7198819297229052}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>▁</td></tr><tr><td>F1_macro</td><td>▁</td></tr><tr><td>F1_micro</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>0.8875</td></tr><tr><td>F1_macro</td><td>0.71988</td></tr><tr><td>F1_micro</td><td>0.71875</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">resilient-wildflower-133</strong>: <a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/3m8htstc\" target=\"_blank\">https://wandb.ai/kjappelbaum/GPT-3/runs/3m8htstc</a><br/>Synced 7 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220703_153441-3m8htstc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_baseline(train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3 on SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fine tune prompts for smiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 52.5k/52.5k [00:00<00:00, 27.9Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-03-15-40-58_train_prompts_molecule_500.jsonl: file-UpKzwSTMZ92uKyqAeCys2qjY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 10.4k/10.4k [00:00<00:00, 19.5Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-03-15-40-58_validsmall_prompts_molecule_18009.jsonl: file-lEex9MuVk41n2pKhPakEE3w2\n",
      "Created fine-tune: ft-SYoz2NDeVkcnlrr4x8qrLNzD\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2022-07-03 15:41:07] Created fine-tune: ft-SYoz2NDeVkcnlrr4x8qrLNzD\n",
      "[2022-07-03 15:41:12] Fine-tune costs $0.04\n",
      "[2022-07-03 15:41:12] Fine-tune enqueued. Queue number: 0\n",
      "[2022-07-03 15:41:15] Fine-tune started\n",
      "[2022-07-03 15:42:42] Completed epoch 1/4\n",
      "[2022-07-03 15:43:54] Completed epoch 2/4\n",
      "[2022-07-03 15:45:07] Completed epoch 3/4\n",
      "[2022-07-03 15:46:20] Completed epoch 4/4\n",
      "[2022-07-03 15:46:45] Uploaded model: ada:ft-epfl-2022-07-03-13-46-43\n",
      "[2022-07-03 15:46:48] Uploaded result file: file-2SZYAFeSu9lkBsJMbgqIN9jZ\n",
      "[2022-07-03 15:46:48] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded 🎉\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m ada:ft-epfl-2022-07-03-13-46-43 -p <YOUR_PROMPT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: kjappelbaum. Use `wandb login --relogin` to force relogin\n",
      "wandb: wandb version 0.12.20 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.12.19\n",
      "wandb: Run data is saved locally in /Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_154700-ft-SYoz2NDeVkcnlrr4x8qrLNzD\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run ft-SYoz2NDeVkcnlrr4x8qrLNzD\n",
      "wandb: ⭐️ View project at https://wandb.ai/kjappelbaum/GPT-3\n",
      "wandb: 🚀 View run at https://wandb.ai/kjappelbaum/GPT-3/runs/ft-SYoz2NDeVkcnlrr4x8qrLNzD\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:             elapsed_examples ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "wandb:               elapsed_tokens ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "wandb:                training_loss ▇█▄▄▄▅▃▄▂▅█▃▄▃▆▄▆▅▅▃▃▃▃▃▃▃▂▃▃▂▂▄▂▃▂▃▂▄▁▂\n",
      "wandb:   training_sequence_accuracy █▁███████▁▁███▁▁▁████▁▁████▁███▁█████▁█▁\n",
      "wandb:      training_token_accuracy █▁███████▁▁███▁▁▁████▁▁████▁███▁█████▁█▁\n",
      "wandb:              validation_loss █▃▃▃▃▂▂▂▃▂▂▂▃▂▃▃▃▂▂▃▂▁▂▃▃▃▁▁▂▂▁▁▂▁▃▁▂▁▂▁\n",
      "wandb: validation_sequence_accuracy ▁▁█▁▁█▁█▁█▁▁▁▁▁█▁▁█▁███▁▁▁█████▁██▁█████\n",
      "wandb:    validation_token_accuracy ▁▆█▆▆█▆█▆█▆▆▆▆▆█▆▆█▆███▆▆▆█████▆██▆█████\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:             elapsed_examples 2001.0\n",
      "wandb:               elapsed_tokens 104009.0\n",
      "wandb:             fine_tuned_model ada:ft-epfl-2022-07-...\n",
      "wandb:                       status succeeded\n",
      "wandb:                training_loss 0.06983\n",
      "wandb:   training_sequence_accuracy 1.0\n",
      "wandb:      training_token_accuracy 1.0\n",
      "wandb:              validation_loss 0.06965\n",
      "wandb: validation_sequence_accuracy 1.0\n",
      "wandb:    validation_token_accuracy 1.0\n",
      "wandb: \n",
      "wandb: Synced ft-SYoz2NDeVkcnlrr4x8qrLNzD: https://wandb.ai/kjappelbaum/GPT-3/runs/ft-SYoz2NDeVkcnlrr4x8qrLNzD\n",
      "wandb: Synced 6 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20220703_154700-ft-SYoz2NDeVkcnlrr4x8qrLNzD/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 wandb sync completed successfully\n"
     ]
    }
   ],
   "source": [
    "train_prompts, valid_prompts = create_prompt_fine_tune(train_df, test_df, text=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.12.20 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_155715-3ks808b6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/3ks808b6\" target=\"_blank\">bumbling-eon-135</a></strong> to <a href=\"https://wandb.ai/kjappelbaum/GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': {0: 0.9, 1: 0.87, 2: 0.84, 3: 0.87, 4: 0.92}, 'ACC_macro': 0.8800000000000001, 'F1_micro': 0.7, 'F1_macro': 0.6967912860595787}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>▁</td></tr><tr><td>F1_macro</td><td>▁</td></tr><tr><td>F1_micro</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>0.88</td></tr><tr><td>F1_macro</td><td>0.69679</td></tr><tr><td>F1_micro</td><td>0.7</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">bumbling-eon-135</strong>: <a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/3ks808b6\" target=\"_blank\">https://wandb.ai/kjappelbaum/GPT-3/runs/3ks808b6</a><br/>Synced 7 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220703_155715-3ks808b6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_gpt3(\"ada:ft-epfl-2022-07-03-13-46-43\", \"ft-SYoz2NDeVkcnlrr4x8qrLNzD\", valid_prompts, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3 on SELFIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fine tune prompts for selfies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 120k/120k [00:00<00:00, 72.5Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-03-15-57-57_train_prompts_molecule_500.jsonl: file-O8fl0nF9ob6gYN1wxNnZ7d4y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 23.6k/23.6k [00:00<00:00, 46.0Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-03-15-57-57_validsmall_prompts_molecule_18009.jsonl: file-odcg4f4IW0be3IiDu31YBdJe\n",
      "Created fine-tune: ft-pmx4lEQJw1L3rPR1PlHUzGXA\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2022-07-03 15:58:07] Created fine-tune: ft-pmx4lEQJw1L3rPR1PlHUzGXA\n",
      "[2022-07-03 15:58:17] Fine-tune costs $0.09\n",
      "[2022-07-03 15:58:17] Fine-tune enqueued. Queue number: 0\n",
      "[2022-07-03 15:58:19] Fine-tune started\n",
      "[2022-07-03 15:59:45] Completed epoch 1/4\n",
      "[2022-07-03 16:00:57] Completed epoch 2/4\n",
      "[2022-07-03 16:02:08] Completed epoch 3/4\n",
      "[2022-07-03 16:03:19] Completed epoch 4/4\n",
      "[2022-07-03 16:03:39] Uploaded model: ada:ft-epfl-2022-07-03-14-03-37\n",
      "[2022-07-03 16:03:42] Uploaded result file: file-ptKx3VPOkc9fhDIFJrfR7PkX\n",
      "[2022-07-03 16:03:43] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded 🎉\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m ada:ft-epfl-2022-07-03-14-03-37 -p <YOUR_PROMPT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: kjappelbaum. Use `wandb login --relogin` to force relogin\n",
      "wandb: wandb version 0.12.20 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.12.19\n",
      "wandb: Run data is saved locally in /Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_160403-ft-pmx4lEQJw1L3rPR1PlHUzGXA\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run ft-pmx4lEQJw1L3rPR1PlHUzGXA\n",
      "wandb: ⭐️ View project at https://wandb.ai/kjappelbaum/GPT-3\n",
      "wandb: 🚀 View run at https://wandb.ai/kjappelbaum/GPT-3/runs/ft-pmx4lEQJw1L3rPR1PlHUzGXA\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb:                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 wandb sync completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:             elapsed_examples ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "wandb:               elapsed_tokens ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "wandb:                training_loss █▆▅▅▅▅▄▄▅▃█▄▃▃▄▃▄▃▄▂▂▃▃▂▃▃▃▃▄▁▂▂▁▁▂▃▂▂▂▂\n",
      "wandb:   training_sequence_accuracy ▁█▁▁▁▁▁▁▁▁▁▁██▁█▁▁▁██▁█████▁▁█████▁▁████\n",
      "wandb:      training_token_accuracy ▁█▁▁▁▁▁▁▁▁▁▁██▁█▁▁▁██▁█████▁▁█████▁▁████\n",
      "wandb:              validation_loss █▂▂▂▄▂▂▂▂▂▂▂▂▂▂▂▄▂▁▂▁▂▂▁▂▁▃▂▂▂▁▂▁▂▂▁▁▂▁▁\n",
      "wandb: validation_sequence_accuracy ▁▁▁█▁█▁▁▁█▁▁▁▁██▁▁▁▁█▁▁▁████████▁▁█▁████\n",
      "wandb:    validation_token_accuracy ▁▆▆█▆█▆▆▆█▆▆▆▆██▆▆▆▆█▆▆▆████████▆▆█▆████\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:             elapsed_examples 2002.0\n",
      "wandb:               elapsed_tokens 236098.0\n",
      "wandb:             fine_tuned_model ada:ft-epfl-2022-07-...\n",
      "wandb:                       status succeeded\n",
      "wandb:                training_loss 0.02437\n",
      "wandb:   training_sequence_accuracy 1.0\n",
      "wandb:      training_token_accuracy 1.0\n",
      "wandb:              validation_loss 0.05212\n",
      "wandb: validation_sequence_accuracy 1.0\n",
      "wandb:    validation_token_accuracy 1.0\n",
      "wandb: \n",
      "wandb: Synced ft-pmx4lEQJw1L3rPR1PlHUzGXA: https://wandb.ai/kjappelbaum/GPT-3/runs/ft-pmx4lEQJw1L3rPR1PlHUzGXA\n",
      "wandb: Synced 6 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20220703_160403-ft-pmx4lEQJw1L3rPR1PlHUzGXA/logs\n"
     ]
    }
   ],
   "source": [
    "train_prompts, valid_prompts = create_prompt_fine_tune(train_df, test_df, text=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.12.20 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_172642-16gw85jd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/16gw85jd\" target=\"_blank\">blooming-fog-138</a></strong> to <a href=\"https://wandb.ai/kjappelbaum/GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': {0: 0.91, 1: 0.85, 2: 0.82, 3: 0.83, 4: 0.93}, 'ACC_macro': 0.868, 'F1_micro': 0.67, 'F1_macro': 0.6703943460103294}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>▁</td></tr><tr><td>F1_macro</td><td>▁</td></tr><tr><td>F1_micro</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>0.868</td></tr><tr><td>F1_macro</td><td>0.67039</td></tr><tr><td>F1_micro</td><td>0.67</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">blooming-fog-138</strong>: <a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/16gw85jd\" target=\"_blank\">https://wandb.ai/kjappelbaum/GPT-3/runs/16gw85jd</a><br/>Synced 7 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220703_172642-16gw85jd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_gpt3(\"ada:ft-epfl-2022-07-03-14-03-37\", \"ft-pmx4lEQJw1L3rPR1PlHUzGXA\", valid_prompts, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3 on name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fine tune prompts for iupac_name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 65.3k/65.3k [00:00<00:00, 43.3Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-03-17-29-40_train_prompts_molecule_500.jsonl: file-KVj1DrSvO59plBgUnpKzGEur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 13.3k/13.3k [00:00<00:00, 22.7Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-03-17-29-40_validsmall_prompts_molecule_18009.jsonl: file-Ep30TnoK6PqKqgrbGN9s5SIV\n",
      "Created fine-tune: ft-qaS5HnCXf2HALlhjhBclqtDC\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2022-07-03 17:29:48] Created fine-tune: ft-qaS5HnCXf2HALlhjhBclqtDC\n",
      "[2022-07-03 17:29:57] Fine-tune costs $0.04\n",
      "[2022-07-03 17:29:58] Fine-tune enqueued. Queue number: 0\n",
      "[2022-07-03 17:30:04] Fine-tune started\n",
      "[2022-07-03 17:31:30] Completed epoch 1/4\n",
      "[2022-07-03 17:32:40] Completed epoch 2/4\n",
      "[2022-07-03 17:33:49] Completed epoch 3/4\n",
      "[2022-07-03 17:34:59] Completed epoch 4/4\n",
      "[2022-07-03 17:35:25] Uploaded model: ada:ft-epfl-2022-07-03-15-35-23\n",
      "[2022-07-03 17:35:28] Uploaded result file: file-gwW2lrlbfkPTuVEQ3G8gzh0O\n",
      "[2022-07-03 17:35:28] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded 🎉\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m ada:ft-epfl-2022-07-03-15-35-23 -p <YOUR_PROMPT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: kjappelbaum. Use `wandb login --relogin` to force relogin\n",
      "wandb: wandb version 0.12.20 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.12.19\n",
      "wandb: Run data is saved locally in /Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_173542-ft-qaS5HnCXf2HALlhjhBclqtDC\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run ft-qaS5HnCXf2HALlhjhBclqtDC\n",
      "wandb: ⭐️ View project at https://wandb.ai/kjappelbaum/GPT-3\n",
      "wandb: 🚀 View run at https://wandb.ai/kjappelbaum/GPT-3/runs/ft-qaS5HnCXf2HALlhjhBclqtDC\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb:                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 wandb sync completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:             elapsed_examples ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "wandb:               elapsed_tokens ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "wandb:                training_loss ▇▅▄▄▄▅▃▇▄█▅▂▂▂▄▄▃▄▃▅▂▂▃▃▂▂▃▂▁▃▂▅▁▃▂▂▂▃▁▃\n",
      "wandb:   training_sequence_accuracy █████▁▁▁█▁▁████▁▁▁█▁█▁▁▁███▁█████████▁█▁\n",
      "wandb:      training_token_accuracy █████▁▁▁█▁▁████▁▁▁█▁█▁▁▁███▁█████████▁█▁\n",
      "wandb:              validation_loss █▂▁▂▁▁▂▁▂▂▁▁▁▁▂▂▂▁▂▂▁▁▂▂▁▁▁▁▁▂▂▁▁▃▁▁▂▃▂▂\n",
      "wandb: validation_sequence_accuracy ▁▁█▁▁█▁▁▁▁█▁▁▁▁▁█▁▁▁███▁█████▁▁██▁██▁▁▁█\n",
      "wandb:    validation_token_accuracy ▁▆█▆▆█▆▆▆▆█▆▆▆▆▆█▆▆▆███▆█████▆▆██▆██▆▆▆█\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:             elapsed_examples 2002.0\n",
      "wandb:               elapsed_tokens 107578.0\n",
      "wandb:             fine_tuned_model ada:ft-epfl-2022-07-...\n",
      "wandb:                       status succeeded\n",
      "wandb:                training_loss 0.08192\n",
      "wandb:   training_sequence_accuracy 1.0\n",
      "wandb:      training_token_accuracy 1.0\n",
      "wandb:              validation_loss 0.15099\n",
      "wandb: validation_sequence_accuracy 1.0\n",
      "wandb:    validation_token_accuracy 1.0\n",
      "wandb: \n",
      "wandb: Synced ft-qaS5HnCXf2HALlhjhBclqtDC: https://wandb.ai/kjappelbaum/GPT-3/runs/ft-qaS5HnCXf2HALlhjhBclqtDC\n",
      "wandb: Synced 6 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20220703_173542-ft-qaS5HnCXf2HALlhjhBclqtDC/logs\n"
     ]
    }
   ],
   "source": [
    "train_prompts, valid_prompts = create_prompt_fine_tune(train_df, test_df, text=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.12.20 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_182137-1r2p1cdc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/1r2p1cdc\" target=\"_blank\">comic-wind-143</a></strong> to <a href=\"https://wandb.ai/kjappelbaum/GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': {0: 0.885, 1: 0.75, 2: 0.72, 3: 0.73, 4: 0.845}, 'ACC_macro': 0.7859999999999999, 'F1_micro': 0.465, 'F1_macro': 0.45763262770109636}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>▁</td></tr><tr><td>F1_macro</td><td>▁</td></tr><tr><td>F1_micro</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>0.786</td></tr><tr><td>F1_macro</td><td>0.45763</td></tr><tr><td>F1_micro</td><td>0.465</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">comic-wind-143</strong>: <a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/1r2p1cdc\" target=\"_blank\">https://wandb.ai/kjappelbaum/GPT-3/runs/1r2p1cdc</a><br/>Synced 7 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220703_182137-1r2p1cdc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_gpt3(\"ada:ft-epfl-2022-07-03-15-35-23\", \"ft-qaS5HnCXf2HALlhjhBclqtDC\", valid_prompts, test_df, max_samples=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Large train set ($N=2000$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 train points and 16509 test points\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, train_size=2000, stratify=df[CAT_TARGETS[0]])\n",
    "train_size = len(train_df)\n",
    "test_size = len(test_df)\n",
    "print(f\"{len(train_df)} train points and {len(test_df)} test points\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_185910-2pucooez</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/2pucooez\" target=\"_blank\">dazzling-firebrand-148</a></strong> to <a href=\"https://wandb.ai/kjappelbaum/GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegressionCV was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': {0: 0.9629898843055303, 1: 0.9213156460112666, 2: 0.9194984553879701, 3: 0.9308861832939609, 4: 0.9665636925313466}, 'ACC_macro': 0.9402507723060148, 'F1_micro': 0.8506269307650373, 'F1_macro': 0.8510644120566495}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>▁</td></tr><tr><td>F1_macro</td><td>▁</td></tr><tr><td>F1_micro</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>0.94025</td></tr><tr><td>F1_macro</td><td>0.85106</td></tr><tr><td>F1_micro</td><td>0.85063</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dazzling-firebrand-148</strong>: <a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/2pucooez\" target=\"_blank\">https://wandb.ai/kjappelbaum/GPT-3/runs/2pucooez</a><br/>Synced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220703_185910-2pucooez/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_baseline(train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune on SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fine tune prompts for smiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 216k/216k [00:00<00:00, 120Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-03-19-53-03_train_prompts_molecule_2000.jsonl: file-dVryjqLtI4bTi2IzugGeOVA7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 11.2k/11.2k [00:00<00:00, 18.1Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-03-19-53-03_validsmall_prompts_molecule_16509.jsonl: file-jvcaIQJBgKmv3OJYq6T7EgGR\n",
      "Created fine-tune: ft-asLUdFrceroByuDJmemXnUm6\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2022-07-03 19:53:15] Created fine-tune: ft-asLUdFrceroByuDJmemXnUm6\n",
      "[2022-07-03 19:53:25] Fine-tune costs $0.16\n",
      "[2022-07-03 19:53:26] Fine-tune enqueued. Queue number: 0\n",
      "[2022-07-03 19:53:28] Fine-tune started\n",
      "[2022-07-03 19:56:47] Completed epoch 1/4\n",
      "[2022-07-03 19:59:49] Completed epoch 2/4\n",
      "[2022-07-03 20:02:52] Completed epoch 3/4\n",
      "[2022-07-03 20:05:55] Completed epoch 4/4\n",
      "[2022-07-03 20:06:21] Uploaded model: ada:ft-epfl-2022-07-03-18-06-20\n",
      "[2022-07-03 20:06:24] Uploaded result file: file-XcdLDGRcgJvD83aFMzFN6Wyy\n",
      "[2022-07-03 20:06:25] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded 🎉\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m ada:ft-epfl-2022-07-03-18-06-20 -p <YOUR_PROMPT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: kjappelbaum. Use `wandb login --relogin` to force relogin\n",
      "wandb: wandb version 0.12.20 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.12.19\n",
      "wandb: Run data is saved locally in /Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_200645-ft-asLUdFrceroByuDJmemXnUm6\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run ft-asLUdFrceroByuDJmemXnUm6\n",
      "wandb: ⭐️ View project at https://wandb.ai/kjappelbaum/GPT-3\n",
      "wandb: 🚀 View run at https://wandb.ai/kjappelbaum/GPT-3/runs/ft-asLUdFrceroByuDJmemXnUm6\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb:                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 wandb sync completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:             elapsed_examples ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "wandb:               elapsed_tokens ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "wandb:                training_loss ▃▄▄▃▂▆▃▃▃▅▃▂▃█▃▃▃▃▃▅▃▁▂▃▃▃▃▃▄▂▁▃▄▃▃▂▂▂▂▃\n",
      "wandb:   training_sequence_accuracy ▃▅▅▁▅▃▆▅▃▆▅▆▆▃▁█▃▆█▆▅▆█▆▃▆▅██▅██▆▆▃▆▅██▅\n",
      "wandb:      training_token_accuracy ▃▄▄▁▄▃▆▄▃▆▄▆▆▃▁█▃▆█▆▄▆█▆▃▆▄██▄██▆▆▃▆▄██▄\n",
      "wandb:              validation_loss █▃▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂\n",
      "wandb: validation_sequence_accuracy ▁▃▅█▆█▅▅▆▃▅▅▃▅█▆▆▆▃▆█▅▆▅▆▅▆▆▆▅█▅▃▃▅▆▆▃▅█\n",
      "wandb:    validation_token_accuracy ▁▆▇█▇█▇▇▇▆▇▇▆▇█▇▇▇▆▇█▇▇▇▇▇▇▇▇▇█▇▆▆▇▇▇▆▇█\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:             elapsed_examples 8008.0\n",
      "wandb:               elapsed_tokens 755368.0\n",
      "wandb:             fine_tuned_model ada:ft-epfl-2022-07-...\n",
      "wandb:                       status succeeded\n",
      "wandb:                training_loss 0.03892\n",
      "wandb:   training_sequence_accuracy 1.0\n",
      "wandb:      training_token_accuracy 1.0\n",
      "wandb:              validation_loss 0.05615\n",
      "wandb: validation_sequence_accuracy 1.0\n",
      "wandb:    validation_token_accuracy 1.0\n",
      "wandb: \n",
      "wandb: Synced ft-asLUdFrceroByuDJmemXnUm6: https://wandb.ai/kjappelbaum/GPT-3/runs/ft-asLUdFrceroByuDJmemXnUm6\n",
      "wandb: Synced 6 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20220703_200645-ft-asLUdFrceroByuDJmemXnUm6/logs\n"
     ]
    }
   ],
   "source": [
    "train_prompts, valid_prompts = create_prompt_fine_tune(train_df, test_df, text=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkjappelbaum\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.20 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_202524-2zlpkn57</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/2zlpkn57\" target=\"_blank\">denim-meadow-152</a></strong> to <a href=\"https://wandb.ai/kjappelbaum/GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': {0: 0.96, 1: 0.885, 2: 0.85, 3: 0.855, 4: 0.9}, 'ACC_macro': 0.89, 'F1_micro': 0.725, 'F1_macro': 0.719282006837113}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>▁</td></tr><tr><td>F1_macro</td><td>▁</td></tr><tr><td>F1_micro</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>0.89</td></tr><tr><td>F1_macro</td><td>0.71928</td></tr><tr><td>F1_micro</td><td>0.725</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">denim-meadow-152</strong>: <a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/2zlpkn57\" target=\"_blank\">https://wandb.ai/kjappelbaum/GPT-3/runs/2zlpkn57</a><br/>Synced 7 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220703_202524-2zlpkn57/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_gpt3(\"ada:ft-epfl-2022-07-03-18-06-20\", \"ft-asLUdFrceroByuDJmemXnUm6\", valid_prompts, test_df, max_samples=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune on SELFIEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fine tune prompts for selfies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 499k/499k [00:00<00:00, 312Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-03-21-30-15_train_prompts_molecule_2000.jsonl: file-GeYUxJPeQVqH1lsjCP9qhPdH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 25.2k/25.2k [00:00<00:00, 45.0Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-03-21-30-15_validsmall_prompts_molecule_16509.jsonl: file-t1xe9Bh1wjYNjtAi8rxrkkK3\n",
      "Created fine-tune: ft-6x5ZSdFqKldtSOJN8L4JlfQ4\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2022-07-03 21:30:24] Created fine-tune: ft-6x5ZSdFqKldtSOJN8L4JlfQ4\n",
      "[2022-07-03 21:30:30] Fine-tune costs $0.38\n",
      "[2022-07-03 21:30:30] Fine-tune enqueued. Queue number: 0\n",
      "[2022-07-03 21:30:34] Fine-tune started\n",
      "[2022-07-03 21:33:56] Completed epoch 1/4\n",
      "[2022-07-03 21:37:02] Completed epoch 2/4\n",
      "[2022-07-03 21:40:10] Completed epoch 3/4\n",
      "[2022-07-03 21:43:14] Completed epoch 4/4\n",
      "[2022-07-03 21:43:37] Uploaded model: ada:ft-epfl-2022-07-03-19-43-35\n",
      "[2022-07-03 21:43:41] Uploaded result file: file-syLXukeauGZtQpbwKzFiCxdP\n",
      "[2022-07-03 21:43:41] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded 🎉\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m ada:ft-epfl-2022-07-03-19-43-35 -p <YOUR_PROMPT>\n",
      "Fine-tune ft-PHRaY9GXvZn8STWIAGvLh0M9 has the status \"pending\" and will not be logged\n",
      "🎉 wandb sync completed successfully\n"
     ]
    }
   ],
   "source": [
    "train_prompts, valid_prompts = create_prompt_fine_tune(train_df, test_df, text=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.12.20 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_222922-2kgzpisf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/2kgzpisf\" target=\"_blank\">clean-durian-154</a></strong> to <a href=\"https://wandb.ai/kjappelbaum/GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': {0: 0.95, 1: 0.855, 2: 0.84, 3: 0.835, 4: 0.91}, 'ACC_macro': 0.8779999999999999, 'F1_micro': 0.695, 'F1_macro': 0.6902066086243617}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>▁</td></tr><tr><td>F1_macro</td><td>▁</td></tr><tr><td>F1_micro</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>0.878</td></tr><tr><td>F1_macro</td><td>0.69021</td></tr><tr><td>F1_micro</td><td>0.695</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">clean-durian-154</strong>: <a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/2kgzpisf\" target=\"_blank\">https://wandb.ai/kjappelbaum/GPT-3/runs/2kgzpisf</a><br/>Synced 7 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220703_222922-2kgzpisf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_gpt3(\"ada:ft-epfl-2022-07-03-19-43-35\", \"ft-6x5ZSdFqKldtSOJN8L4JlfQ4\", valid_prompts, test_df, max_samples=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune on chemical name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fine tune prompts for iupac_name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 271k/271k [00:00<00:00, 105Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-03-22-30-58_train_prompts_molecule_2000.jsonl: file-YLYPRIXt6c4putPYfUW9y9UX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 13.3k/13.3k [00:00<00:00, 25.1Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from run_files/2022-07-03-22-30-58_validsmall_prompts_molecule_16509.jsonl: file-dVTOz1B4ZrDROlXUlcIIBOF4\n",
      "Created fine-tune: ft-DnPs5LPxYEX7ChafDaQgropw\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2022-07-03 22:31:07] Created fine-tune: ft-DnPs5LPxYEX7ChafDaQgropw\n",
      "[2022-07-03 22:31:19] Fine-tune costs $0.17\n",
      "[2022-07-03 22:31:19] Fine-tune enqueued. Queue number: 0\n",
      "[2022-07-03 22:31:23] Fine-tune started\n",
      "[2022-07-03 22:34:40] Completed epoch 1/4\n",
      "[2022-07-03 22:37:41] Completed epoch 2/4\n",
      "[2022-07-03 22:40:42] Completed epoch 3/4\n",
      "[2022-07-03 22:43:44] Completed epoch 4/4\n",
      "[2022-07-03 22:44:06] Uploaded model: ada:ft-epfl-2022-07-03-20-44-04\n",
      "[2022-07-03 22:44:09] Uploaded result file: file-RPFWnN6tbnsd84gGvLV62Gg0\n",
      "[2022-07-03 22:44:10] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded 🎉\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m ada:ft-epfl-2022-07-03-20-44-04 -p <YOUR_PROMPT>\n",
      "Fine-tune ft-K30MLnN9SerMQlSWTCeKpTJP has the status \"pending\" and will not be logged\n",
      "🎉 wandb sync completed successfully\n"
     ]
    }
   ],
   "source": [
    "train_prompts, valid_prompts = create_prompt_fine_tune(train_df, test_df, text=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/experiments/molecules/wandb/run-20220703_124734-1ax55w15</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/1ax55w15\" target=\"_blank\">bumbling-shadow-116</a></strong> to <a href=\"https://wandb.ai/kjappelbaum/GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': {0: 0.935, 1: 0.86, 2: 0.825, 3: 0.86, 4: 0.93}, 'ACC_macro': 0.882, 'F1_micro': 0.705, 'F1_macro': 0.6999996459233866}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>▁</td></tr><tr><td>F1_macro</td><td>▁</td></tr><tr><td>F1_micro</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ACC_macro</td><td>0.882</td></tr><tr><td>F1_macro</td><td>0.7</td></tr><tr><td>F1_micro</td><td>0.705</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">bumbling-shadow-116</strong>: <a href=\"https://wandb.ai/kjappelbaum/GPT-3/runs/1ax55w15\" target=\"_blank\">https://wandb.ai/kjappelbaum/GPT-3/runs/1ax55w15</a><br/>Synced 7 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220703_124734-1ax55w15/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_gpt3(\"ada:ft-epfl-2022-07-03-10-09-06\", \"ft-kTPe2DuwB9twH9wsPpBc2cDi\", valid_prompts, test_df, max_samples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a4fa60962de90e73b5da8d67a44b01d2de04630d82b94b8db1f727a73d31e61"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpt3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
