{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp api_wrappers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0x10 but this version of numpy is 0xf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/notebooks/01_api_wrappers.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/notebooks/01_api_wrappers.ipynb#ch0000002?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenai\u001b[39;00m \u001b[39mimport\u001b[39;00m FineTune\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/notebooks/01_api_wrappers.ipynb#ch0000002?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpycm\u001b[39;00m \u001b[39mimport\u001b[39;00m ConfusionMatrix\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/notebooks/01_api_wrappers.ipynb#ch0000002?line=16'>17</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt3/lib/python3.9/site-packages/sklearn/__init__.py:82\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _distributor_init  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __check_build  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[1;32m     83\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[1;32m     85\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     86\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     87\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    129\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt3/lib/python3.9/site-packages/sklearn/base.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_32BIT\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_tags\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     _DEFAULT_TAGS,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m check_X_y\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt3/lib/python3.9/site-packages/sklearn/utils/__init__.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DataConversionWarning\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_version, threadpool_info\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     as_float_array,\n\u001b[1;32m     33\u001b[0m     assert_all_finite,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     check_scalar,\n\u001b[1;32m     42\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt3/lib/python3.9/site-packages/sklearn/utils/fixes.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mthreadpoolctl\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m config_context, get_config\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt3/lib/python3.9/site-packages/scipy/stats/__init__.py:467\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    462\u001b[0m \n\u001b[1;32m    463\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    466\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 467\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    468\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[1;32m    469\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt3/lib/python3.9/site-packages/scipy/stats/_stats_py.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m array, asarray, ma\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m \u001b[39mimport\u001b[39;00m NumpyVersion\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mimport\u001b[39;00m suppress_warnings\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspatial\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistance\u001b[39;00m \u001b[39mimport\u001b[39;00m cdist\n\u001b[1;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mndimage\u001b[39;00m \u001b[39mimport\u001b[39;00m _measurements\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt3/lib/python3.9/site-packages/numpy/testing/__init__.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"Common test support for all numpy test scripts.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39mThis single module should provide all the common functionality for numpy tests\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39munittest\u001b[39;00m \u001b[39mimport\u001b[39;00m TestCase\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_private\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_private\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (_assert_valid_refcount, _gen_alignment_data)\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_private\u001b[39;00m \u001b[39mimport\u001b[39;00m extbuild, decorators \u001b[39mas\u001b[39;00m dec\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt3/lib/python3.9/site-packages/numpy/testing/_private/utils.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m(\n\u001b[1;32m     22\u001b[0m      intp, float32, empty, arange, array_repr, ndarray, isnat, array)\n\u001b[0;32m---> 23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlapack_lite\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m StringIO\n\u001b[1;32m     27\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     28\u001b[0m         \u001b[39m'\u001b[39m\u001b[39massert_equal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39massert_almost_equal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39massert_approx_equal\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     29\u001b[0m         \u001b[39m'\u001b[39m\u001b[39massert_array_equal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39massert_array_less\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39massert_string_equal\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[39m'\u001b[39m\u001b[39massert_no_gc_cycles\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbreak_cycles\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mHAS_LAPACK64\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIS_PYSTON\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     39\u001b[0m         ]\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "# |export\n",
    "\n",
    "import concurrent.futures  # fastcore parallel fails for partial functions (https://github.com/fastai/fastcore/pull/294)\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import time\n",
    "from functools import partial, lru_cache\n",
    "from typing import List\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "from fastcore.basics import chunked\n",
    "from fastcore.parallel import parallel\n",
    "from openai import FineTune\n",
    "from pycm import ConfusionMatrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API wrappers\n",
    "\n",
    "> Helper functions that make it easier to use the OpenAI API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def _check_ft_state(ft_id):\n",
    "    ft = FineTune.retrieve(id=ft_id)\n",
    "    return ft.get('status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_ft_model_name(ft_id, sleep=60):\n",
    "    while True:\n",
    "        ft = FineTune.retrieve(id=ft_id)\n",
    "        if ft.get('status') == 'succeeded':\n",
    "            return ft.get('fine_tuned_model')\n",
    "        time.sleep(sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ada:ft-lsmoepfl-2022-08-18-00-13-00'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ft_model_name('ft-jK0ziTZX6y5d2DXbB3kdct4w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ada:ft-lsmoepfl-2022-09-02-14-15-28'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ft_model_name('ft-f5xtILIGM6yjvLrj0J1GH5FQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def fine_tune(\n",
    "    train_file,  # path to json file with training prompts (column names \"prompt\" and \"completion\")\n",
    "    valid_file,  # path to json file with validation prompts (column names \"prompt\" and \"completion\")\n",
    "    model: str = \"ada\",  # model type to use. One of \"ada\", \"babbage\", \"curie\", \"davinci\". \"ada\" is the default (and cheapest).\n",
    "    n_epochs: int = 4,  # number of epochs to fine-tune for\n",
    "    sleep: int = 120,  # number of seconds to wait between checking the status of the fine-tuning task\n",
    "):\n",
    "    \"\"\"Run the fine tuning of a GPT-3 model via the OpenAI API.\n",
    "    \n",
    "    There is some logic here to wait until the fine-tuning task is complete.\n",
    "    Often, the job might end up in the queue and we do not have the model id yet. \n",
    "    In this case, we will ask for the status of the job regularly and wait until it is complete.\n",
    "    \"\"\"\n",
    "    modelname = None\n",
    "    # ToDo: perhaps also use their Python wrapper? Or call directly via requests? \n",
    "    # subprocess is probably the ugliest way to do this, but it works.\n",
    "    result = subprocess.run(\n",
    "        f\"openai api fine_tunes.create -t {train_file}  -m {model} --n_epochs {n_epochs}\" + f\" -v {valid_file}\" if valid_file is not None else \"\",\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        ft_id = re.findall(r\"Created fine-tune: ([\\w\\d:-]+)\\n\", result.stdout)[\n",
    "                0\n",
    "            ]\n",
    "        modelname =  get_ft_model_name(ft_id, sleep)\n",
    "        # sync runs with wandb\n",
    "        subprocess.run(\"openai wandb sync -n 1\", shell=True)\n",
    "    except Exception:\n",
    "        print(result.stdout, result.stderr)\n",
    "    return modelname\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helpers to make it easiers to get completions from the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@lru_cache(maxsize=None)\n",
    "def query_gpt3(\n",
    "    model: str,  # name of the model to use, e.g. \"ada:ft-personal-2022-08-24-10-41-29\"\n",
    "    df: pd.DataFrame,  # hashable dataframe with prompts and expected completions (column names \"prompt\" and \"completion\")\n",
    "    temperature: float = 0,  # temperature, 0 is the default and corresponds to argmax\n",
    "    max_tokens: int = 10,  # maximum number of tokens to generate\n",
    "    sleep: float = 5,  # number of seconds to wait between queries\n",
    "    one_by_one: bool = False,  # if True, generate one completion at a time (i.e., due to submit the maximum number of prompts per request)\n",
    "    parallel_max: int = 20,  # maximum number of prompts that can be sent per request\n",
    "):\n",
    "    \n",
    "    \"\"\"Get completions for all prompts in a dataframe.\"\"\"\n",
    "    if one_by_one:\n",
    "        completions = []\n",
    "        for i, row in df.iterrows():\n",
    "            try:\n",
    "                completion = openai.Completion.create(\n",
    "                    model=model,\n",
    "                    prompt=row[\"prompt\"],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                )\n",
    "                completions.append(completion)\n",
    "                time.sleep(sleep)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Error on row {i}\")\n",
    "                completions.append(None)\n",
    "    else:\n",
    "        # they have a limit on the maximum number of parallel completions\n",
    "        # otherwise you get\n",
    "        # openai.error.InvalidRequestError: Too many parallel completions requested.\n",
    "        # You submitted 500 prompts, but you can currently request up to at most a total of 20).\n",
    "        # Please contact support@openai.com and tell us about your use-case if you would like this limit increased.\n",
    "        # (HINT: if you want to just evaluate probabilities without generating new text, you can submit more prompts if you set 'max_tokens' to 0.)\n",
    "        completions = []\n",
    "        for chunk in chunked(df[\"prompt\"], parallel_max):\n",
    "            completions_ = openai.Completion.create(\n",
    "                model=model,\n",
    "                prompt=chunk,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            completions.append(completions_)\n",
    "            time.sleep(sleep)\n",
    "\n",
    "        completions = {\n",
    "            \"choices\": [choice for c in completions for choice in c[\"choices\"]],\n",
    "        }\n",
    "\n",
    "    return completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def extract_prediction(\n",
    "    completion,  # dictionary with \"choices\" key returned by the API\n",
    "    i: int = 0,  # index of the \"choice\" (relevant if multiple completions have been returned)\n",
    ") -> str:\n",
    "    return completion[\"choices\"][i][\"text\"].split(\"@\")[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pred = {\n",
    "    \"choices\": [{\"finish_reason\": \"length\", \"index\": 0, \"text\": \" 0@@@@@@@\"}]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_prediction(example_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def extract_regression_prediction(\n",
    "    completion,  # dictionary with \"choices\" key returned by the API\n",
    "    i: int = 0,  # index of the \"choice\" (relevant if multiple completions have been returned)\n",
    ") -> float:\n",
    "    \"\"\"Similar to `extract_prediction`, but returns a float.\"\"\"\n",
    "    return float(completion[\"choices\"][i][\"text\"].split(\"@\")[0].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pred = {\n",
    "    \"choices\": [{\"finish_reason\": \"length\", \"index\": 0, \"text\": \" -8.2@@@@@@@\"}]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_regression_prediction(example_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export \n",
    "def extract_inverse_prediction(completion, i=0) -> str:\n",
    "    \"\"\"Extracts the prediction of a molecule/material generative task.\"\"\"\n",
    "    return completion[\"choices\"][i][\"text\"].split(\"@@@\")[0].strip().split()[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC1=C(C(C)=NN1)/N=N/C2=CC=C(C(F)(F)F)C=C2',\n",
       " 'CC(C=C(N(CCC#N)CCO)C=C1)=C1/N=N/C2=CC=CC=C2',\n",
       " 'CC(C=C(N(CCC#N)CCO)C=C1)=C1/N=N/C2=CC=CC=C2']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_inverse_predictions = {\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"finish_reason\": \"length\",\n",
    "            \"index\": 0,\n",
    "            \"text\": \"CC1=C(C(C)=NN1)/N=N/C2=CC=C(C(F)(F)F)C=C2@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\",\n",
    "        },\n",
    "        {\n",
    "            \"finish_reason\": \"length\",\n",
    "            \"index\": 1,\n",
    "            \"text\": \"CC(C=C(N(CCC#N)CCO)C=C1)=C1/N=N/C2=CC=CC=C2@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\",\n",
    "        },\n",
    "        {\n",
    "            \"finish_reason\": \"length\",\n",
    "            \"index\": 2,\n",
    "            \"text\": \"CC(C=C(N(CCC#N)CCO)C=C1)=C1/N=N/C2=CC=CC=C2@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\",\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "[extract_inverse_prediction(example_inverse_predictions, i) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def train_test_loop(\n",
    "    df: pd.DataFrame,  # dataframe with prompts and expected completions (column names \"prompt\" and \"completion\"). Split will be performed within this function.\n",
    "    train_size: int,  # number of rows to use for training\n",
    "    prompt_create_fn: callable,  # function to create a prompt from a row of the dataframe\n",
    "    random_state: int,  # random state for splitting the dataframe\n",
    "    stratify: Optional[str] = None,  # column name to use for stratification\n",
    "    test_subset: Optional[\n",
    "        int\n",
    "    ] = None,  # number of rows to use for testing. If None, use the remainder of the dataframe.\n",
    ") -> dict:\n",
    "    \"\"\"Run the full training and testing process for the classification task.\"\"\"\n",
    "\n",
    "    out = {}\n",
    "    train, test = train_test_split(\n",
    "        df, train_size=train_size, random_state=random_state, stratify=stratify\n",
    "    )\n",
    "\n",
    "    train_prompts = prompt_create_fn(train)\n",
    "    test_prompts = prompt_create_fn(test)\n",
    "\n",
    "    train_size = len(train_prompts)\n",
    "    test_size = len(test_prompts)\n",
    "\n",
    "    filename_base = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "    train_filename = (\n",
    "        f\"run_files/{filename_base}_train_prompts_polymers_{train_size}.jsonl\"\n",
    "    )\n",
    "    valid_filename = (\n",
    "        f\"run_files/{filename_base}_valid_prompts_polymers_{test_size}.jsonl\"\n",
    "    )\n",
    "\n",
    "    train_prompts.to_json(train_filename, orient=\"records\", lines=True)\n",
    "    test_prompts.to_json(valid_filename, orient=\"records\", lines=True)\n",
    "\n",
    "    out[\"train_filename\"] = train_filename\n",
    "    out[\"valid_filename\"] = valid_filename\n",
    "    out[\"modelname\"] = fine_tune(train_filename, valid_filename)\n",
    "\n",
    "    test_prompt_subset = test_prompts\n",
    "    if test_subset is not None:\n",
    "        test_prompt_subset = test_prompts.sample(test_subset)\n",
    "    completions = query_gpt3(out[\"modelname\"], test_prompt_subset)\n",
    "\n",
    "    ok_completions = [(i, c) for i, c in enumerate(completions) if c is not None]\n",
    "\n",
    "    predictions = [extract_prediction(completion) for _, completion in ok_completions]\n",
    "    true = [\n",
    "        int(test_prompt_subset.iloc[i][\"completion\"].split(\"@\")[0])\n",
    "        for i, _ in ok_completions\n",
    "    ]\n",
    "    cm = ConfusionMatrix(true, predictions)\n",
    "\n",
    "    out[\"cm\"] = cm\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Deep ensembles](https://cims.nyu.edu/~andrewgw/deepensembles/) are a powerful technique to make neural networks \"Bayesian\". It can make them more robust and also be used to obtain uncertainty estimates.\n",
    "\n",
    "Typically, they rely on the fact that there is some inherent randomness in training of a model due to the random intialization. However, when we fine-tune a model, we always start from the same weights. Hence we anticipate that we'll need to sample the data to achieve enough randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def _fine_tune(file_tuple): return fine_tune(*file_tuple)\n",
    "\n",
    "def multiple_fine_tunes(\n",
    "    train_files, \n",
    "    valid_files,\n",
    "):\n",
    "    print('Fine tuning on {} train files and {} valid files'.format(len(train_files), len(valid_files)))\n",
    "    models = parallel(_fine_tune, [(train_file, valid_file) for train_file, valid_file in zip(train_files, valid_files)])\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def ensemble_fine_tune(\n",
    "    train_frame, \n",
    "    valid_frame, \n",
    "    num_models: int = 10,\n",
    "    subsample: float = 0.8, \n",
    "    run_file_dir: str = \"run_files\",\n",
    "    filename_base_string: str = \"\"\n",
    "): \n",
    "    train_frames = [train_frame.sample(frac=subsample) for _ in range(num_models)]\n",
    "    valid_frames = [valid_frame] * num_models\n",
    "\n",
    "    train_filenames = []\n",
    "    for i, train_frame in enumerate(train_frames):\n",
    "        train_size  = len(train_frame)\n",
    "\n",
    "        filename_base = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "        train_filename = f\"run_files/{filename_base}_train_{filename_base_string}_ensemble_{i}_{train_size}.jsonl\"\n",
    "        train_filenames.append(train_filename)\n",
    "        train_frame.to_json(train_filename, orient=\"records\", lines=True)\n",
    "\n",
    "    valid_filenames = []\n",
    "    for i, valid_frame in enumerate(valid_frames):\n",
    "        valid_size  = len(valid_frame)\n",
    "\n",
    "        filename_base = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "        valid_filename = os.path.join(run_file_dir, f\"{filename_base}_valid_{filename_base_string}_ensemble_{i}_{valid_size}.jsonl\")\n",
    "        valid_filenames.append(valid_filename)\n",
    "        valid_frame.to_json(valid_filename, orient=\"records\", lines=True)\n",
    "\n",
    "    models = multiple_fine_tunes(train_filenames, valid_filenames)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def multiple_query_gpt3(\n",
    "    models: List[str],  # names of the models to use, e.g. \"ada:ft-personal-2022-08-24-10-41-29\"\n",
    "    df: pd.DataFrame,  # dataframe with prompts and expected completions (column names \"prompt\" and \"completion\")\n",
    "    temperature: float = 0,  # temperature, 0 is the default and corresponds to argmax\n",
    "    max_tokens: int = 10,  # maximum number of tokens to generate\n",
    "    sleep: float = 5,  # number of seconds to wait between queries\n",
    "    one_by_one: bool = False,  # if True, generate one completion at a time (i.e., due to submit the maximum number of prompts per request)\n",
    "    parallel_max: int = 20,  # maximum number of prompts that can be sent per request\n",
    "):\n",
    "    models = [model for model in models if model is not None]\n",
    "    curried_query = partial(query_gpt3, df=df, temperature=temperature, max_tokens=max_tokens, sleep=sleep, one_by_one=one_by_one, parallel_max=parallel_max)\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        completions = executor.map(curried_query, models)\n",
    "\n",
    "    return list(completions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def _get_embedding(texts, model):\n",
    "   embedding_responds =  openai.Embedding.create(input = texts, model=model)\n",
    "   return [['data'][i]['embedding'] for i in range(len(embedding_responds['data']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gpt3')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
