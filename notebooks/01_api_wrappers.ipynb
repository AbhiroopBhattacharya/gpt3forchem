{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp api_wrappers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "import re\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "import openai\n",
    "from pycm import ConfusionMatrix\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API wrappers\n",
    "\n",
    "> Helper functions that make it easier to use the OpenAI API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def fine_tune(\n",
    "    train_file,  # path to json file with training prompts (column names \"prompt\" and \"completion\")\n",
    "    valid_file,  # path to json file with validation prompts (column names \"prompt\" and \"completion\")\n",
    "    model: str = \"ada\",  # model type to use. One of \"ada\", \"davinci\". \"ada\" is the default (and cheapest).\n",
    "):\n",
    "    \"\"\"Run the fine tuning of a GPT-3 model via the OpenAI API.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        f\"openai api fine_tunes.create -t {train_file} -v {valid_file} -m {model}\",\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "    )\n",
    "    try:\n",
    "        modelname = re.findall(r\"completions.create -m ([\\w\\d:-]+) -p\", result.stdout)[\n",
    "            0\n",
    "        ]\n",
    "        # sync runs with wandb\n",
    "        subprocess.run(\"openai wandb sync -n 1\", shell=True)\n",
    "    except Exception:\n",
    "        print(result.stdout, result.stderr)\n",
    "    return modelname\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helpers to make it easiers to get completions from the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import pandas as pd\n",
    "from fastcore.basics import chunked\n",
    "\n",
    "\n",
    "def query_gpt3(\n",
    "    model: str,  # name of the model to use, e.g. \"ada:ft-personal-2022-08-24-10-41-29\"\n",
    "    df: pd.DataFrame,  # dataframe with prompts and expected completions (column names \"prompt\" and \"completion\")\n",
    "    temperature: float = 0,  # temperature, 0 is the default and corresponds to argmax\n",
    "    max_tokens: int = 10,  # maximum number of tokens to generate\n",
    "    sleep: float = 5,  # number of seconds to wait between queries\n",
    "    one_by_one: bool = False,  # if True, generate one completion at a time (i.e., due to submit the maximum number of prompts per request)\n",
    "    parallel_max: int = 20,  # maximum number of prompts that can be sent per request\n",
    "):\n",
    "    \"\"\"Get completions for all prompts in a dataframe.\"\"\"\n",
    "    if one_by_one:\n",
    "        completions = []\n",
    "        for i, row in df.iterrows():\n",
    "            try:\n",
    "                completion = openai.Completion.create(\n",
    "                    model=model,\n",
    "                    prompt=row[\"prompt\"],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                )\n",
    "                completions.append(completion)\n",
    "                time.sleep(sleep)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Error on row {i}\")\n",
    "                completions.append(None)\n",
    "    else:\n",
    "        # they have a limit on the maximum number of parallel completions\n",
    "        # otherwise you get\n",
    "        # openai.error.InvalidRequestError: Too many parallel completions requested.\n",
    "        # You submitted 500 prompts, but you can currently request up to at most a total of 20).\n",
    "        # Please contact support@openai.com and tell us about your use-case if you would like this limit increased.\n",
    "        # (HINT: if you want to just evaluate probabilities without generating new text, you can submit more prompts if you set 'max_tokens' to 0.)\n",
    "        completions = []\n",
    "        for chunk in chunked(df[\"prompt\"], parallel_max):\n",
    "            completions_ = openai.Completion.create(\n",
    "                model=model,\n",
    "                prompt=chunk,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            completions.append(completions_)\n",
    "            time.sleep(sleep)\n",
    "\n",
    "        completions = {\n",
    "            \"choices\": [choice for c in completions for choice in c[\"choices\"]],\n",
    "        }\n",
    "\n",
    "    return completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def extract_prediction(\n",
    "    completion,  # dictionary with \"choices\" key returned by the API\n",
    "    i: int = 0,  # index of the \"choice\" (relevant if multiple completions have been returned)\n",
    ") -> str:\n",
    "    return completion[\"choices\"][i][\"text\"].split(\"@\")[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pred = {\n",
    "    \"choices\": [{\"finish_reason\": \"length\", \"index\": 0, \"text\": \" 0@@@@@@@\"}]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_prediction(example_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def extract_regression_prediction(\n",
    "    completion,  # dictionary with \"choices\" key returned by the API\n",
    "    i: int = 0,  # index of the \"choice\" (relevant if multiple completions have been returned)\n",
    ") -> float:\n",
    "    \"\"\"Similar to `extract_prediction`, but returns a float.\"\"\"\n",
    "    return float(completion[\"choices\"][i][\"text\"].split(\"@\")[0].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pred = {\n",
    "    \"choices\": [{\"finish_reason\": \"length\", \"index\": 0, \"text\": \" -8.2@@@@@@@\"}]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_regression_prediction(example_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def train_test_loop(\n",
    "    df: pd.DataFrame,  # dataframe with prompts and expected completions (column names \"prompt\" and \"completion\"). Split will be performed within this function.\n",
    "    train_size: int,  # number of rows to use for training\n",
    "    prompt_create_fn: callable,  # function to create a prompt from a row of the dataframe\n",
    "    random_state: int,  # random state for splitting the dataframe\n",
    "    stratify: Optional[str] = None,  # column name to use for stratification\n",
    "    test_subset: Optional[\n",
    "        int\n",
    "    ] = None,  # number of rows to use for testing. If None, use the remainder of the dataframe.\n",
    ") -> dict:\n",
    "    \"\"\"Run the full training and testing process for the classification task.\"\"\"\n",
    "\n",
    "    out = {}\n",
    "    train, test = train_test_split(\n",
    "        df, train_size=train_size, random_state=random_state, stratify=stratify\n",
    "    )\n",
    "\n",
    "    train_prompts = prompt_create_fn(train)\n",
    "    test_prompts = prompt_create_fn(test)\n",
    "\n",
    "    train_size = len(train_prompts)\n",
    "    test_size = len(test_prompts)\n",
    "\n",
    "    filename_base = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "    train_filename = (\n",
    "        f\"run_files/{filename_base}_train_prompts_polymers_{train_size}.jsonl\"\n",
    "    )\n",
    "    valid_filename = (\n",
    "        f\"run_files/{filename_base}_valid_prompts_polymers_{test_size}.jsonl\"\n",
    "    )\n",
    "\n",
    "    train_prompts.to_json(train_filename, orient=\"records\", lines=True)\n",
    "    test_prompts.to_json(valid_filename, orient=\"records\", lines=True)\n",
    "\n",
    "    out[\"train_filename\"] = train_filename\n",
    "    out[\"valid_filename\"] = valid_filename\n",
    "    out[\"modelname\"] = fine_tune(train_filename, valid_filename)\n",
    "\n",
    "    test_prompt_subset = test_prompts\n",
    "    if test_subset is not None:\n",
    "        test_prompt_subset = test_prompts.sample(test_subset)\n",
    "    completions = query_gpt3(out[\"modelname\"], test_prompt_subset)\n",
    "\n",
    "    ok_completions = [(i, c) for i, c in enumerate(completions) if c is not None]\n",
    "\n",
    "    predictions = [extract_prediction(completion) for _, completion in ok_completions]\n",
    "    true = [\n",
    "        int(test_prompt_subset.iloc[i][\"completion\"].split(\"@\")[0])\n",
    "        for i, _ in ok_completions\n",
    "    ]\n",
    "    cm = ConfusionMatrix(true, predictions)\n",
    "\n",
    "    out[\"cm\"] = cm\n",
    "\n",
    "    return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
