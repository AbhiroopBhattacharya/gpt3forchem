{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp api_wrappers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "import re\n",
    "import subprocess\n",
    "import time\n",
    "from fastcore.parallel import parallel\n",
    "import openai\n",
    "from pycm import ConfusionMatrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from typing import List\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from fastcore.basics import chunked\n",
    "import concurrent.futures # fastcore parallel fails for partial functions (https://github.com/fastai/fastcore/pull/294)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API wrappers\n",
    "\n",
    "> Helper functions that make it easier to use the OpenAI API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def fine_tune(\n",
    "    train_file,  # path to json file with training prompts (column names \"prompt\" and \"completion\")\n",
    "    valid_file,  # path to json file with validation prompts (column names \"prompt\" and \"completion\")\n",
    "    model: str = \"ada\",  # model type to use. One of \"ada\", \"babbage\", \"curie\", \"davinci\". \"ada\" is the default (and cheapest).\n",
    "    n_epochs: int = 4,  # number of epochs to fine-tune for\n",
    "):\n",
    "    \"\"\"Run the fine tuning of a GPT-3 model via the OpenAI API.\"\"\"\n",
    "    modelname = None\n",
    "    # ToDo: perhaps also use their Python wrapper? Or call directly via requests? \n",
    "    # subprocess is probably the ugliest way to do this, but it works.\n",
    "    result = subprocess.run(\n",
    "        f\"openai api fine_tunes.create -t {train_file}  -m {model} --n_epochs {n_epochs}\" + f\" -v {valid_file}\" if valid_file is not None else \"\",\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "    )\n",
    "    try:\n",
    "        modelname = re.findall(r\"completions.create -m ([\\w\\d:-]+) -p\", result.stdout)[\n",
    "            0\n",
    "        ]\n",
    "        # sync runs with wandb\n",
    "        subprocess.run(\"openai wandb sync -n 1\", shell=True)\n",
    "    except Exception:\n",
    "        print(result.stdout, result.stderr)\n",
    "    return modelname\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helpers to make it easiers to get completions from the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def query_gpt3(\n",
    "    model: str,  # name of the model to use, e.g. \"ada:ft-personal-2022-08-24-10-41-29\"\n",
    "    df: pd.DataFrame,  # dataframe with prompts and expected completions (column names \"prompt\" and \"completion\")\n",
    "    temperature: float = 0,  # temperature, 0 is the default and corresponds to argmax\n",
    "    max_tokens: int = 10,  # maximum number of tokens to generate\n",
    "    sleep: float = 5,  # number of seconds to wait between queries\n",
    "    one_by_one: bool = False,  # if True, generate one completion at a time (i.e., due to submit the maximum number of prompts per request)\n",
    "    parallel_max: int = 20,  # maximum number of prompts that can be sent per request\n",
    "):\n",
    "    \"\"\"Get completions for all prompts in a dataframe.\"\"\"\n",
    "    if one_by_one:\n",
    "        completions = []\n",
    "        for i, row in df.iterrows():\n",
    "            try:\n",
    "                completion = openai.Completion.create(\n",
    "                    model=model,\n",
    "                    prompt=row[\"prompt\"],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                )\n",
    "                completions.append(completion)\n",
    "                time.sleep(sleep)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Error on row {i}\")\n",
    "                completions.append(None)\n",
    "    else:\n",
    "        # they have a limit on the maximum number of parallel completions\n",
    "        # otherwise you get\n",
    "        # openai.error.InvalidRequestError: Too many parallel completions requested.\n",
    "        # You submitted 500 prompts, but you can currently request up to at most a total of 20).\n",
    "        # Please contact support@openai.com and tell us about your use-case if you would like this limit increased.\n",
    "        # (HINT: if you want to just evaluate probabilities without generating new text, you can submit more prompts if you set 'max_tokens' to 0.)\n",
    "        completions = []\n",
    "        for chunk in chunked(df[\"prompt\"], parallel_max):\n",
    "            completions_ = openai.Completion.create(\n",
    "                model=model,\n",
    "                prompt=chunk,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            completions.append(completions_)\n",
    "            time.sleep(sleep)\n",
    "\n",
    "        completions = {\n",
    "            \"choices\": [choice for c in completions for choice in c[\"choices\"]],\n",
    "        }\n",
    "\n",
    "    return completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def extract_prediction(\n",
    "    completion,  # dictionary with \"choices\" key returned by the API\n",
    "    i: int = 0,  # index of the \"choice\" (relevant if multiple completions have been returned)\n",
    ") -> str:\n",
    "    return completion[\"choices\"][i][\"text\"].split(\"@\")[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pred = {\n",
    "    \"choices\": [{\"finish_reason\": \"length\", \"index\": 0, \"text\": \" 0@@@@@@@\"}]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_prediction(example_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def extract_regression_prediction(\n",
    "    completion,  # dictionary with \"choices\" key returned by the API\n",
    "    i: int = 0,  # index of the \"choice\" (relevant if multiple completions have been returned)\n",
    ") -> float:\n",
    "    \"\"\"Similar to `extract_prediction`, but returns a float.\"\"\"\n",
    "    return float(completion[\"choices\"][i][\"text\"].split(\"@\")[0].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pred = {\n",
    "    \"choices\": [{\"finish_reason\": \"length\", \"index\": 0, \"text\": \" -8.2@@@@@@@\"}]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_regression_prediction(example_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def train_test_loop(\n",
    "    df: pd.DataFrame,  # dataframe with prompts and expected completions (column names \"prompt\" and \"completion\"). Split will be performed within this function.\n",
    "    train_size: int,  # number of rows to use for training\n",
    "    prompt_create_fn: callable,  # function to create a prompt from a row of the dataframe\n",
    "    random_state: int,  # random state for splitting the dataframe\n",
    "    stratify: Optional[str] = None,  # column name to use for stratification\n",
    "    test_subset: Optional[\n",
    "        int\n",
    "    ] = None,  # number of rows to use for testing. If None, use the remainder of the dataframe.\n",
    ") -> dict:\n",
    "    \"\"\"Run the full training and testing process for the classification task.\"\"\"\n",
    "\n",
    "    out = {}\n",
    "    train, test = train_test_split(\n",
    "        df, train_size=train_size, random_state=random_state, stratify=stratify\n",
    "    )\n",
    "\n",
    "    train_prompts = prompt_create_fn(train)\n",
    "    test_prompts = prompt_create_fn(test)\n",
    "\n",
    "    train_size = len(train_prompts)\n",
    "    test_size = len(test_prompts)\n",
    "\n",
    "    filename_base = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "    train_filename = (\n",
    "        f\"run_files/{filename_base}_train_prompts_polymers_{train_size}.jsonl\"\n",
    "    )\n",
    "    valid_filename = (\n",
    "        f\"run_files/{filename_base}_valid_prompts_polymers_{test_size}.jsonl\"\n",
    "    )\n",
    "\n",
    "    train_prompts.to_json(train_filename, orient=\"records\", lines=True)\n",
    "    test_prompts.to_json(valid_filename, orient=\"records\", lines=True)\n",
    "\n",
    "    out[\"train_filename\"] = train_filename\n",
    "    out[\"valid_filename\"] = valid_filename\n",
    "    out[\"modelname\"] = fine_tune(train_filename, valid_filename)\n",
    "\n",
    "    test_prompt_subset = test_prompts\n",
    "    if test_subset is not None:\n",
    "        test_prompt_subset = test_prompts.sample(test_subset)\n",
    "    completions = query_gpt3(out[\"modelname\"], test_prompt_subset)\n",
    "\n",
    "    ok_completions = [(i, c) for i, c in enumerate(completions) if c is not None]\n",
    "\n",
    "    predictions = [extract_prediction(completion) for _, completion in ok_completions]\n",
    "    true = [\n",
    "        int(test_prompt_subset.iloc[i][\"completion\"].split(\"@\")[0])\n",
    "        for i, _ in ok_completions\n",
    "    ]\n",
    "    cm = ConfusionMatrix(true, predictions)\n",
    "\n",
    "    out[\"cm\"] = cm\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Deep ensembles](https://cims.nyu.edu/~andrewgw/deepensembles/) are a powerful technique to make neural networks \"Bayesian\". It can make them more robust and also be used to obtain uncertainty estimates.\n",
    "\n",
    "Typically, they rely on the fact that there is some inherent randomness in training of a model due to the random intialization. However, when we fine-tune a model, we always start from the same weights. Hence we anticipate that we'll need to sample the data to achieve enough randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def _fine_tune(file_tuple): return fine_tune(*file_tuple)\n",
    "\n",
    "def multiple_fine_tunes(\n",
    "    train_files, \n",
    "    valid_files,\n",
    "):\n",
    "    print('Fine tuning on {} train files and {} valid files'.format(len(train_files), len(valid_files)))\n",
    "    models = parallel(_fine_tune, [(train_file, valid_file) for train_file, valid_file in zip(train_files, valid_files)])\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def ensemble_fine_tune(\n",
    "    train_frame, \n",
    "    valid_frame, \n",
    "    num_models: int = 10,\n",
    "    subsample: float = 0.8, \n",
    "    run_file_dir: str = \"run_files\",\n",
    "    filename_base_string: str = \"\"\n",
    "): \n",
    "    train_frames = [train_frame.sample(frac=subsample) for _ in range(num_models)]\n",
    "    valid_frames = [valid_frame] * num_models\n",
    "\n",
    "    train_filenames = []\n",
    "    for i, train_frame in enumerate(train_frames):\n",
    "        train_size  = len(train_frame)\n",
    "\n",
    "        filename_base = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "        train_filename = f\"run_files/{filename_base}_train_{filename_base_string}_ensemble_{i}_{train_size}.jsonl\"\n",
    "        train_filenames.append(train_filename)\n",
    "        train_frame.to_json(train_filename, orient=\"records\", lines=True)\n",
    "\n",
    "    valid_filenames = []\n",
    "    for i, valid_frame in enumerate(valid_frames):\n",
    "        valid_size  = len(valid_frame)\n",
    "\n",
    "        filename_base = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "        valid_filename = os.path.join(run_file_dir, f\"{filename_base}_valid_{filename_base_string}_ensemble_{i}_{valid_size}.jsonl\")\n",
    "        valid_filenames.append(valid_filename)\n",
    "        valid_frame.to_json(valid_filename, orient=\"records\", lines=True)\n",
    "\n",
    "    models = multiple_fine_tunes(train_filenames, valid_filenames)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def multiple_query_gpt3(\n",
    "    models: List[str],  # names of the models to use, e.g. \"ada:ft-personal-2022-08-24-10-41-29\"\n",
    "    df: pd.DataFrame,  # dataframe with prompts and expected completions (column names \"prompt\" and \"completion\")\n",
    "    temperature: float = 0,  # temperature, 0 is the default and corresponds to argmax\n",
    "    max_tokens: int = 10,  # maximum number of tokens to generate\n",
    "    sleep: float = 5,  # number of seconds to wait between queries\n",
    "    one_by_one: bool = False,  # if True, generate one completion at a time (i.e., due to submit the maximum number of prompts per request)\n",
    "    parallel_max: int = 20,  # maximum number of prompts that can be sent per request\n",
    "):\n",
    "    models = [model for model in models if model is not None]\n",
    "    curried_query = partial(query_gpt3, df=df, temperature=temperature, max_tokens=max_tokens, sleep=sleep, one_by_one=one_by_one, parallel_max=parallel_max)\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        completions = executor.map(curried_query, models)\n",
    "\n",
    "    return list(completions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def _get_embedding(texts, model):\n",
    "   embedding_responds =  openai.Embedding.create(input = texts, model=model)\n",
    "   return [['data'][i]['embedding'] for i in range(len(embedding_responds['data']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gpt3')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
