{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp api_wrappers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import openai\n",
    "import time\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycm import ConfusionMatrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API wrappers\n",
    "\n",
    "> Helper functions that make it easier to use the OpenAI API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def fine_tune(train_file, valid_file, model: str = \"ada\"):\n",
    "    # run the fine tuning\n",
    "    result = subprocess.run(\n",
    "        f\"openai api fine_tunes.create -t {train_file} -v {valid_file} -m {model}\",\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "    )\n",
    "    try:\n",
    "        modelname = re.findall(r\"completions.create -m ([\\w\\d:-]+) -p\", result.stdout)[\n",
    "            0\n",
    "        ]\n",
    "        # sync runs with wandb\n",
    "        subprocess.run(\"openai wandb sync -n 1\", shell=True)\n",
    "    except Exception:\n",
    "        print(result.stdout, result.stderr)\n",
    "    return modelname\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from fastcore.basics import chunked\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def query_gpt3(\n",
    "    model,\n",
    "    df,\n",
    "    temperature=0,\n",
    "    max_tokens=10,\n",
    "    sleep=5,\n",
    "    one_by_one=False,\n",
    "    parallel_max: int = 20,\n",
    "):\n",
    "    if one_by_one:\n",
    "        completions = []\n",
    "        for i, row in df.iterrows():\n",
    "            try:\n",
    "                completion = openai.Completion.create(\n",
    "                    model=model,\n",
    "                    prompt=row[\"prompt\"],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                )\n",
    "                completions.append(completion)\n",
    "                time.sleep(sleep)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Error on row {i}\")\n",
    "                completions.append(None)\n",
    "    else:\n",
    "        # they have a limit on the maximum number of parallel completions\n",
    "        # otherwise you get\n",
    "        # openai.error.InvalidRequestError: Too many parallel completions requested.\n",
    "        # You submitted 500 prompts, but you can currently request up to at most a total of 20).\n",
    "        # Please contact support@openai.com and tell us about your use-case if you would like this limit increased.\n",
    "        # (HINT: if you want to just evaluate probabilities without generating new text, you can submit more prompts if you set 'max_tokens' to 0.)\n",
    "        completions = []\n",
    "        for chunk in chunked(df[\"prompt\"], parallel_max):\n",
    "            completions_ = openai.Completion.create(\n",
    "                model=model,\n",
    "                prompt=chunk,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            completions.append(completions_)\n",
    "            time.sleep(sleep)\n",
    "\n",
    "        completions = {\n",
    "            \"choices\": [choice for c in completions for choice in c['choices']],\n",
    "        }\n",
    "\n",
    "    return completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def extract_prediction(completion, i=0):\n",
    "    return completion[\"choices\"][i][\"text\"].split(\"@\")[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def train_test_loop(\n",
    "    df, train_size, prompt_create_fn, random_state, stratify=None, test_subset=None\n",
    "):\n",
    "\n",
    "    out = {}\n",
    "    train, test = train_test_split(\n",
    "        df, train_size=train_size, random_state=random_state, stratify=stratify\n",
    "    )\n",
    "\n",
    "    train_prompts = prompt_create_fn(train)\n",
    "    test_prompts = prompt_create_fn(test)\n",
    "\n",
    "    train_size = len(train_prompts)\n",
    "    test_size = len(test_prompts)\n",
    "\n",
    "    filename_base = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "    train_filename = (\n",
    "        f\"run_files/{filename_base}_train_prompts_polymers_{train_size}.jsonl\"\n",
    "    )\n",
    "    valid_filename = (\n",
    "        f\"run_files/{filename_base}_valid_prompts_polymers_{test_size}.jsonl\"\n",
    "    )\n",
    "\n",
    "    train_prompts.to_json(train_filename, orient=\"records\", lines=True)\n",
    "    test_prompts.to_json(valid_filename, orient=\"records\", lines=True)\n",
    "\n",
    "    out[\"train_filename\"] = train_filename\n",
    "    out[\"valid_filename\"] = valid_filename\n",
    "    out[\"modelname\"] = fine_tune(train_filename, valid_filename)\n",
    "\n",
    "    test_prompt_subset = test_prompts\n",
    "    if test_subset is not None:\n",
    "        test_prompt_subset = test_prompts.sample(test_subset)\n",
    "    completions = query_gpt3(out[\"modelname\"], test_prompt_subset)\n",
    "\n",
    "    ok_completions = [(i, c) for i, c in enumerate(completions) if c is not None]\n",
    "\n",
    "    predictions = [extract_prediction(completion) for _, completion in ok_completions]\n",
    "    true = [\n",
    "        int(test_prompt_subset.iloc[i][\"completion\"].split(\"@\")[0])\n",
    "        for i, _ in ok_completions\n",
    "    ]\n",
    "    cm = ConfusionMatrix(true, predictions)\n",
    "\n",
    "    out[\"cm\"] = cm\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpt3')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
