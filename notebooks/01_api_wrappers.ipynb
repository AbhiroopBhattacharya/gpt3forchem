{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp api_wrappers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import openai\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API wrappers\n",
    "\n",
    "> Helper functions that make it easier to use the OpenAI API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def fine_tune(train_file, valid_file, model: str = \"ada\"):\n",
    "    # run the fine tuning\n",
    "    result = subprocess.run(\n",
    "        f\"openai api fine_tunes.create -t {train_file} -v {valid_file} -m {model}\",\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
    "    )\n",
    "    modelname = re.findall(r'completions.create -m ([\\w\\d:-]+) -p', result.stdout)[0]\n",
    "    # sync runs with wandb\n",
    "    subprocess.run(\"openai wandb sync -n 1\", shell=True)\n",
    "    return modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stdout_fragment = \"openai api completions.create -m ada:ft-epfl-2022-06-23-09-10-58 -p <YOUR_PROMPT>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def query_gpt3(model, df, temperature=0, max_tokens=10):\n",
    "    completions = []\n",
    "    for i, row in df.iterrows():\n",
    "        completion = openai.Completion.create(\n",
    "            model=model,\n",
    "            prompt=row[\"prompt\"],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        completions.append(completion)\n",
    "        time.sleep(5)\n",
    "\n",
    "    return completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def extract_prediction(completion):\n",
    "    return completion[\"choices\"][0][\"text\"].split(\"@\")[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpt3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a4fa60962de90e73b5da8d67a44b01d2de04630d82b94b8db1f727a73d31e61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
