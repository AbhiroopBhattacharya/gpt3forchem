{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "from nbdev.showdoc import *\n",
    "from strsimpy.levenshtein import Levenshtein\n",
    "from strsimpy.longest_common_subsequence import LongestCommonSubsequence\n",
    "from strsimpy.normalized_levenshtein import NormalizedLevenshtein\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, max_error, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusable code for analyzing results\n",
    "\n",
    "> Analyze the outputs of the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure how different our outputs are from the input data, we'll use string distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def string_distances(training_set: Iterable[str], query_string: str):\n",
    "\n",
    "    distances = defaultdict(list)\n",
    "\n",
    "    metrics = [\n",
    "        (\"Levenshtein\", Levenshtein()),\n",
    "        (\"NormalizedLevenshtein\", NormalizedLevenshtein()),\n",
    "        (\"LongestCommonSubsequence\", LongestCommonSubsequence()),\n",
    "    ]\n",
    "\n",
    "    aggregations = [\n",
    "        (\"min\", lambda x: np.min(x)),\n",
    "        (\"max\", lambda x: np.max(x)),\n",
    "        (\"mean\", lambda x: np.mean(x)),\n",
    "        (\"std\", lambda x: np.std(x)),\n",
    "    ]\n",
    "\n",
    "    for training_string in training_set:\n",
    "        for metric_name, metric in metrics:\n",
    "            distances[metric_name].append(\n",
    "                metric.distance(training_string, query_string)\n",
    "            )\n",
    "\n",
    "    aggregated_distances = {}\n",
    "\n",
    "    for k, v in distances.items():\n",
    "        for agg_name, agg_func in aggregations:\n",
    "            aggregated_distances[f\"{k}_{agg_name}\"] = agg_func(v)\n",
    "\n",
    "    return aggregated_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "training_set = [\"AAA\", \"BBB\", \"CCC\"]\n",
    "query_string = \"BBB\"\n",
    "result = string_distances(training_set, query_string)\n",
    "\n",
    "assert result[\"NormalizedLevenshtein_min\"] == 0.0\n",
    "assert result[\"NormalizedLevenshtein_max\"] == 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polymers\n",
    "\n",
    "> Code specific for the polymer test case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def convert2smiles(string):\n",
    "    new_encoding = {\"A\": \"[Ta]\", \"B\": \"[Tr]\", \"W\": \"[W]\", \"R\": \"[R]\"}\n",
    "\n",
    "    for k, v in new_encoding.items():\n",
    "        string = string.replace(k, v)\n",
    "\n",
    "    string = string.replace(\"-\", \"\")\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we simply use single letters, without any special characters such as brackets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Ta][W][W][R][R][Ta]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert2smiles(\"AWWRRA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the composition from the prompt, we will check how often we find a given monomer in the string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_num_monomer(string, monomer):\n",
    "    num = re.findall(f\"([\\d+]) {monomer}\", string)\n",
    "    try:\n",
    "        num = int(num[0])\n",
    "    except Exception:\n",
    "        num = 0\n",
    "    return num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_monomer(\"Polymer with 3 A, 5 B and 0 C\", \"A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_prompt_compostion(prompt):\n",
    "    composition = {}\n",
    "\n",
    "    for monomer in [\"R\", \"W\", \"A\", \"B\"]:\n",
    "        composition[monomer] = get_num_monomer(prompt, monomer)\n",
    "\n",
    "    return composition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def get_target(string, target_name=\"adsorption\"):\n",
    "    num = re.findall(f\"([\\d+]) {target_name}\", string)\n",
    "    return int(num[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def get_prompt_data(prompt):\n",
    "    composition = get_prompt_compostion(prompt)\n",
    "\n",
    "    return composition, get_target(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def get_completion_composition(string):\n",
    "    parts = string.split(\"-\")\n",
    "    counts = Counter(parts)\n",
    "    return dict(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def string2performance(string):\n",
    "    # we need to perform a bunch of tasks here:\n",
    "    # 1) Featurize\n",
    "    # 2) Query the model\n",
    "\n",
    "    predicted_monomer_sequence = string.split(\"@\")[0].strip()\n",
    "    monomer_sq = re.findall(\"[(R|W|A|B)\\-(R|W|A|B)]+\", predicted_monomer_sequence)[0]\n",
    "    composition = get_completion_composition(monomer_sq)\n",
    "    smiles = convert2smiles(predicted_monomer_sequence)\n",
    "\n",
    "    features = pd.DataFrame(featurize_many([smiles]))\n",
    "    prediction = DELTA_G_MODEL.predict(features[FEATURES])\n",
    "    return {\n",
    "        \"monomer_squence\": monomer_sq,\n",
    "        \"composition\": composition,\n",
    "        \"smiles\": smiles,\n",
    "        \"prediction\": prediction,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def composition_mismatch(composition: dict, found: dict):\n",
    "    distances = []\n",
    "\n",
    "    # We also might have the case the there are keys that the input did not contain\n",
    "    all_keys = set(composition.keys()) & set(found.keys())\n",
    "\n",
    "    expected_len = []\n",
    "    found_len = []\n",
    "\n",
    "    for key in all_keys:\n",
    "        try:\n",
    "            expected = composition[key]\n",
    "        except KeyError:\n",
    "            expected = 0\n",
    "        expected_len.append(expected)\n",
    "        try:\n",
    "            f = found[key]\n",
    "        except KeyError:\n",
    "            f = 0\n",
    "        found_len.append(f)\n",
    "\n",
    "        distances.append(np.abs(expected - f))\n",
    "\n",
    "    expected_len = sum(expected_len)\n",
    "    found_len = sum(found_len)\n",
    "    return {\n",
    "        \"distances\": distances,\n",
    "        \"min\": np.min(distances),\n",
    "        \"max\": np.max(distances),\n",
    "        \"mean\": np.mean(distances),\n",
    "        \"expected_len\": expected_len,\n",
    "        \"found_len\": found_len,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def get_regression_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"r2\": r2_score(y_true, y_pred),\n",
    "        \"max_error\": max_error(y_true, y_pred),\n",
    "        \"mean_absolute_error\": mean_absolute_error(y_true, y_pred),\n",
    "        \"mean_squared_error\": mean_squared_error(y_true, y_pred),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r2': 1.0,\n",
       " 'max_error': 0,\n",
       " 'mean_absolute_error': 0.0,\n",
       " 'mean_squared_error': 0.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_regression_metrics(\n",
    "    [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gpt3')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
