{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import math\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nbdev.showdoc import *\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from sklearn.metrics import (max_error, mean_absolute_error,\n",
    "                             mean_squared_error, r2_score)\n",
    "from strsimpy.levenshtein import Levenshtein\n",
    "from strsimpy.longest_common_subsequence import LongestCommonSubsequence\n",
    "from strsimpy.normalized_levenshtein import NormalizedLevenshtein\n",
    "\n",
    "from gpt3forchem.api_wrappers import extract_inverse_prediction, query_gpt3\n",
    "from gpt3forchem.baselines import compute_fragprints\n",
    "from gpt3forchem.data import POLYMER_FEATURES\n",
    "from gpt3forchem.input import encode_categorical_value\n",
    "\n",
    "from tqdm import tqdm\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing results\n",
    "\n",
    "> Analyze the outputs of the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure how different our outputs are from the input data, we'll use string distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export \n",
    "\n",
    "_DEFAULT_AGGREGATIONS =  [\n",
    "        (\"min\", lambda x: np.min(x)),\n",
    "        (\"max\", lambda x: np.max(x)),\n",
    "        (\"mean\", lambda x: np.mean(x)),\n",
    "        (\"std\", lambda x: np.std(x)),\n",
    "    ]\n",
    "\n",
    "def aggregate_array(array, aggregations: Optional[List[Tuple[str, callable]]]= None): \n",
    "    if aggregations is None:\n",
    "        aggregations = _DEFAULT_AGGREGATIONS\n",
    "\n",
    "    aggregated_array = {}\n",
    "    for k,v in aggregations:\n",
    "        aggregated_array[k] = v(array)\n",
    "    return aggregated_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 3.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_array(np.array([1,2,3,4,5]), aggregations=[(\"mean\", lambda x: np.mean(x))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no aggregation functions are specified, the default aggregation functions are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min': 1, 'max': 5, 'mean': 3.0, 'std': 1.4142135623730951}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_array(np.array([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def string_distances(\n",
    "    training_set: Iterable[str], # string representations of the compounds in the training set\n",
    "    query_string: str # string representation of the compound to be queried\n",
    "):\n",
    "\n",
    "    distances = defaultdict(list)\n",
    "\n",
    "    metrics = [\n",
    "        (\"Levenshtein\", Levenshtein()),\n",
    "        (\"NormalizedLevenshtein\", NormalizedLevenshtein()),\n",
    "        (\"LongestCommonSubsequence\", LongestCommonSubsequence()),\n",
    "    ]\n",
    "\n",
    "    aggregations = [\n",
    "        (\"min\", lambda x: np.min(x)),\n",
    "        (\"max\", lambda x: np.max(x)),\n",
    "        (\"mean\", lambda x: np.mean(x)),\n",
    "        (\"std\", lambda x: np.std(x)),\n",
    "    ]\n",
    "\n",
    "    for training_string in training_set:\n",
    "        for metric_name, metric in metrics:\n",
    "            distances[metric_name].append(\n",
    "                metric.distance(training_string, query_string)\n",
    "            )\n",
    "\n",
    "    aggregated_distances = {}\n",
    "\n",
    "    for k, v in distances.items():\n",
    "        for agg_name, agg_func in aggregations:\n",
    "            aggregated_distances[f\"{k}_{agg_name}\"] = agg_func(v)\n",
    "\n",
    "    return aggregated_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "training_set = [\"AAA\", \"BBB\", \"CCC\"]\n",
    "query_string = \"BBB\"\n",
    "result = string_distances(training_set, query_string)\n",
    "\n",
    "assert result[\"NormalizedLevenshtein_min\"] == 0.0\n",
    "assert result[\"NormalizedLevenshtein_max\"] == 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Overall ACC': 0.352,\n",
       " 'Overall RACCU': 0.260694,\n",
       " 'Overall RACC': 0.204088,\n",
       " 'Kappa': 0.18583964056327834,\n",
       " 'Gwet AC1': 0.205077201356521,\n",
       " 'Bennett S': 0.18999999999999995,\n",
       " 'Kappa Standard Error': 0.026835443670701672,\n",
       " 'Kappa Unbiased': 0.12350231162739109,\n",
       " 'Scott PI': 0.12350231162739109,\n",
       " 'Kappa No Prevalence': -0.29600000000000004,\n",
       " 'Kappa 95% CI': (0.13324217096870306, 0.23843711015785363),\n",
       " 'Standard Error': 0.02135865164283551,\n",
       " '95% CI': (0.3101370427800424, 0.39386295721995757),\n",
       " 'Chi-Squared': 'None',\n",
       " 'Phi-Squared': 'None',\n",
       " 'Cramer V': 'None',\n",
       " 'Response Entropy': 1.3814056651434996,\n",
       " 'Reference Entropy': 2.316058449955823,\n",
       " 'Cross Entropy': 1.1144477741367746,\n",
       " 'Joint Entropy': 3.4137961407287363,\n",
       " 'Conditional Entropy': 1.0977376907729133,\n",
       " 'Mutual Information': 0.28366797437058633,\n",
       " 'KL Divergence': 'None',\n",
       " 'Lambda B': 0.1941747572815534,\n",
       " 'Lambda A': 0.1906005221932115,\n",
       " 'Chi-Squared DF': 16,\n",
       " 'Overall J': (0.7983837510803802, 0.15967675021607602),\n",
       " 'Hamming Loss': 0.648,\n",
       " 'Zero-one Loss': 324,\n",
       " 'NIR': 0.234,\n",
       " 'P-Value': 1.7884608238816213e-09,\n",
       " 'Overall CEN': 0.5121480827814909,\n",
       " 'Overall MCEN': 0.5578230580585823,\n",
       " 'Overall MCC': 0.21983337458274124,\n",
       " 'RR': 100.0,\n",
       " 'CBA': 0.2135499462943072,\n",
       " 'AUNU': 0.5893833133570839,\n",
       " 'AUNP': 0.5938131121782059,\n",
       " 'RCI': 0.12247876316592832,\n",
       " 'Pearson C': 'None',\n",
       " 'TPR Micro': 0.352,\n",
       " 'TPR Macro': 0.3412413818428856,\n",
       " 'CSI': 'None',\n",
       " 'ARI': 0.10567033844458766,\n",
       " 'TNR Micro': 0.838,\n",
       " 'TNR Macro': 0.8375252448712823,\n",
       " 'Bangdiwala B': 0.24636431343342088,\n",
       " 'Krippendorff Alpha': 0.12437880931576367,\n",
       " 'SOA1(Landis & Koch)': 'Slight',\n",
       " 'SOA2(Fleiss)': 'Poor',\n",
       " 'SOA3(Altman)': 'Poor',\n",
       " 'SOA4(Cicchetti)': 'Poor',\n",
       " 'SOA5(Cramer)': 'None',\n",
       " 'SOA6(Matthews)': 'Negligible',\n",
       " 'FPR Macro': 0.16247475512871767,\n",
       " 'FNR Macro': 0.6587586181571143,\n",
       " 'PPV Macro': 'None',\n",
       " 'ACC Macro': 0.7407999999999999,\n",
       " 'F1 Macro': 0.2487042288766685,\n",
       " 'FPR Micro': 0.16200000000000003,\n",
       " 'FNR Micro': 0.648,\n",
       " 'PPV Micro': 0.352,\n",
       " 'F1 Micro': 0.352}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm.overall_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def is_valid_smiles(smiles: str) -> bool:\n",
    "    \"\"\"We say a SMILES is valid if RDKit can parse it.\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:14:37] SMILES Parse Error: syntax error while parsing: aba\n",
      "[20:14:37] SMILES Parse Error: Failed parsing SMILES 'aba' for input: 'aba'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_valid_smiles('aba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_valid_smiles(\"CCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def is_string_in_training_data(string: str, training_data: Iterable[str]) -> bool:\n",
    "    \"\"\"Check if a string is in the training data.\n",
    "    \n",
    "    Note that this is not an exact check of a molecule is in the training data \n",
    "    as the model might in principle generate an equivalent, non-canonical SMILES.\n",
    "    However, one might expect that if a model remembers the training data\n",
    "    it will simple remember the canonical SMILES.\n",
    "    \"\"\"\n",
    "    return string in training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_string_in_training_data('a a hahah', ['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_string_in_training_data('a a hahah', ['a', 'b', 'c', 'a a hahah'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def get_similarity_to_train_mols(smiles: str, train_smiles: List[str]) -> List[float]: \n",
    "    train_mols = [Chem.MolFromSmiles(x) for x in train_smiles]\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    train_fps = [Chem.RDKFingerprint(x) for x in train_mols]\n",
    "    fp = Chem.RDKFingerprint(mol)\n",
    "\n",
    "    s = DataStructs.BulkTanimotoSimilarity(fp, train_fps)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.25, 0.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity_to_train_mols('CCC', ['CCC', 'CCCNC', \"N\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min': 0.25, 'max': 1.0, 'mean': 0.625, 'std': 0.375}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_array(get_similarity_to_train_mols('CCC', ['CCC', 'CCCNC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "def extract_numeric_prediction(predictions: List[str], is_int: bool = True):\n",
    "    converter = int if is_int else float\n",
    "    converted = []\n",
    "    for p in predictions:\n",
    "        try:\n",
    "            converted.append(converter(p))\n",
    "        except:\n",
    "            converted.append(np.nan)\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, nan]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_numeric_prediction(['1', '2', '3', '4', '5', 'nknik'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export \n",
    "\n",
    "def get_continuos_binned_distance(prediction, bin, bins):\n",
    "    in_bin = (prediction >= bins[bin][0]) & (prediction < bins[bin][1])\n",
    "    if in_bin:\n",
    "        loss = 0\n",
    "    else:\n",
    "        # compute the minimum distance to bin\n",
    "        left_edge_distance = abs(prediction - bins[bin][0])\n",
    "        right_edge_distance = abs(prediction - bins[bin][1])\n",
    "        loss = min(left_edge_distance, right_edge_distance)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polymers\n",
    "\n",
    "> Code specific for the polymer test case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def convert2smiles(string):\n",
    "    new_encoding = {\"A\": \"[Ta]\", \"B\": \"[Tr]\", \"W\": \"[W]\", \"R\": \"[R]\"}\n",
    "\n",
    "    for k, v in new_encoding.items():\n",
    "        string = string.replace(k, v)\n",
    "\n",
    "    string = string.replace(\"-\", \"\")\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we simply use single letters, without any special characters such as brackets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Ta][W][W][R][R][Ta]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert2smiles(\"AWWRRA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the composition from the prompt, we will check how often we find a given monomer in the string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_num_monomer(string, monomer):\n",
    "    num = re.findall(f\"([\\d]+) {monomer}\", string)\n",
    "    try:\n",
    "        num = int(num[0])\n",
    "    except Exception:\n",
    "        num = 0\n",
    "    return num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_monomer(\"Polymer with 3 A, 5 B and 0 C\", \"A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_num_monomer('what is a polymer with large adsorption energy and 4 A, 4 B, 12 W, and 12 R?###', 'R') == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_prompt_compostion(prompt):\n",
    "    composition = {}\n",
    "\n",
    "    for monomer in [\"R\", \"W\", \"A\", \"B\"]:\n",
    "        composition[monomer] = get_num_monomer(prompt, monomer)\n",
    "\n",
    "    return composition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R': 12, 'W': 12, 'A': 4, 'B': 4}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prompt_compostion('what is a polymer with large adsorption energy and 4 A, 4 B, 12 W, and 12 R?###')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def get_target(string, target_name=\"adsorption\", numerically_encoded=True):\n",
    "    if numerically_encoded:\n",
    "        num = re.findall(f\"([\\d+]) {target_name}\", string)\n",
    "        return int(num[0])\n",
    "    else:\n",
    "        val = re.findall(f\"(very large|large|medium|small|very small) {target_name}\", string)\n",
    "        return val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'large'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_target('what is a polymer with large adsorption energy', numerically_encoded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'very large'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_target('what is a polymer with very large adsorption energy', numerically_encoded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_target('what is a polymer with very small adsorption energy', numerically_encoded=False) == 'very small'\n",
    "assert get_target('what is a polymer with large adsorption energy and 8 A, 12 B, 12 W, and 8 R?###', numerically_encoded=False) == 'large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def get_polymer_prompt_data(prompt, numerically_encoded=False):\n",
    "    composition = get_prompt_compostion(prompt)\n",
    "\n",
    "    return composition, get_target(prompt, numerically_encoded=numerically_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'R': 8, 'W': 0, 'A': 8, 'B': 8}, 3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_polymer_prompt_data('what is a polymer with 3 adsorption energy and 8 A, 8 B, 10 W, and 8 R?###', numerically_encoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'R': 8, 'W': 2, 'A': 8, 'B': 2}, 'large')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_polymer_prompt_data('what is a polymer with large adsorption energy and 8 A, 12 B, 12 W, and 8 R?###', numerically_encoded=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def get_polymer_completion_composition(string):\n",
    "    parts = string.split(\"-\")\n",
    "    counts = Counter(parts)\n",
    "    return dict(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': 10, 'R': 8, 'A': 6, 'B': 7}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_polymer_completion_composition('W-W-R-W-R-W-A-W-R-B-W-A-R-B-W-A-R-B-W-A-R-B-W-R-A-B-W-B-R-A-B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': 12, 'R': 12, 'A': 4, 'B': 5}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_polymer_completion_composition('W-R-W-R-R-W-R-W-R-W-R-W-R-W-A-R-W-R-W-A-R-B-W-B-A-R-B-W-B-W-R-A-B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_polymer_completion_composition('W-W') == {'W': 2}\n",
    "assert get_polymer_completion_composition('W-R-A-B') == {'W': 1, 'R': 1, 'A': 1, 'B': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's reuse the featurizer [we used in the original work on those polymers](https://www.nature.com/articles/s41467-021-22437-0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "# Copyright 2020 PyPAL authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Turn a Polymer SMILES into features\"\"\"\n",
    "\n",
    "\n",
    "def featurize_many_polymers(smiless: list) -> pd.DataFrame:\n",
    "    \"\"\"Utility function that runs featurizaton on a\n",
    "    list of linear polymer smiles and returns a dataframe\"\"\"\n",
    "    features = []\n",
    "    for smiles in smiless:\n",
    "        pmsf = LinearPolymerSmilesFeaturizer(smiles)\n",
    "        features.append(pmsf.featurize())\n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "class LinearPolymerSmilesFeaturizer:\n",
    "    \"\"\"Compute features for linear polymers\"\"\"\n",
    "\n",
    "    def __init__(self, smiles: str, normalized_cluster_stats: bool = True):\n",
    "        self.smiles = smiles\n",
    "        assert \"(\" not in smiles, \"This featurizer does not work for branched polymers\"\n",
    "        self.characters = [\"[W]\", \"[Tr]\", \"[Ta]\", \"[R]\"]\n",
    "        self.replacement_dict = dict(\n",
    "            list(zip(self.characters, [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\"]))\n",
    "        )\n",
    "        self.normalized_cluster_stats = normalized_cluster_stats\n",
    "        self.surface_interactions = {\"[W]\": 30, \"[Ta]\": 20, \"[Tr]\": 30, \"[R]\": 20}\n",
    "        self.solvent_interactions = {\"[W]\": 30, \"[Ta]\": 25, \"[Tr]\": 35, \"[R]\": 30}\n",
    "        self._character_count = None\n",
    "        self._balance = None\n",
    "        self._relative_shannon = None\n",
    "        self._cluster_stats = None\n",
    "        self._head_tail_feat = None\n",
    "        self.features = None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_head_tail_features(string: str, characters: list) -> dict:\n",
    "        \"\"\"0/1/2 encoded feature indicating if the building block is at start/end of the polymer chain\"\"\"\n",
    "        is_head_tail = [0] * len(characters)\n",
    "\n",
    "        for i, char in enumerate(characters):\n",
    "            if string.startswith(char):\n",
    "                is_head_tail[i] += 1\n",
    "            if string.endswith(char):\n",
    "                is_head_tail[i] += 1\n",
    "\n",
    "        new_keys = [\"head_tail_\" + char for char in characters]\n",
    "        return dict(list(zip(new_keys, is_head_tail)))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cluster_stats(\n",
    "        s: str, replacement_dict: dict, normalized: bool = True\n",
    "    ) -> dict:  # pylint:disable=invalid-name\n",
    "        \"\"\"Statistics describing clusters such as [Tr][Tr][Tr]\"\"\"\n",
    "        clusters = LinearPolymerSmilesFeaturizer.find_clusters(s, replacement_dict)\n",
    "        cluster_stats = {}\n",
    "        cluster_stats[\"total_clusters\"] = 0\n",
    "        for key, value in clusters.items():\n",
    "            if value:\n",
    "                cluster_stats[\"num\" + \"_\" + key] = len(value)\n",
    "                cluster_stats[\"total_clusters\"] += len(value)\n",
    "                cluster_stats[\"max\" + \"_\" + key] = max(value)\n",
    "                cluster_stats[\"min\" + \"_\" + key] = min(value)\n",
    "                cluster_stats[\"mean\" + \"_\" + key] = np.mean(value)\n",
    "            else:\n",
    "                cluster_stats[\"num\" + \"_\" + key] = 0\n",
    "                cluster_stats[\"max\" + \"_\" + key] = 0\n",
    "                cluster_stats[\"min\" + \"_\" + key] = 0\n",
    "                cluster_stats[\"mean\" + \"_\" + key] = 0\n",
    "\n",
    "        if normalized:\n",
    "            for key, value in cluster_stats.items():\n",
    "                if \"num\" in key:\n",
    "                    try:\n",
    "                        cluster_stats[key] = value / cluster_stats[\"total_clusters\"]\n",
    "                    except ZeroDivisionError:\n",
    "                        cluster_stats[key] = 0\n",
    "\n",
    "        return cluster_stats\n",
    "\n",
    "    @staticmethod\n",
    "    def find_clusters(s: str, replacement_dict: dict) -> dict:  # pylint:disable=invalid-name\n",
    "        \"\"\"Use regex to find clusters\"\"\"\n",
    "        clusters = re.findall(\n",
    "            r\"((\\w)\\2{1,})\", LinearPolymerSmilesFeaturizer._multiple_replace(s, replacement_dict)\n",
    "        )\n",
    "        cluster_dict = dict(\n",
    "            list(zip(replacement_dict.keys(), [[] for i in replacement_dict.keys()]))\n",
    "        )\n",
    "        inv_replacement_dict = {v: k for k, v in replacement_dict.items()}\n",
    "        for cluster, character in clusters:\n",
    "            cluster_dict[inv_replacement_dict[character]].append(len(cluster))\n",
    "\n",
    "        return cluster_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def _multiple_replace(s: str, replacement_dict: dict) -> str:  # pylint:disable=invalid-name\n",
    "        for word in replacement_dict:\n",
    "            s = s.replace(word, replacement_dict[word])\n",
    "        return s\n",
    "\n",
    "    @staticmethod\n",
    "    def get_counts(smiles: str, characters: list) -> dict:\n",
    "        \"\"\"Count characters in SMILES string\"\"\"\n",
    "        counts = [smiles.count(char) for char in characters]\n",
    "        return dict(list(zip(characters, counts)))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_relative_shannon(character_count: dict) -> float:\n",
    "        \"\"\"Shannon entropy of string relative to maximum entropy of a string of the same length\"\"\"\n",
    "        counts = [c for c in character_count.values() if c > 0]\n",
    "        length = sum(counts)\n",
    "        probs = [count / length for count in counts]\n",
    "        ideal_entropy = LinearPolymerSmilesFeaturizer._entropy_max(length)\n",
    "        entropy = -sum([p * math.log(p) / math.log(2.0) for p in probs])\n",
    "\n",
    "        return entropy / ideal_entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def _entropy_max(length: int) -> float:\n",
    "        \"Calculates the max Shannon entropy of a string with given length\"\n",
    "\n",
    "        prob = 1.0 / length\n",
    "\n",
    "        return -1.0 * length * prob * math.log(prob) / math.log(2.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_balance(character_count: dict) -> dict:\n",
    "        \"\"\"Frequencies of characters\"\"\"\n",
    "        counts = list(character_count.values())\n",
    "        length = sum(counts)\n",
    "        frequencies = [c / length for c in counts]\n",
    "        return dict(list(zip(character_count.keys(), frequencies)))\n",
    "\n",
    "    def _featurize(self):\n",
    "        \"\"\"Run all available featurization methods\"\"\"\n",
    "        self._character_count = LinearPolymerSmilesFeaturizer.get_counts(\n",
    "            self.smiles, self.characters\n",
    "        )\n",
    "        self._balance = LinearPolymerSmilesFeaturizer.get_balance(self._character_count)\n",
    "        self._relative_shannon = LinearPolymerSmilesFeaturizer.get_relative_shannon(\n",
    "            self._character_count\n",
    "        )\n",
    "        self._cluster_stats = LinearPolymerSmilesFeaturizer.get_cluster_stats(\n",
    "            self.smiles, self.replacement_dict, self.normalized_cluster_stats\n",
    "        )\n",
    "        self._head_tail_feat = LinearPolymerSmilesFeaturizer.get_head_tail_features(\n",
    "            self.smiles, self.characters\n",
    "        )\n",
    "\n",
    "        self.features = self._head_tail_feat\n",
    "        self.features.update(self._cluster_stats)\n",
    "        self.features.update(self._balance)\n",
    "        self.features[\"rel_shannon\"] = self._relative_shannon\n",
    "        self.features[\"length\"] = sum(self._character_count.values())\n",
    "        solvent_interactions = sum(\n",
    "            [\n",
    "                [self.solvent_interactions[char]] * count\n",
    "                for char, count in self._character_count.items()\n",
    "            ],\n",
    "            [],\n",
    "        )\n",
    "        self.features[\"total_solvent\"] = sum(solvent_interactions)\n",
    "        self.features[\"std_solvent\"] = np.std(solvent_interactions)\n",
    "        surface_interactions = sum(\n",
    "            [\n",
    "                [self.surface_interactions[char]] * count\n",
    "                for char, count in self._character_count.items()\n",
    "            ],\n",
    "            [],\n",
    "        )\n",
    "        self.features[\"total_surface\"] = sum(surface_interactions)\n",
    "        self.features[\"std_surface\"] = np.std(surface_interactions)\n",
    "\n",
    "    def featurize(self) -> dict:\n",
    "        \"\"\"Run featurization\"\"\"\n",
    "        self._featurize()\n",
    "        return self.features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the new polymers our model came up with, we have to build a model that predicts the target (the adsorption energy). We have done so in a separate module (`27_polymer_delta_g_model.ipynb`) and saved the output in the `models` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def polymer_string2performance(string, model_dir = '../models'):\n",
    "    # we need to perform a bunch of tasks here:\n",
    "    # 1) Featurize\n",
    "    # 2) Query the model\n",
    "\n",
    "    DELTA_G_MODEL = joblib.load(os.path.join(model_dir, 'delta_g_model.joblib'))\n",
    "\n",
    "    predicted_monomer_sequence = string.split(\"@\")[0].strip()\n",
    "    monomer_sq = re.findall(\"[(R|W|A|B)\\-(R|W|A|B)]+\", predicted_monomer_sequence)[0]\n",
    "    composition = get_polymer_completion_composition(monomer_sq)\n",
    "    smiles = convert2smiles(predicted_monomer_sequence)\n",
    "\n",
    "    features = pd.DataFrame(featurize_many_polymers([smiles]))\n",
    "    prediction = DELTA_G_MODEL.predict(features[POLYMER_FEATURES])\n",
    "    return {\n",
    "        \"monomer_squence\": monomer_sq,\n",
    "        \"composition\": composition,\n",
    "        \"smiles\": smiles,\n",
    "        \"prediction\": prediction,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'monomer_squence': 'W-W-R-W-R-W-A-W-R-B-W-A-R-B-W-A-R-B-W-A-R-B-W-R-A-B-W-B-R-A-B',\n",
       " 'composition': {'W': 10, 'R': 8, 'A': 6, 'B': 7},\n",
       " 'smiles': '[W][W][R][W][R][W][Ta][W][R][Tr][W][Ta][R][Tr][W][Ta][R][Tr][W][Ta][R][Tr][W][R][Ta][Tr][W][Tr][R][Ta][Tr]',\n",
       " 'prediction': array([-7.335449], dtype=float32)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polymer_string2performance('W-W-R-W-R-W-A-W-R-B-W-A-R-B-W-A-R-B-W-A-R-B-W-R-A-B-W-B-R-A-B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def composition_mismatch(composition: dict, found: dict):\n",
    "    distances = []\n",
    "\n",
    "    # We also might have the case the there are keys that the input did not contain\n",
    "    all_keys = set(composition.keys()) & set(found.keys())\n",
    "\n",
    "    expected_len = []\n",
    "    found_len = []\n",
    "\n",
    "    for key in all_keys:\n",
    "        try:\n",
    "            expected = composition[key]\n",
    "        except KeyError:\n",
    "            expected = 0\n",
    "        expected_len.append(expected)\n",
    "        try:\n",
    "            f = found[key]\n",
    "        except KeyError:\n",
    "            f = 0\n",
    "        found_len.append(f)\n",
    "\n",
    "        distances.append(np.abs(expected - f))\n",
    "\n",
    "    expected_len = sum(expected_len)\n",
    "    found_len = sum(found_len)\n",
    "    return {\n",
    "        \"distances\": distances,\n",
    "        \"min\": np.min(distances),\n",
    "        \"max\": np.max(distances),\n",
    "        \"mean\": np.mean(distances),\n",
    "        \"expected_len\": expected_len,\n",
    "        \"found_len\": found_len,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distances': [0, 0, 0, 1],\n",
       " 'min': 0,\n",
       " 'max': 1,\n",
       " 'mean': 0.25,\n",
       " 'expected_len': 32,\n",
       " 'found_len': 33}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composition_mismatch(\n",
    "    {\"A\": 4, \"B\": 4, \"R\": 12, \"W\": 12},\n",
    "    {'W': 12, 'R': 12, 'A': 4, 'B': 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def get_inverse_polymer_metrics(completion_texts, df_test, df_train, bins, max_num_train_sequences = 2000, numerically_encoded = False):\n",
    "    losses = []\n",
    "    composition_mismatches = []\n",
    "\n",
    "    train_sequences = [polymer_string2performance(seq)[\"monomer_squence\"] for seq in df_train[\"completion\"]]\n",
    "    print(f\"Using {len(train_sequences)} training sequences\")\n",
    "    for i, row in tqdm(df_test.iterrows(), total=len(completion_texts)):\n",
    "        if i < len(completion_texts):\n",
    "            try:\n",
    "                composition, bin = get_polymer_prompt_data(row[\"prompt\"], numerically_encoded = numerically_encoded)\n",
    "                completion_data = polymer_string2performance(completion_texts[i])\n",
    "                bin = bin if numerically_encoded else encode_categorical_value(bin)\n",
    "                loss = get_continuos_binned_distance(completion_data[\"prediction\"][0], bin, bins)\n",
    "                losses.append(loss)\n",
    "\n",
    "                mm = composition_mismatch(composition, completion_data[\"composition\"])\n",
    " \n",
    "                distances = string_distances(\n",
    "                    train_sequences[:max_num_train_sequences], completion_data[\"monomer_squence\"]\n",
    "                )\n",
    "                mm.update(completion_data)\n",
    "                mm.update(distances)\n",
    "                mm.update({\"loss\": loss})\n",
    "                composition_mismatches.append(mm)\n",
    "            except Exception as e:\n",
    "                logger.exception(f'Error in get_inverse_polymer_metrics {e}')\n",
    "    return losses, pd.DataFrame(composition_mismatches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def get_regression_metrics(\n",
    "    y_true,  # actual values (ArrayLike)\n",
    "    y_pred,  # predicted values (ArrayLike)\n",
    ") -> dict:\n",
    "\n",
    "    try:\n",
    "        return {\n",
    "            \"r2\": r2_score(y_true, y_pred),\n",
    "            \"max_error\": max_error(y_true, y_pred),\n",
    "            \"mean_absolute_error\": mean_absolute_error(y_true, y_pred),\n",
    "            \"mean_squared_error\": mean_squared_error(y_true, y_pred),\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"r2\": np.nan,\n",
    "            \"max_error\": np.nan,\n",
    "            \"mean_absolute_error\": np.nan,\n",
    "            \"mean_squared_error\": np.nan,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r2': 1.0,\n",
       " 'max_error': 0,\n",
       " 'mean_absolute_error': 0.0,\n",
       " 'mean_squared_error': 0.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_regression_metrics([1, 2, 3, 4, 5], [1, 2, 3, 4, 5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photoswitches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code specific for the photoswitch case study.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll have some wrapper around GPR models that predict for us the $\\pi-\\pi^*$ and $n-\\pi^*$ transition energies. \n",
    "For simplicity, we'll just go via joblib files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def _predict_photoswitch(smiles_string: str,pi_pi_star_model_file='../models/pi_pi_star_model.joblib', n_pi_star_model_file='../models/n_pi_star_model.joblib'):\n",
    "    \"\"\"Predicting for a single SMILES string. Not really efficient due to the I/O overhead in loading the model.\"\"\"\n",
    "    pi_pi_star_model = joblib.load(pi_pi_star_model_file)\n",
    "    n_pi_star_model = joblib.load(n_pi_star_model_file)\n",
    "    fragprints = compute_fragprints([smiles_string])\n",
    "    return pi_pi_star_model.predict(fragprints)[0], n_pi_star_model.predict(fragprints)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def predict_photoswitch(smiles: Iterable[str], pi_pi_star_model_file='../models/pi_pi_star_model.joblib', n_pi_star_model_file='../models/n_pi_star_model.joblib'): \n",
    "    \"\"\"Predicting for a single SMILES string. Not really efficient due to the I/O overhead in loading the model.\"\"\"\n",
    "    if not isinstance(smiles, Iterable):\n",
    "        smiles = [smiles]\n",
    "    pi_pi_star_model = joblib.load(pi_pi_star_model_file)\n",
    "    n_pi_star_model = joblib.load(n_pi_star_model_file)\n",
    "    fragprints = compute_fragprints(smiles)\n",
    "    return pi_pi_star_model.predict(fragprints), n_pi_star_model.predict(fragprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[390.91004025]]), array([[446.54990223]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_photoswitch(['C1=CC=C(/N=N/C2=CC=C(NCCC#N)C=C2)C=C1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |  export\n",
    "\n",
    "_PI_PI_STAR_REGEX = r'pi-pi\\* transition wavelength of ([.\\d]+) nm'\n",
    "_N_PI_STAR_REGEX = r'n-pi\\* transition wavelength of ([.\\d]+) nm'\n",
    "\n",
    "def get_expected_wavelengths(prompt): \n",
    "    pi_pi_star_match = re.search(_PI_PI_STAR_REGEX, prompt)\n",
    "    n_pi_star_match = re.search(_N_PI_STAR_REGEX, prompt)\n",
    "    pi_pi_star = float(pi_pi_star_match.group(1)) if pi_pi_star_match else None\n",
    "    n_pi_star = float(n_pi_star_match.group(1)) if n_pi_star_match else None\n",
    "    return pi_pi_star, n_pi_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404.0, None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_expected_wavelengths('What is a molecule pi-pi* transition wavelength of 404.0 nm###')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(321.0, 424.0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_expected_wavelengths('What is a molecule pi-pi* transition wavelength of 321.0 nm and n-pi* transition wavelength of 424.0 nm###')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full evaluation is wrapped upped in the function below. Note that sampling at temperature > 0 is associated with some randomness. In other works, people samples $k$ times and took the best prediction for analysis. In the function below, we do not do this; we only sample once.\n",
    "\n",
    "Query multiple times to estimate the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def test_inverse_photoswitch(\n",
    "    prompt_frame, model, train_smiles, temperature, max_tokens: int = 80\n",
    "):\n",
    "    completions = query_gpt3(\n",
    "        model, prompt_frame, max_tokens=max_tokens, temperature=temperature\n",
    "    )\n",
    "    predictions = np.array(\n",
    "        [\n",
    "            extract_inverse_prediction(completions, i)\n",
    "            for i in range(len(completions[\"choices\"]))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    valid_smiles = [is_valid_smiles(smiles) for smiles in predictions]\n",
    "\n",
    "    smiles_in_train = [\n",
    "        is_string_in_training_data(smiles, train_smiles)\n",
    "        for smiles in predictions[valid_smiles]\n",
    "    ]\n",
    "\n",
    "    expected_pi_pi_star, expected_n_pi_star = [], []\n",
    "\n",
    "    for prompt in prompt_frame[\"prompt\"].values:\n",
    "        pi_pi_star, n_pi_star = get_expected_wavelengths(prompt)\n",
    "        expected_pi_pi_star.append(pi_pi_star)\n",
    "        expected_n_pi_star.append(n_pi_star)\n",
    "\n",
    "    expected_pi_pi_star = np.array(expected_pi_pi_star)\n",
    "    expected_n_pi_star = np.array(expected_n_pi_star)\n",
    "\n",
    "    has_expected_n_pi_star = np.array(\n",
    "        [n_pi_star is not None for n_pi_star in expected_n_pi_star]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        predicted_pi_pi_star, predicted_n_pi_star = predict_photoswitch(\n",
    "            predictions[valid_smiles]\n",
    "        )\n",
    "\n",
    "        predicted_pi_pi_star = predicted_pi_pi_star.flatten()\n",
    "        predicted_n_pi_star = predicted_n_pi_star.flatten()\n",
    "\n",
    "        pi_pi_star_metrics = get_regression_metrics(\n",
    "            expected_pi_pi_star[valid_smiles],\n",
    "            predicted_pi_pi_star,\n",
    "        )\n",
    "\n",
    "        mask_n_valid_smiles = [\n",
    "            has_expected_n_pi_star[i] for i in range(len(valid_smiles)) if valid_smiles[i]\n",
    "        ]\n",
    "        n_pi_star_metrics = get_regression_metrics(\n",
    "            expected_n_pi_star[valid_smiles & has_expected_n_pi_star],\n",
    "            np.array(predicted_n_pi_star)[mask_n_valid_smiles],\n",
    "        )\n",
    "\n",
    "        error_pi_pi_star = np.abs(expected_pi_pi_star[valid_smiles] - predicted_pi_pi_star)\n",
    "        error_n_pi_star = np.abs(\n",
    "            expected_n_pi_star[valid_smiles & has_expected_n_pi_star]\n",
    "            - np.array(predicted_n_pi_star)[mask_n_valid_smiles]\n",
    "        )\n",
    "\n",
    "        min_error_pi_pi_star = predictions[valid_smiles][np.argmin(error_pi_pi_star)]\n",
    "        min_error_n_pi_star = predictions[valid_smiles & has_expected_n_pi_star][np.argmin(error_n_pi_star)]\n",
    "\n",
    "        max_error_pi_pi_star = predictions[valid_smiles][np.argmax(error_pi_pi_star)]\n",
    "        max_error_n_pi_star = predictions[valid_smiles & has_expected_n_pi_star][np.argmax(error_n_pi_star)]\n",
    "\n",
    "        error_pi_pi_star_w_n = np.abs(\n",
    "            expected_pi_pi_star[valid_smiles & has_expected_n_pi_star]\n",
    "            - np.array(predicted_pi_pi_star)[mask_n_valid_smiles]\n",
    "        )\n",
    "\n",
    "        total_error_pi_pi_star = error_n_pi_star + error_pi_pi_star_w_n\n",
    "        min_total_error_pi_pi_star = predictions[valid_smiles & has_expected_n_pi_star][\n",
    "            np.argmin(total_error_pi_pi_star)\n",
    "        ]\n",
    "        max_total_error_pi_pi_star = predictions[valid_smiles & has_expected_n_pi_star][\n",
    "            np.argmax(total_error_pi_pi_star)\n",
    "        ]\n",
    "\n",
    "        mol_similarity_metrics = pd.DataFrame(\n",
    "            [\n",
    "                aggregate_array(get_similarity_to_train_mols(smile, train_smiles))\n",
    "                for smile in predictions[valid_smiles]\n",
    "            ]\n",
    "        )\n",
    "    except Exception:\n",
    "        smiles_in_train = []\n",
    "        predicted_pi_pi_star = None\n",
    "        predicted_n_pi_star = None\n",
    "        expected_pi_pi_star = None\n",
    "        expected_n_pi_star = None\n",
    "        valid_smiles = []\n",
    "        pi_pi_star_metrics= None\n",
    "        n_pi_star_metrics = None\n",
    "        min_error_pi_pi_star = None\n",
    "        max_error_pi_pi_star = None\n",
    "        min_error_n_pi_star = None\n",
    "        max_error_n_pi_star = None\n",
    "        min_total_error_pi_pi_star = None\n",
    "        max_total_error_pi_pi_star = None\n",
    "        mol_similarity_metrics = pd.DataFrame([])\n",
    "\n",
    "    results = {\n",
    "        \"meta\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"model\": model,\n",
    "        },\n",
    "        \"predictions\": predictions,\n",
    "        \"valid_smiles\": valid_smiles,\n",
    "        \"smiles_in_train\": smiles_in_train,\n",
    "        \"predicted_pi_pi_star\": predicted_pi_pi_star,\n",
    "        \"predicted_n_pi_star\": predicted_n_pi_star,\n",
    "        \"expected_pi_pi_star\": expected_pi_pi_star,\n",
    "        \"expected_n_pi_star\": expected_n_pi_star,\n",
    "        \"fractions_valid_smiles\": np.mean(valid_smiles),\n",
    "        \"fractions_smiles_in_train\": np.mean(smiles_in_train),\n",
    "        \"pi_pi_star_metrics\": pi_pi_star_metrics,\n",
    "        \"n_pi_star_metrics\": n_pi_star_metrics,\n",
    "        \"examples\": {\n",
    "            \"min_error_pi_pi_star\": min_error_pi_pi_star,\n",
    "            \"max_error_pi_pi_star\": max_error_pi_pi_star,\n",
    "            \"min_error_n_pi_star\": min_error_n_pi_star,\n",
    "            \"max_error_n_pi_star\": max_error_n_pi_star,\n",
    "            \"min_total_error_pi_pi_star\": min_total_error_pi_pi_star,\n",
    "            \"max_total_error_pi_pi_star\": max_total_error_pi_pi_star,\n",
    "        },\n",
    "        \"mol_similarity_metrics\": mol_similarity_metrics,\n",
    "        \"mol_similarity_metrics_mean\": mol_similarity_metrics.mean(),\n",
    "        \"mol_similarity_metrics_std\": mol_similarity_metrics.std(),\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gpt3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a4fa60962de90e73b5da8d67a44b01d2de04630d82b94b8db1f727a73d31e61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
