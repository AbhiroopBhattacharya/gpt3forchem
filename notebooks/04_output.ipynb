{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.9/site-packages/gpflow/experimental/utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.decorator.check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.9/site-packages/gpflow/experimental/utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.inheritance.inherit_check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# |export\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nbdev.showdoc import *\n",
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from sklearn.metrics import (max_error, mean_absolute_error,\n",
    "                             mean_squared_error, r2_score)\n",
    "from strsimpy.levenshtein import Levenshtein\n",
    "from strsimpy.longest_common_subsequence import LongestCommonSubsequence\n",
    "from strsimpy.normalized_levenshtein import NormalizedLevenshtein\n",
    "\n",
    "from gpt3forchem.baselines import compute_fragprints\n",
    "from gpt3forchem.api_wrappers import query_gpt3, extract_inverse_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing results\n",
    "\n",
    "> Analyze the outputs of the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure how different our outputs are from the input data, we'll use string distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export \n",
    "\n",
    "_DEFAULT_AGGREGATIONS =  [\n",
    "        (\"min\", lambda x: np.min(x)),\n",
    "        (\"max\", lambda x: np.max(x)),\n",
    "        (\"mean\", lambda x: np.mean(x)),\n",
    "        (\"std\", lambda x: np.std(x)),\n",
    "    ]\n",
    "\n",
    "def aggregate_array(array, aggregations: Optional[List[Tuple[str, callable]]]= None): \n",
    "    if aggregations is None:\n",
    "        aggregations = _DEFAULT_AGGREGATIONS\n",
    "\n",
    "    aggregated_array = {}\n",
    "    for k,v in aggregations:\n",
    "        aggregated_array[k] = v(array)\n",
    "    return aggregated_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 3.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_array(np.array([1,2,3,4,5]), aggregations=[(\"mean\", lambda x: np.mean(x))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no aggregation functions are specified, the default aggregation functions are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min': 1, 'max': 5, 'mean': 3.0, 'std': 1.4142135623730951}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_array(np.array([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def string_distances(\n",
    "    training_set: Iterable[str], # string representations of the compounds in the training set\n",
    "    query_string: str # string representation of the compound to be queried\n",
    "):\n",
    "\n",
    "    distances = defaultdict(list)\n",
    "\n",
    "    metrics = [\n",
    "        (\"Levenshtein\", Levenshtein()),\n",
    "        (\"NormalizedLevenshtein\", NormalizedLevenshtein()),\n",
    "        (\"LongestCommonSubsequence\", LongestCommonSubsequence()),\n",
    "    ]\n",
    "\n",
    "    aggregations = [\n",
    "        (\"min\", lambda x: np.min(x)),\n",
    "        (\"max\", lambda x: np.max(x)),\n",
    "        (\"mean\", lambda x: np.mean(x)),\n",
    "        (\"std\", lambda x: np.std(x)),\n",
    "    ]\n",
    "\n",
    "    for training_string in training_set:\n",
    "        for metric_name, metric in metrics:\n",
    "            distances[metric_name].append(\n",
    "                metric.distance(training_string, query_string)\n",
    "            )\n",
    "\n",
    "    aggregated_distances = {}\n",
    "\n",
    "    for k, v in distances.items():\n",
    "        for agg_name, agg_func in aggregations:\n",
    "            aggregated_distances[f\"{k}_{agg_name}\"] = agg_func(v)\n",
    "\n",
    "    return aggregated_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "training_set = [\"AAA\", \"BBB\", \"CCC\"]\n",
    "query_string = \"BBB\"\n",
    "result = string_distances(training_set, query_string)\n",
    "\n",
    "assert result[\"NormalizedLevenshtein_min\"] == 0.0\n",
    "assert result[\"NormalizedLevenshtein_max\"] == 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def is_valid_smiles(smiles: str) -> bool:\n",
    "    \"\"\"We say a SMILES is valid if RDKit can parse it.\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08:49:37] SMILES Parse Error: syntax error while parsing: aba\n",
      "[08:49:37] SMILES Parse Error: Failed parsing SMILES 'aba' for input: 'aba'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_valid_smiles('aba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_valid_smiles(\"CCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def is_string_in_training_data(string: str, training_data: Iterable[str]) -> bool:\n",
    "    \"\"\"Check if a string is in the training data.\n",
    "    \n",
    "    Note that this is not an exact check of a molecule is in the training data \n",
    "    as the model might in principle generate an equivalent, non-canonical SMILES.\n",
    "    However, one might expect that if a model remembers the training data\n",
    "    it will simple remember the canonical SMILES.\n",
    "    \"\"\"\n",
    "    return string in training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_string_in_training_data('a a hahah', ['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_string_in_training_data('a a hahah', ['a', 'b', 'c', 'a a hahah'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def get_similarity_to_train_mols(smiles: str, train_smiles: List[str]) -> List[float]: \n",
    "    train_mols = [Chem.MolFromSmiles(x) for x in train_smiles]\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    train_fps = [Chem.RDKFingerprint(x) for x in train_mols]\n",
    "    fp = Chem.RDKFingerprint(mol)\n",
    "\n",
    "    s = DataStructs.BulkTanimotoSimilarity(fp, train_fps)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.25, 0.0]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity_to_train_mols('CCC', ['CCC', 'CCCNC', \"N\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min': 0.25, 'max': 1.0, 'mean': 0.625, 'std': 0.375}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_array(get_similarity_to_train_mols('CCC', ['CCC', 'CCCNC']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polymers\n",
    "\n",
    "> Code specific for the polymer test case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def convert2smiles(string):\n",
    "    new_encoding = {\"A\": \"[Ta]\", \"B\": \"[Tr]\", \"W\": \"[W]\", \"R\": \"[R]\"}\n",
    "\n",
    "    for k, v in new_encoding.items():\n",
    "        string = string.replace(k, v)\n",
    "\n",
    "    string = string.replace(\"-\", \"\")\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we simply use single letters, without any special characters such as brackets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Ta][W][W][R][R][Ta]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert2smiles(\"AWWRRA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the composition from the prompt, we will check how often we find a given monomer in the string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_num_monomer(string, monomer):\n",
    "    num = re.findall(f\"([\\d+]) {monomer}\", string)\n",
    "    try:\n",
    "        num = int(num[0])\n",
    "    except Exception:\n",
    "        num = 0\n",
    "    return num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_monomer(\"Polymer with 3 A, 5 B and 0 C\", \"A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_prompt_compostion(prompt):\n",
    "    composition = {}\n",
    "\n",
    "    for monomer in [\"R\", \"W\", \"A\", \"B\"]:\n",
    "        composition[monomer] = get_num_monomer(prompt, monomer)\n",
    "\n",
    "    return composition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def get_target(string, target_name=\"adsorption\"):\n",
    "    num = re.findall(f\"([\\d+]) {target_name}\", string)\n",
    "    return int(num[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def get_prompt_data(prompt):\n",
    "    composition = get_prompt_compostion(prompt)\n",
    "\n",
    "    return composition, get_target(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def get_completion_composition(string):\n",
    "    parts = string.split(\"-\")\n",
    "    counts = Counter(parts)\n",
    "    return dict(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def string2performance(string):\n",
    "    # we need to perform a bunch of tasks here:\n",
    "    # 1) Featurize\n",
    "    # 2) Query the model\n",
    "\n",
    "    predicted_monomer_sequence = string.split(\"@\")[0].strip()\n",
    "    monomer_sq = re.findall(\"[(R|W|A|B)\\-(R|W|A|B)]+\", predicted_monomer_sequence)[0]\n",
    "    composition = get_completion_composition(monomer_sq)\n",
    "    smiles = convert2smiles(predicted_monomer_sequence)\n",
    "\n",
    "    features = pd.DataFrame(featurize_many([smiles]))\n",
    "    prediction = DELTA_G_MODEL.predict(features[FEATURES])\n",
    "    return {\n",
    "        \"monomer_squence\": monomer_sq,\n",
    "        \"composition\": composition,\n",
    "        \"smiles\": smiles,\n",
    "        \"prediction\": prediction,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def composition_mismatch(composition: dict, found: dict):\n",
    "    distances = []\n",
    "\n",
    "    # We also might have the case the there are keys that the input did not contain\n",
    "    all_keys = set(composition.keys()) & set(found.keys())\n",
    "\n",
    "    expected_len = []\n",
    "    found_len = []\n",
    "\n",
    "    for key in all_keys:\n",
    "        try:\n",
    "            expected = composition[key]\n",
    "        except KeyError:\n",
    "            expected = 0\n",
    "        expected_len.append(expected)\n",
    "        try:\n",
    "            f = found[key]\n",
    "        except KeyError:\n",
    "            f = 0\n",
    "        found_len.append(f)\n",
    "\n",
    "        distances.append(np.abs(expected - f))\n",
    "\n",
    "    expected_len = sum(expected_len)\n",
    "    found_len = sum(found_len)\n",
    "    return {\n",
    "        \"distances\": distances,\n",
    "        \"min\": np.min(distances),\n",
    "        \"max\": np.max(distances),\n",
    "        \"mean\": np.mean(distances),\n",
    "        \"expected_len\": expected_len,\n",
    "        \"found_len\": found_len,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def get_regression_metrics(\n",
    "    y_true,  # actual values (ArrayLike)\n",
    "    y_pred,  # predicted values (ArrayLike)\n",
    ") -> dict:\n",
    "\n",
    "    try:\n",
    "        return {\n",
    "            \"r2\": r2_score(y_true, y_pred),\n",
    "            \"max_error\": max_error(y_true, y_pred),\n",
    "            \"mean_absolute_error\": mean_absolute_error(y_true, y_pred),\n",
    "            \"mean_squared_error\": mean_squared_error(y_true, y_pred),\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"r2\": np.nan,\n",
    "            \"max_error\": np.nan,\n",
    "            \"mean_absolute_error\": np.nan,\n",
    "            \"mean_squared_error\": np.nan,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r2': 1.0,\n",
       " 'max_error': 0,\n",
       " 'mean_absolute_error': 0.0,\n",
       " 'mean_squared_error': 0.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_regression_metrics([1, 2, 3, 4, 5], [1, 2, 3, 4, 5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photoswitches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code specific for the photoswitch case study.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll have some wrapper around GPR models that predict for us the $\\pi-\\pi^*$ and $n-\\pi^*$ transition energies. \n",
    "For simplicity, we'll just go via joblib files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def _predict_photoswitch(smiles_string: str,pi_pi_star_model_file='../models/pi_pi_star_model.joblib', n_pi_star_model_file='../models/n_pi_star_model.joblib'):\n",
    "    \"\"\"Predicting for a single SMILES string. Not really efficient due to the I/O overhead in loading the model.\"\"\"\n",
    "    pi_pi_star_model = joblib.load(pi_pi_star_model_file)\n",
    "    n_pi_star_model = joblib.load(n_pi_star_model_file)\n",
    "    fragprints = compute_fragprints([smiles_string])\n",
    "    return pi_pi_star_model.predict(fragprints)[0], n_pi_star_model.predict(fragprints)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def predict_photoswitch(smiles: Iterable[str], pi_pi_star_model_file='../models/pi_pi_star_model.joblib', n_pi_star_model_file='../models/n_pi_star_model.joblib'): \n",
    "    \"\"\"Predicting for a single SMILES string. Not really efficient due to the I/O overhead in loading the model.\"\"\"\n",
    "    if not isinstance(smiles, Iterable):\n",
    "        smiles = [smiles]\n",
    "    pi_pi_star_model = joblib.load(pi_pi_star_model_file)\n",
    "    n_pi_star_model = joblib.load(n_pi_star_model_file)\n",
    "    fragprints = compute_fragprints(smiles)\n",
    "    return pi_pi_star_model.predict(fragprints), n_pi_star_model.predict(fragprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[390.91004025]]), array([[446.54990223]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_photoswitch(['C1=CC=C(/N=N/C2=CC=C(NCCC#N)C=C2)C=C1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |  export\n",
    "\n",
    "_PI_PI_STAR_REGEX = r'pi-pi\\* transition wavelength of ([.\\d]+) nm'\n",
    "_N_PI_STAR_REGEX = r'n-pi\\* transition wavelength of ([.\\d]+) nm'\n",
    "\n",
    "def get_expected_wavelengths(prompt): \n",
    "    pi_pi_star_match = re.search(_PI_PI_STAR_REGEX, prompt)\n",
    "    n_pi_star_match = re.search(_N_PI_STAR_REGEX, prompt)\n",
    "    pi_pi_star = float(pi_pi_star_match.group(1)) if pi_pi_star_match else None\n",
    "    n_pi_star = float(n_pi_star_match.group(1)) if n_pi_star_match else None\n",
    "    return pi_pi_star, n_pi_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404.0, None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_expected_wavelengths('What is a molecule pi-pi* transition wavelength of 404.0 nm###')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(321.0, 424.0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_expected_wavelengths('What is a molecule pi-pi* transition wavelength of 321.0 nm and n-pi* transition wavelength of 424.0 nm###')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full evaluation is wrapped upped in the function below. Note that sampling at temperature > 0 is associated with some randomness. In other works, people samples $k$ times and took the best prediction for analysis. In the function below, we do not do this; we only sample once.\n",
    "\n",
    "Query multiple times to estimate the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def test_inverse_photoswitch(\n",
    "    prompt_frame, model, train_smiles, temperature, max_tokens: int = 80\n",
    "):\n",
    "    completions = query_gpt3(\n",
    "        model, prompt_frame, max_tokens=max_tokens, temperature=temperature\n",
    "    )\n",
    "    predictions = np.array(\n",
    "        [\n",
    "            extract_inverse_prediction(completions, i)\n",
    "            for i in range(len(completions[\"choices\"]))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    valid_smiles = [is_valid_smiles(smiles) for smiles in predictions]\n",
    "\n",
    "    smiles_in_train = [\n",
    "        is_string_in_training_data(smiles, train_smiles)\n",
    "        for smiles in predictions[valid_smiles]\n",
    "    ]\n",
    "\n",
    "    expected_pi_pi_star, expected_n_pi_star = [], []\n",
    "\n",
    "    for prompt in prompt_frame[\"prompt\"].values:\n",
    "        pi_pi_star, n_pi_star = get_expected_wavelengths(prompt)\n",
    "        expected_pi_pi_star.append(pi_pi_star)\n",
    "        expected_n_pi_star.append(n_pi_star)\n",
    "\n",
    "    expected_pi_pi_star = np.array(expected_pi_pi_star)\n",
    "    expected_n_pi_star = np.array(expected_n_pi_star)\n",
    "\n",
    "    has_expected_n_pi_star = np.array(\n",
    "        [n_pi_star is not None for n_pi_star in expected_n_pi_star]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        predicted_pi_pi_star, predicted_n_pi_star = predict_photoswitch(\n",
    "            predictions[valid_smiles]\n",
    "        )\n",
    "\n",
    "        predicted_pi_pi_star = predicted_pi_pi_star.flatten()\n",
    "        predicted_n_pi_star = predicted_n_pi_star.flatten()\n",
    "\n",
    "        pi_pi_star_metrics = get_regression_metrics(\n",
    "            expected_pi_pi_star[valid_smiles],\n",
    "            predicted_pi_pi_star,\n",
    "        )\n",
    "\n",
    "        mask_n_valid_smiles = [\n",
    "            has_expected_n_pi_star[i] for i in range(len(valid_smiles)) if valid_smiles[i]\n",
    "        ]\n",
    "        n_pi_star_metrics = get_regression_metrics(\n",
    "            expected_n_pi_star[valid_smiles & has_expected_n_pi_star],\n",
    "            np.array(predicted_n_pi_star)[mask_n_valid_smiles],\n",
    "        )\n",
    "\n",
    "        error_pi_pi_star = np.abs(expected_pi_pi_star[valid_smiles] - predicted_pi_pi_star)\n",
    "        error_n_pi_star = np.abs(\n",
    "            expected_n_pi_star[valid_smiles & has_expected_n_pi_star]\n",
    "            - np.array(predicted_n_pi_star)[mask_n_valid_smiles]\n",
    "        )\n",
    "\n",
    "        min_error_pi_pi_star = predictions[valid_smiles][np.argmin(error_pi_pi_star)]\n",
    "        min_error_n_pi_star = predictions[valid_smiles & has_expected_n_pi_star][np.argmin(error_n_pi_star)]\n",
    "\n",
    "        max_error_pi_pi_star = predictions[valid_smiles][np.argmax(error_pi_pi_star)]\n",
    "        max_error_n_pi_star = predictions[valid_smiles & has_expected_n_pi_star][np.argmax(error_n_pi_star)]\n",
    "\n",
    "        error_pi_pi_star_w_n = np.abs(\n",
    "            expected_pi_pi_star[valid_smiles & has_expected_n_pi_star]\n",
    "            - np.array(predicted_pi_pi_star)[mask_n_valid_smiles]\n",
    "        )\n",
    "\n",
    "        total_error_pi_pi_star = error_n_pi_star + error_pi_pi_star_w_n\n",
    "        min_total_error_pi_pi_star = predictions[valid_smiles & has_expected_n_pi_star][\n",
    "            np.argmin(total_error_pi_pi_star)\n",
    "        ]\n",
    "        max_total_error_pi_pi_star = predictions[valid_smiles & has_expected_n_pi_star][\n",
    "            np.argmax(total_error_pi_pi_star)\n",
    "        ]\n",
    "\n",
    "        mol_similarity_metrics = pd.DataFrame(\n",
    "            [\n",
    "                aggregate_array(get_similarity_to_train_mols(smile, train_smiles))\n",
    "                for smile in predictions[valid_smiles]\n",
    "            ]\n",
    "        )\n",
    "    except Exception:\n",
    "        smiles_in_train = []\n",
    "        predicted_pi_pi_star = None\n",
    "        predicted_n_pi_star = None\n",
    "        expected_pi_pi_star = None\n",
    "        expected_n_pi_star = None\n",
    "        valid_smiles = []\n",
    "        pi_pi_star_metrics= None\n",
    "        n_pi_star_metrics = None\n",
    "        min_error_pi_pi_star = None\n",
    "        max_error_pi_pi_star = None\n",
    "        min_error_n_pi_star = None\n",
    "        max_error_n_pi_star = None\n",
    "        min_total_error_pi_pi_star = None\n",
    "        max_total_error_pi_pi_star = None\n",
    "        mol_similarity_metrics = pd.DataFrame([])\n",
    "\n",
    "    results = {\n",
    "        \"meta\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"model\": model,\n",
    "        },\n",
    "        \"predictions\": predictions,\n",
    "        \"valid_smiles\": valid_smiles,\n",
    "        \"smiles_in_train\": smiles_in_train,\n",
    "        \"predicted_pi_pi_star\": predicted_pi_pi_star,\n",
    "        \"predicted_n_pi_star\": predicted_n_pi_star,\n",
    "        \"expected_pi_pi_star\": expected_pi_pi_star,\n",
    "        \"expected_n_pi_star\": expected_n_pi_star,\n",
    "        \"fractions_valid_smiles\": np.mean(valid_smiles),\n",
    "        \"fractions_smiles_in_train\": np.mean(smiles_in_train),\n",
    "        \"pi_pi_star_metrics\": pi_pi_star_metrics,\n",
    "        \"n_pi_star_metrics\": n_pi_star_metrics,\n",
    "        \"examples\": {\n",
    "            \"min_error_pi_pi_star\": min_error_pi_pi_star,\n",
    "            \"max_error_pi_pi_star\": max_error_pi_pi_star,\n",
    "            \"min_error_n_pi_star\": min_error_n_pi_star,\n",
    "            \"max_error_n_pi_star\": max_error_n_pi_star,\n",
    "            \"min_total_error_pi_pi_star\": min_total_error_pi_pi_star,\n",
    "            \"max_total_error_pi_pi_star\": max_total_error_pi_pi_star,\n",
    "        },\n",
    "        \"mol_similarity_metrics\": mol_similarity_metrics,\n",
    "        \"mol_similarity_metrics_mean\": mol_similarity_metrics.mean(),\n",
    "        \"mol_similarity_metrics_std\": mol_similarity_metrics.std(),\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gpt3')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
