{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Iterable, List\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nbdev.showdoc import *\n",
    "from rdkit import Chem\n",
    "from sklearn.metrics import (max_error, mean_absolute_error,\n",
    "                             mean_squared_error, r2_score)\n",
    "from strsimpy.levenshtein import Levenshtein\n",
    "from strsimpy.longest_common_subsequence import LongestCommonSubsequence\n",
    "from strsimpy.normalized_levenshtein import NormalizedLevenshtein\n",
    "\n",
    "from gpt3forchem.baselines import compute_fragprints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing results\n",
    "\n",
    "> Analyze the outputs of the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure how different our outputs are from the input data, we'll use string distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def string_distances(\n",
    "    training_set: Iterable[str], # string representations of the compounds in the training set\n",
    "    query_string: str # string representation of the compound to be queried\n",
    "):\n",
    "\n",
    "    distances = defaultdict(list)\n",
    "\n",
    "    metrics = [\n",
    "        (\"Levenshtein\", Levenshtein()),\n",
    "        (\"NormalizedLevenshtein\", NormalizedLevenshtein()),\n",
    "        (\"LongestCommonSubsequence\", LongestCommonSubsequence()),\n",
    "    ]\n",
    "\n",
    "    aggregations = [\n",
    "        (\"min\", lambda x: np.min(x)),\n",
    "        (\"max\", lambda x: np.max(x)),\n",
    "        (\"mean\", lambda x: np.mean(x)),\n",
    "        (\"std\", lambda x: np.std(x)),\n",
    "    ]\n",
    "\n",
    "    for training_string in training_set:\n",
    "        for metric_name, metric in metrics:\n",
    "            distances[metric_name].append(\n",
    "                metric.distance(training_string, query_string)\n",
    "            )\n",
    "\n",
    "    aggregated_distances = {}\n",
    "\n",
    "    for k, v in distances.items():\n",
    "        for agg_name, agg_func in aggregations:\n",
    "            aggregated_distances[f\"{k}_{agg_name}\"] = agg_func(v)\n",
    "\n",
    "    return aggregated_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "training_set = [\"AAA\", \"BBB\", \"CCC\"]\n",
    "query_string = \"BBB\"\n",
    "result = string_distances(training_set, query_string)\n",
    "\n",
    "assert result[\"NormalizedLevenshtein_min\"] == 0.0\n",
    "assert result[\"NormalizedLevenshtein_max\"] == 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def is_valid_smiles(smiles: str) -> bool:\n",
    "    \"\"\"We say a SMILES is valid if RDKit can parse it.\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08:49:37] SMILES Parse Error: syntax error while parsing: aba\n",
      "[08:49:37] SMILES Parse Error: Failed parsing SMILES 'aba' for input: 'aba'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_valid_smiles('aba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_valid_smiles(\"CCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def is_string_in_training_data(string: str, training_data: Iterable[str]) -> bool:\n",
    "    \"\"\"Check if a string is in the training data.\n",
    "    \n",
    "    Note that this is not an exact check of a molecule is in the training data \n",
    "    as the model might in principle generate an equivalent, non-canonical SMILES.\n",
    "    However, one might expect that if a model remembers the training data\n",
    "    it will simple remember the canonical SMILES.\n",
    "    \"\"\"\n",
    "    return string in training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_string_in_training_data('a a hahah', ['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_string_in_training_data('a a hahah', ['a', 'b', 'c', 'a a hahah'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polymers\n",
    "\n",
    "> Code specific for the polymer test case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def convert2smiles(string):\n",
    "    new_encoding = {\"A\": \"[Ta]\", \"B\": \"[Tr]\", \"W\": \"[W]\", \"R\": \"[R]\"}\n",
    "\n",
    "    for k, v in new_encoding.items():\n",
    "        string = string.replace(k, v)\n",
    "\n",
    "    string = string.replace(\"-\", \"\")\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we simply use single letters, without any special characters such as brackets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Ta][W][W][R][R][Ta]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert2smiles(\"AWWRRA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the composition from the prompt, we will check how often we find a given monomer in the string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_num_monomer(string, monomer):\n",
    "    num = re.findall(f\"([\\d+]) {monomer}\", string)\n",
    "    try:\n",
    "        num = int(num[0])\n",
    "    except Exception:\n",
    "        num = 0\n",
    "    return num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_monomer(\"Polymer with 3 A, 5 B and 0 C\", \"A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_prompt_compostion(prompt):\n",
    "    composition = {}\n",
    "\n",
    "    for monomer in [\"R\", \"W\", \"A\", \"B\"]:\n",
    "        composition[monomer] = get_num_monomer(prompt, monomer)\n",
    "\n",
    "    return composition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def get_target(string, target_name=\"adsorption\"):\n",
    "    num = re.findall(f\"([\\d+]) {target_name}\", string)\n",
    "    return int(num[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def get_prompt_data(prompt):\n",
    "    composition = get_prompt_compostion(prompt)\n",
    "\n",
    "    return composition, get_target(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def get_completion_composition(string):\n",
    "    parts = string.split(\"-\")\n",
    "    counts = Counter(parts)\n",
    "    return dict(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def string2performance(string):\n",
    "    # we need to perform a bunch of tasks here:\n",
    "    # 1) Featurize\n",
    "    # 2) Query the model\n",
    "\n",
    "    predicted_monomer_sequence = string.split(\"@\")[0].strip()\n",
    "    monomer_sq = re.findall(\"[(R|W|A|B)\\-(R|W|A|B)]+\", predicted_monomer_sequence)[0]\n",
    "    composition = get_completion_composition(monomer_sq)\n",
    "    smiles = convert2smiles(predicted_monomer_sequence)\n",
    "\n",
    "    features = pd.DataFrame(featurize_many([smiles]))\n",
    "    prediction = DELTA_G_MODEL.predict(features[FEATURES])\n",
    "    return {\n",
    "        \"monomer_squence\": monomer_sq,\n",
    "        \"composition\": composition,\n",
    "        \"smiles\": smiles,\n",
    "        \"prediction\": prediction,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def composition_mismatch(composition: dict, found: dict):\n",
    "    distances = []\n",
    "\n",
    "    # We also might have the case the there are keys that the input did not contain\n",
    "    all_keys = set(composition.keys()) & set(found.keys())\n",
    "\n",
    "    expected_len = []\n",
    "    found_len = []\n",
    "\n",
    "    for key in all_keys:\n",
    "        try:\n",
    "            expected = composition[key]\n",
    "        except KeyError:\n",
    "            expected = 0\n",
    "        expected_len.append(expected)\n",
    "        try:\n",
    "            f = found[key]\n",
    "        except KeyError:\n",
    "            f = 0\n",
    "        found_len.append(f)\n",
    "\n",
    "        distances.append(np.abs(expected - f))\n",
    "\n",
    "    expected_len = sum(expected_len)\n",
    "    found_len = sum(found_len)\n",
    "    return {\n",
    "        \"distances\": distances,\n",
    "        \"min\": np.min(distances),\n",
    "        \"max\": np.max(distances),\n",
    "        \"mean\": np.mean(distances),\n",
    "        \"expected_len\": expected_len,\n",
    "        \"found_len\": found_len,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def get_regression_metrics(\n",
    "    y_true,  # actual values (ArrayLike)\n",
    "    y_pred,  # predicted values (ArrayLike)\n",
    ") -> dict:\n",
    "    return {\n",
    "        \"r2\": r2_score(y_true, y_pred),\n",
    "        \"max_error\": max_error(y_true, y_pred),\n",
    "        \"mean_absolute_error\": mean_absolute_error(y_true, y_pred),\n",
    "        \"mean_squared_error\": mean_squared_error(y_true, y_pred),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r2': 1.0,\n",
       " 'max_error': 0,\n",
       " 'mean_absolute_error': 0.0,\n",
       " 'mean_squared_error': 0.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_regression_metrics([1, 2, 3, 4, 5], [1, 2, 3, 4, 5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photoswitches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code specific for the photoswitch case study.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll have some wrapper around GPR models that predict for us the $\\pi-\\pi^*$ and $n-\\pi^*$ transition energies. \n",
    "For simplicity, we'll just go via joblib files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def _predict_photoswitch(smiles_string: str,pi_pi_star_model_file='../models/pi_pi_star_model.joblib', n_pi_star_model_file='../models/n_pi_star_model.joblib'):\n",
    "    \"\"\"Predicting for a single SMILES string. Not really efficient due to the I/O overhead in loading the model.\"\"\"\n",
    "    pi_pi_star_model = joblib.load(pi_pi_star_model_file)\n",
    "    n_pi_star_model = joblib.load(n_pi_star_model_file)\n",
    "    fragprints = compute_fragprints([smiles_string])\n",
    "    return pi_pi_star_model.predict(fragprints)[0], n_pi_star_model.predict(fragprints)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def predict_photoswitch(smiles: Iterable[str], pi_pi_star_model_file='../models/pi_pi_star_model.joblib', n_pi_star_model_file='../models/n_pi_star_model.joblib'): \n",
    "    \"\"\"Predicting for a single SMILES string. Not really efficient due to the I/O overhead in loading the model.\"\"\"\n",
    "    if not isinstance(smiles, Iterable):\n",
    "        smiles = [smiles]\n",
    "    pi_pi_star_model = joblib.load(pi_pi_star_model_file)\n",
    "    n_pi_star_model = joblib.load(n_pi_star_model_file)\n",
    "    fragprints = compute_fragprints(smiles)\n",
    "    return pi_pi_star_model.predict(fragprints), n_pi_star_model.predict(fragprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[390.91004025]]), array([[446.54990223]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_photoswitch(['C1=CC=C(/N=N/C2=CC=C(NCCC#N)C=C2)C=C1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "PI_PI_STAR_REGEX = r\"pi-pi\\* transition wavelength of ([.\\d]+) nm\"\n",
    "N_PI_STAR_REGEX = r\"n-pi\\* transition wavelength of ([.\\d]+) nm\"\n",
    "\n",
    "def get_expected_wavelengths(prompt): \n",
    "    pi_pi_star_match = re.search(PI_PI_STAR_REGEX, prompt)\n",
    "    n_pi_star_match = re.search(N_PI_STAR_REGEX, prompt)\n",
    "    if pi_pi_star_match:\n",
    "        pi_pi_star = float(pi_pi_star_match.group(1))\n",
    "    else: \n",
    "        pi_pi_star = None\n",
    "    \n",
    "    if n_pi_star_match:\n",
    "        n_pi_star = float(n_pi_star_match.group(1))\n",
    "    else:\n",
    "        n_pi_star = None\n",
    "\n",
    "    return pi_pi_star, n_pi_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404.0, None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_expected_wavelengths('What is a molecule pi-pi* transition wavelength of 404.0 nm###')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(321.0, 424.0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_expected_wavelengths('What is a molecule pi-pi* transition wavelength of 321.0 nm and n-pi* transition wavelength of 424.0 nm###')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gpt3')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
