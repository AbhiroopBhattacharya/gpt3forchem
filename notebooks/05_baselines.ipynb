{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.9/site-packages/gpflow/experimental/utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.decorator.check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.9/site-packages/gpflow/experimental/utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.inheritance.inherit_check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# |export\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Iterable\n",
    "\n",
    "import gpflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from gpflow.mean_functions import Constant\n",
    "from gpflow.utilities import positive, print_summary\n",
    "from gpflow.utilities.ops import broadcasting_elementwise\n",
    "from nbdev.showdoc import *\n",
    "from optuna import create_study\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "from optuna.samplers import TPESampler\n",
    "from pycm import ConfusionMatrix\n",
    "from rdkit.Chem import AllChem, Descriptors, MolFromSmiles, MolToSmiles\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from wandb.xgboost import WandbCallback\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "from gpt3forchem.data import get_photoswitch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "\n",
    "> Code for baseline models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "class BaseLineModel(ABC):\n",
    "    @abstractmethod\n",
    "    def tune(self, X_train, y_train):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X_train, y_train):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "class XGBClassificationBaseline(BaseLineModel):\n",
    "    def __init__(self, seed, num_trials=100) -> None:\n",
    "        self.seed = seed\n",
    "        self.num_trials = num_trials\n",
    "        self.model = XGBClassifier()\n",
    "\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def tune(self, X_train, y_train):\n",
    "        y_train = self.label_encoder.fit_transform(y_train)\n",
    "\n",
    "        def objective(\n",
    "            trial,\n",
    "            X,\n",
    "            y,\n",
    "            random_state=22,\n",
    "            n_splits=5,\n",
    "            n_jobs=1,\n",
    "            early_stopping_rounds=100,\n",
    "        ):\n",
    "            # XGBoost parameters\n",
    "            params = {\n",
    "                \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 4, 10_000),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 4, 100),\n",
    "                \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.05),\n",
    "                \"colsample_bytree\": trial.suggest_loguniform(\n",
    "                    \"colsample_bytree\", 0.2, 1\n",
    "                ),\n",
    "                \"subsample\": trial.suggest_loguniform(\"subsample\", 0.00001, 1),\n",
    "                \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-8, 10.0),\n",
    "                \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 10.0),\n",
    "                \"seed\": random_state,\n",
    "                \"n_jobs\": n_jobs,\n",
    "            }\n",
    "\n",
    "            model = XGBClassifier(**params)\n",
    "            pruning_callback = XGBoostPruningCallback(trial, \"validation_0-mlogloss\")\n",
    "            kf = KFold(n_splits=n_splits)\n",
    "            X_values = X.values\n",
    "            y_values = y\n",
    "\n",
    "            scores = []\n",
    "            for train_index, test_index in kf.split(X_values):\n",
    "                X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n",
    "                y_A, y_B = y_values[train_index], y_values[test_index]\n",
    "\n",
    "                model.fit(\n",
    "                    X_A,\n",
    "                    y_A,\n",
    "                    eval_set=[(X_B, y_B)],\n",
    "                    eval_metric=\"mlogloss\",\n",
    "                    verbose=0,\n",
    "                    callbacks=[pruning_callback],\n",
    "                    early_stopping_rounds=early_stopping_rounds,\n",
    "                )\n",
    "                y_pred = model.predict(X_B)\n",
    "                scores.append(f1_score(y_pred, y_B, average=\"macro\"))\n",
    "            return np.mean(scores)\n",
    "\n",
    "        sampler = TPESampler(seed=self.seed)\n",
    "        study = create_study(direction=\"maximize\", sampler=sampler)\n",
    "        study.optimize(\n",
    "            lambda trial: objective(\n",
    "                trial,\n",
    "                X_train,\n",
    "                y_train,\n",
    "                random_state=self.seed,\n",
    "                n_splits=5,\n",
    "                n_jobs=-1,\n",
    "                early_stopping_rounds=100,\n",
    "            ),\n",
    "            n_trials=self.num_trials,\n",
    "            n_jobs=1,\n",
    "        )\n",
    "\n",
    "        self.model = XGBClassifier(**study.best_params, callbacks=[WandbCallback()])\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        y_train = self.label_encoder.fit_transform(y_train)\n",
    "        self.model.fit(X_train.values, y_train)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.label_encoder.inverse_transform(self.model.predict(X.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class XGBRegressionBaseline(BaseLineModel):\n",
    "    def __init__(self, seed, num_trials=100) -> None:\n",
    "        self.seed = seed\n",
    "        self.num_trials = num_trials\n",
    "        self.model = XGBRegressor()\n",
    "\n",
    "    def tune(self, X_train, y_train):\n",
    "        def objective(\n",
    "            trial,\n",
    "            X,\n",
    "            y,\n",
    "            random_state=22,\n",
    "            n_splits=5,\n",
    "            n_jobs=1,\n",
    "            early_stopping_rounds=50,\n",
    "        ):\n",
    "            # XGBoost parameters\n",
    "            params = {\n",
    "                \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n",
    "                \"objective\": \"reg:squarederror\",\n",
    "                \"n_estimators\": 10000,\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12),\n",
    "                \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.005, 0.05),\n",
    "                \"colsample_bytree\": trial.suggest_loguniform(\n",
    "                    \"colsample_bytree\", 0.2, 0.6\n",
    "                ),\n",
    "                \"subsample\": trial.suggest_loguniform(\"subsample\", 0.4, 0.8),\n",
    "                \"alpha\": trial.suggest_loguniform(\"alpha\", 0.01, 10.0),\n",
    "                \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 10.0),\n",
    "                \"gamma\": trial.suggest_loguniform(\"lambda\", 1e-8, 10.0),\n",
    "                \"min_child_weight\": trial.suggest_loguniform(\n",
    "                    \"min_child_weight\", 10, 1000\n",
    "                ),\n",
    "                \"seed\": random_state,\n",
    "                \"n_jobs\": n_jobs,\n",
    "            }\n",
    "\n",
    "            model = XGBRegressor(**params)\n",
    "            pruning_callback = XGBoostPruningCallback(trial, \"validation_0-rmse\")\n",
    "            kf = KFold(n_splits=n_splits)\n",
    "            X_values = X.values\n",
    "            y_values = y.values\n",
    "            scores = []\n",
    "            for train_index, test_index in kf.split(X_values):\n",
    "                X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n",
    "                y_A, y_B = y_values[train_index], y_values[test_index]\n",
    "                model.fit(\n",
    "                    X_A,\n",
    "                    y_A,\n",
    "                    eval_set=[(X_B, y_B)],\n",
    "                    eval_metric=\"rmse\",\n",
    "                    verbose=0,\n",
    "                    callbacks=[pruning_callback],\n",
    "                    early_stopping_rounds=early_stopping_rounds,\n",
    "                )\n",
    "                y_pred = model.predict(X_B)\n",
    "                scores.append(mean_squared_error(y_pred, y_B))\n",
    "            return np.mean(scores)\n",
    "\n",
    "        sampler = TPESampler(seed=self.seed)\n",
    "        study = create_study(direction=\"minimize\", sampler=sampler)\n",
    "        study.optimize(\n",
    "            lambda trial: objective(\n",
    "                trial,\n",
    "                X_train,\n",
    "                y_train,\n",
    "                random_state=self.seed,\n",
    "                n_splits=5,\n",
    "                n_jobs=8,\n",
    "                early_stopping_rounds=100,\n",
    "            ),\n",
    "            n_trials=self.num_trials,\n",
    "            n_jobs=1,\n",
    "        )\n",
    "\n",
    "        self.model = XGBRegressor(**study.best_params)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model.fit(X_train.values, y_train)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photoswitch\n",
    "\n",
    "> Code specific for the photoswitch test case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the photoswitch datataset, we'll use a GPR on the \"fragprint\" representation using a Tanimoto kernel (as in [the original implementation](https://github.com/Ryan-Rhys/The-Photoswitch-Dataset/blob/master/property_prediction/predict_with_GPR.py))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "\n",
    "class Tanimoto(gpflow.kernels.Kernel):\n",
    "    \"\"\"Tanimoto kernel.\n",
    "\n",
    "    Taken from https://github.com/Ryan-Rhys/The-Photoswitch-Dataset/blob/master/property_prediction/kernels.py.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        :param kwargs: accepts `name` and `active_dims`, which is a list or\n",
    "            slice of indices which controls which columns of X are used (by\n",
    "            default, all columns are used).\n",
    "        \"\"\"\n",
    "        for kwarg in kwargs:\n",
    "            if kwarg not in {\"name\", \"active_dims\"}:\n",
    "                raise TypeError(\"Unknown keyword argument:\", kwarg)\n",
    "        super().__init__(**kwargs)\n",
    "        self.variance = gpflow.Parameter(1.0, transform=positive())\n",
    "\n",
    "    def K(self, X, X2=None):\n",
    "        \"\"\"\n",
    "        Compute the Tanimoto kernel matrix σ² * ((<x, y>) / (||x||^2 + ||y||^2 - <x, y>))\n",
    "        :param X: N x D array\n",
    "        :param X2: M x D array. If None, compute the N x N kernel matrix for X.\n",
    "        :return: The kernel matrix of dimension N x M\n",
    "        \"\"\"\n",
    "        if X2 is None:\n",
    "            X2 = X\n",
    "\n",
    "        Xs = tf.reduce_sum(tf.square(X), axis=-1)  # Squared L2-norm of X\n",
    "        X2s = tf.reduce_sum(tf.square(X2), axis=-1)  # Squared L2-norm of X2\n",
    "        cross_product = tf.tensordot(\n",
    "            X, X2, [[-1], [-1]]\n",
    "        )  # outer product of the matrices X and X2\n",
    "\n",
    "        # Analogue of denominator in Tanimoto formula\n",
    "\n",
    "        denominator = -cross_product + broadcasting_elementwise(tf.add, Xs, X2s)\n",
    "\n",
    "        return self.variance * cross_product / denominator\n",
    "\n",
    "    def K_diag(self, X):\n",
    "        \"\"\"\n",
    "        Compute the diagonal of the N x N kernel matrix of X\n",
    "        :param X: N x D array\n",
    "        :return: N x 1 array\n",
    "        \"\"\"\n",
    "        return tf.fill(tf.shape(X)[:-1], tf.squeeze(self.variance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def compute_morgan_fingerprints(smiles_list: Iterable[str] # list of SMILEs\n",
    ") -> np.ndarray:\n",
    "    rdkit_mols = [MolFromSmiles(smiles) for smiles in smiles_list]\n",
    "    rdkit_smiles = [MolToSmiles(mol, isomericSmiles=False) for mol in rdkit_mols]\n",
    "    rdkit_mols = [MolFromSmiles(smiles) for smiles in rdkit_smiles]\n",
    "    X = [\n",
    "        AllChem.GetMorganFingerprintAsBitVect(mol, 3, nBits=2048)\n",
    "        for mol in rdkit_mols\n",
    "    ]\n",
    "    X = np.asarray(X)\n",
    "    return X\n",
    "\n",
    "def compute_fragprints(\n",
    "    smiles_list: Iterable[str] # list of SMILEs\n",
    ") -> np.ndarray:\n",
    "    X = compute_morgan_fingerprints(smiles_list)\n",
    "\n",
    "    fragments = {d[0]: d[1] for d in Descriptors.descList[115:]}\n",
    "    X1 = np.zeros((len(smiles_list), len(fragments)))\n",
    "    for i in range(len(smiles_list)):\n",
    "        mol = MolFromSmiles(smiles_list[i])\n",
    "        try:\n",
    "            features = [fragments[d](mol) for d in fragments]\n",
    "        except:\n",
    "            raise Exception(\"molecule {}\".format(i) + \" is not canonicalised\")\n",
    "        X1[i, :] = features\n",
    "\n",
    "    X = np.concatenate((X, X1), axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_morgan_fingerprints(['C1=CC=CC=C1', 'CCC', 'CCC#N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_fragprints(['C1=CC=CC=C1', 'CCC', 'CCC#N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "class GPRBaseline(BaseLineModel):\n",
    "    \"\"\"GPR w/ Tanimoto kernel baseline.\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.model = None\n",
    "        self.y_scaler = StandardScaler()\n",
    "\n",
    "    def tune(\n",
    "        self,\n",
    "        X_train: np.ndarray, # N x D features\n",
    "        y_train: np.ndarray  # N x 1 target\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: np.ndarray, # N x D features\n",
    "        y_train: np.ndarray  # N x 1 target\n",
    "    ):\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "        def objective_closure():\n",
    "            return -m.log_marginal_likelihood()\n",
    "\n",
    "        y_train = self.y_scaler.fit_transform(y_train)\n",
    "\n",
    "        m = gpflow.models.GPR(\n",
    "            data=(X_train, y_train),\n",
    "            mean_function=Constant(np.mean(y_train)),\n",
    "            kernel=Tanimoto(),\n",
    "            noise_variance=1,\n",
    "        )\n",
    "\n",
    "        # Optimise the kernel variance and noise level by the marginal likelihood\n",
    "        opt = gpflow.optimizers.Scipy()\n",
    "        opt.minimize(\n",
    "            objective_closure, m.trainable_variables, options=dict(maxiter=10000)\n",
    "        )\n",
    "        print_summary(m)\n",
    "        self.model = m\n",
    "\n",
    "    def predict(\n",
    "            self, \n",
    "            X_test: np.ndarray # N x D features\n",
    "        ):  \n",
    "        y_pred, y_var = self.model.predict_f(X_test)\n",
    "        y_pred = self.y_scaler.inverse_transform(y_pred)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gpt3forchem.data import get_photoswitch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some data using \"fragprint\" features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_photoswitch_data()\n",
    "smiles_list = df['SMILES'].values\n",
    "y = df['E isomer pi-pi* wavelength in nm'].values\n",
    "X = compute_fragprints(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a random train/test split. In the original work they use a random, unstratified split in 80/20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmaikjablonka/miniconda3/envs/gpt3/lib/python3.9/site-packages/gpflow/experimental/utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.checker.ShapeChecker.__init__ which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n",
      "2022-09-13 13:26:43.267626: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕\n",
      "│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │\n",
      "╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡\n",
      "│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.27311 │\n",
      "├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤\n",
      "│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 39.2955  │\n",
      "├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤\n",
      "│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.02172 │\n",
      "╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "baseline = GPRBaseline()\n",
    "baseline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = baseline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8597867662966574"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems quite comparable to the results from the original paper (0.9 stated in the [README](https://github.com/Ryan-Rhys/The-Photoswitch-Dataset))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export \n",
    "\n",
    "def train_test_gpr_baseline(train_file, test_file, delete_from_prompt: str = 'what is the transition wavelength of', representation_column: str = 'SMILES'): \n",
    "    df = get_photoswitch_data()\n",
    "    train_frame = pd.read_json(train_file, orient=\"records\", lines=True)\n",
    "    test_frame = pd.read_json(test_file, orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "    repr_train = train_frame['repr']\n",
    "    repr_test = test_frame['repr']\n",
    "    y_train = np.array([df[df[representation_column]==smile]['E isomer pi-pi* wavelength in nm'].values[0] for smile in repr_train])\n",
    "    y_test = np.array([df[df[representation_column]==smile]['E isomer pi-pi* wavelength in nm'].values[0] for smile in repr_test])\n",
    "\n",
    "    if representation_column =='SMILES': \n",
    "        smiles_train = repr_train\n",
    "        smiles_test = repr_test\n",
    "    else: \n",
    "        smiles_train = np.array([df[df[representation_column]==smile]['SMILES'].values[0] for smile in repr_train])\n",
    "        smiles_test = np.array([df[df[representation_column]==smile]['SMILES'].values[0] for smile in repr_test]) \n",
    "\n",
    "    df_train = pd.DataFrame(\n",
    "        {\n",
    "            \"SMILES\": smiles_train,\n",
    "            'y': y_train\n",
    "        }\n",
    "    )\n",
    "    df_test = pd.DataFrame(\n",
    "        {\n",
    "            \"SMILES\": smiles_test,\n",
    "            'y': y_test\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df_train = df_train.drop_duplicates(subset=['SMILES'])\n",
    "    df_test = df_test.drop_duplicates(subset=['SMILES'])\n",
    "\n",
    "    X_train = compute_fragprints(df_train['SMILES'].values)\n",
    "    X_test = compute_fragprints(df_test['SMILES'].values)\n",
    "\n",
    "    baseline = GPRBaseline()\n",
    "    baseline.fit(X_train, df_train['y'].values)\n",
    "\n",
    "    predictions = baseline.predict(X_test)\n",
    "\n",
    "    _, bins = pd.cut(df['E isomer pi-pi* wavelength in nm'], 5, retbins=True)\n",
    "\n",
    "    # we clip as out-of-bound predictions result in NaNs\n",
    "    pred = np.clip(predictions.flatten(), a_min=bins[0], a_max=bins[-1])\n",
    "    predicted_bins = pd.cut(pred, bins, labels=np.arange(5), include_lowest=True)\n",
    "    true_bins = pd.cut(df_test['y'].values.flatten(), bins, labels=np.arange(5))\n",
    "\n",
    "    cm = ConfusionMatrix(true_bins.astype(int), predicted_bins.astype(int))\n",
    "\n",
    "    return {\n",
    "        'true_bins': true_bins,\n",
    "        'predicted_bins': predicted_bins,\n",
    "        'cm': cm,\n",
    "        'predictions': predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕\n",
      "│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │\n",
      "╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡\n",
      "│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  1.28793 │\n",
      "├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤\n",
      "│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 41.4062  │\n",
      "├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤\n",
      "│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.03538 │\n",
      "╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'true_bins': [2, 2, 0, 2, 1, ..., 0, 0, 0, 1, 0]\n",
       " Length: 290\n",
       " Categories (5, int64): [0 < 1 < 2 < 3 < 4],\n",
       " 'predicted_bins': [2, 2, 0, 2, 1, ..., 0, 0, 0, 2, 0]\n",
       " Length: 290\n",
       " Categories (5, int64): [0 < 1 < 2 < 3 < 4],\n",
       " 'cm': pycm.ConfusionMatrix(classes: [0, 1, 2, 3, 4]),\n",
       " 'predictions': array([[435.65814317],\n",
       "        [456.2081001 ],\n",
       "        [319.40649458],\n",
       "        [432.68606792],\n",
       "        [344.01382717],\n",
       "        [388.67247023],\n",
       "        [347.90198219],\n",
       "        [434.16644013],\n",
       "        [412.4666958 ],\n",
       "        [341.14555502],\n",
       "        [329.75477267],\n",
       "        [328.18578774],\n",
       "        [343.976142  ],\n",
       "        [446.82263129],\n",
       "        [352.72274405],\n",
       "        [354.09817687],\n",
       "        [399.00422577],\n",
       "        [326.55266368],\n",
       "        [422.23416214],\n",
       "        [500.74564931],\n",
       "        [344.13263567],\n",
       "        [345.46164537],\n",
       "        [366.18218521],\n",
       "        [398.96651022],\n",
       "        [340.37443871],\n",
       "        [394.17931939],\n",
       "        [344.2005382 ],\n",
       "        [319.8577194 ],\n",
       "        [367.92456951],\n",
       "        [326.09379036],\n",
       "        [423.70415395],\n",
       "        [349.68698636],\n",
       "        [398.08119174],\n",
       "        [310.20631167],\n",
       "        [388.23708612],\n",
       "        [423.63551875],\n",
       "        [341.89979684],\n",
       "        [331.83479219],\n",
       "        [424.87017439],\n",
       "        [406.50030338],\n",
       "        [313.45716657],\n",
       "        [336.47662703],\n",
       "        [411.15072811],\n",
       "        [348.4908666 ],\n",
       "        [334.37261777],\n",
       "        [388.5587939 ],\n",
       "        [410.16229326],\n",
       "        [340.16562572],\n",
       "        [379.61392824],\n",
       "        [387.61183599],\n",
       "        [331.4702544 ],\n",
       "        [356.37959227],\n",
       "        [347.7573889 ],\n",
       "        [340.04381881],\n",
       "        [448.8257801 ],\n",
       "        [330.93163049],\n",
       "        [346.98228405],\n",
       "        [381.60473034],\n",
       "        [449.63593975],\n",
       "        [366.46645123],\n",
       "        [452.61821666],\n",
       "        [502.75283587],\n",
       "        [345.45615659],\n",
       "        [501.67487977],\n",
       "        [415.30055336],\n",
       "        [440.62647644],\n",
       "        [398.39455695],\n",
       "        [538.89690825],\n",
       "        [337.79952704],\n",
       "        [404.85595232],\n",
       "        [351.19715115],\n",
       "        [456.16602766],\n",
       "        [315.21160821],\n",
       "        [339.90623128],\n",
       "        [365.29618808],\n",
       "        [331.44613587],\n",
       "        [479.19810921],\n",
       "        [395.92097111],\n",
       "        [407.45119526],\n",
       "        [435.57698974],\n",
       "        [509.22026329],\n",
       "        [389.37567956],\n",
       "        [385.49403058],\n",
       "        [466.52930855],\n",
       "        [316.0538552 ],\n",
       "        [382.07592625],\n",
       "        [432.62567795],\n",
       "        [440.87819116],\n",
       "        [438.77715938],\n",
       "        [334.83124486],\n",
       "        [422.68032944],\n",
       "        [398.82767307],\n",
       "        [345.23369507],\n",
       "        [330.03002572],\n",
       "        [451.41390496],\n",
       "        [331.50975949],\n",
       "        [311.72076625],\n",
       "        [452.23998037],\n",
       "        [393.7670393 ],\n",
       "        [360.78908461],\n",
       "        [335.98016215],\n",
       "        [354.30655325],\n",
       "        [420.81395161],\n",
       "        [345.36970135],\n",
       "        [347.44725304],\n",
       "        [435.65330294],\n",
       "        [387.35839488],\n",
       "        [324.08418386],\n",
       "        [323.76001118],\n",
       "        [318.62556279],\n",
       "        [361.24610913],\n",
       "        [345.27442095],\n",
       "        [520.22010575],\n",
       "        [395.6664562 ],\n",
       "        [352.35050989],\n",
       "        [441.3774046 ],\n",
       "        [405.64324725],\n",
       "        [442.10883214],\n",
       "        [411.29719426],\n",
       "        [410.50223212],\n",
       "        [455.19845556],\n",
       "        [360.54488079],\n",
       "        [525.69909803],\n",
       "        [372.71856393],\n",
       "        [326.06026175],\n",
       "        [397.08114243],\n",
       "        [428.03032815],\n",
       "        [530.01912414],\n",
       "        [509.75706564],\n",
       "        [431.29624681],\n",
       "        [445.12023425],\n",
       "        [310.33300107],\n",
       "        [393.21872327],\n",
       "        [494.05527263],\n",
       "        [427.76840626],\n",
       "        [385.16278479],\n",
       "        [398.83877168],\n",
       "        [323.75388273],\n",
       "        [324.97071797],\n",
       "        [395.8642232 ],\n",
       "        [390.55373437],\n",
       "        [400.23764935],\n",
       "        [438.35430361],\n",
       "        [452.89002496],\n",
       "        [406.87891008],\n",
       "        [410.89652359],\n",
       "        [418.12310305],\n",
       "        [339.05413387],\n",
       "        [399.25030692],\n",
       "        [340.13245364],\n",
       "        [423.22146456],\n",
       "        [387.20098709],\n",
       "        [414.73573438],\n",
       "        [416.4945307 ],\n",
       "        [368.87690167],\n",
       "        [363.27342411],\n",
       "        [341.33567995],\n",
       "        [467.68442327],\n",
       "        [433.24452966],\n",
       "        [365.60674977],\n",
       "        [466.52930855],\n",
       "        [427.18439651],\n",
       "        [343.21846258],\n",
       "        [431.19040277],\n",
       "        [336.12685376],\n",
       "        [427.80856378],\n",
       "        [316.21461123],\n",
       "        [403.48734069],\n",
       "        [335.60806251],\n",
       "        [357.11626265],\n",
       "        [330.69063799],\n",
       "        [344.75807693],\n",
       "        [335.32276438],\n",
       "        [337.60033074],\n",
       "        [411.53957574],\n",
       "        [388.8119982 ],\n",
       "        [537.33036056],\n",
       "        [457.38260601],\n",
       "        [346.91875913],\n",
       "        [322.28820749],\n",
       "        [485.16748302],\n",
       "        [539.91647371],\n",
       "        [430.0326076 ],\n",
       "        [441.96406665],\n",
       "        [330.59343715],\n",
       "        [321.57512675],\n",
       "        [309.73926604],\n",
       "        [391.4370721 ],\n",
       "        [410.04846904],\n",
       "        [417.30549213],\n",
       "        [484.02305241],\n",
       "        [353.85027211],\n",
       "        [404.80046347],\n",
       "        [533.14616521],\n",
       "        [351.96487794],\n",
       "        [553.35479523],\n",
       "        [426.88057474],\n",
       "        [338.96795126],\n",
       "        [418.94815417],\n",
       "        [402.77427326],\n",
       "        [396.09748528],\n",
       "        [336.2896457 ],\n",
       "        [318.30323336],\n",
       "        [339.80740425],\n",
       "        [336.03323014],\n",
       "        [451.14501725],\n",
       "        [479.57621834],\n",
       "        [321.9696028 ],\n",
       "        [483.06437143],\n",
       "        [415.66878965],\n",
       "        [492.1042819 ],\n",
       "        [409.62963671],\n",
       "        [425.08511983],\n",
       "        [446.47904576],\n",
       "        [516.9670321 ],\n",
       "        [420.37688798],\n",
       "        [322.27577044],\n",
       "        [388.35932236],\n",
       "        [387.35839488],\n",
       "        [388.96686115],\n",
       "        [407.66683564],\n",
       "        [407.33424528],\n",
       "        [452.61821666],\n",
       "        [401.67093347],\n",
       "        [440.87644917],\n",
       "        [462.18001458],\n",
       "        [409.87738876],\n",
       "        [326.21813826],\n",
       "        [343.42617425],\n",
       "        [332.42057879],\n",
       "        [324.18839033],\n",
       "        [412.78855439],\n",
       "        [348.58176805],\n",
       "        [362.34559753],\n",
       "        [433.05805354],\n",
       "        [418.56129712],\n",
       "        [406.74486918],\n",
       "        [402.77427326],\n",
       "        [323.28846257],\n",
       "        [529.28249189],\n",
       "        [347.10702631],\n",
       "        [530.37539469],\n",
       "        [446.98495996],\n",
       "        [450.62705561],\n",
       "        [401.36514164],\n",
       "        [391.10668453],\n",
       "        [317.33622284],\n",
       "        [370.65484953],\n",
       "        [344.78427907],\n",
       "        [441.36070956],\n",
       "        [350.67389974],\n",
       "        [416.76102335],\n",
       "        [393.43278826],\n",
       "        [481.44667298],\n",
       "        [313.45328148],\n",
       "        [332.72563823],\n",
       "        [364.59903108],\n",
       "        [352.14663141],\n",
       "        [391.83384733],\n",
       "        [370.63734771],\n",
       "        [409.28229797],\n",
       "        [431.45480687],\n",
       "        [340.86662878],\n",
       "        [329.54243731],\n",
       "        [380.89672718],\n",
       "        [404.6885056 ],\n",
       "        [345.42746717],\n",
       "        [366.05691296],\n",
       "        [350.38083494],\n",
       "        [426.34433321],\n",
       "        [392.69272381],\n",
       "        [320.92395728],\n",
       "        [422.88596151],\n",
       "        [387.76346948],\n",
       "        [361.35027595],\n",
       "        [387.27901782],\n",
       "        [407.32755291],\n",
       "        [313.67583554],\n",
       "        [305.12421184],\n",
       "        [440.41153601],\n",
       "        [425.77604836],\n",
       "        [324.49618128],\n",
       "        [314.5306413 ],\n",
       "        [397.64036766],\n",
       "        [421.56431358],\n",
       "        [320.62655393],\n",
       "        [334.80804888],\n",
       "        [316.87611038],\n",
       "        [409.42476822],\n",
       "        [316.51882537]])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_gpr_baseline('../experiments/run_files/2022-09-05-20-10-07_train_prompts_photoswitch_100_SMILES.jsonl', '../experiments/run_files/2022-09-05-20-10-07_valid_prompts_photoswitch_290_SMILES.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gpt3')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
