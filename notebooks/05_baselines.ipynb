{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "from nbdev.showdoc import *\n",
    "from optuna import create_study\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from wandb.xgboost import WandbCallback\n",
    "from xgboost import XGBClassifier, XGBRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "\n",
    "> Code for baseline models that are reused across case studies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "class BaseLineModel(ABC):\n",
    "    @abstractmethod\n",
    "    def tune(self, X_train, y_train):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X_train, y_train):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "class XGBClassificationBaseline(BaseLineModel):\n",
    "    def __init__(self, seed, num_trials=100) -> None:\n",
    "        self.seed = seed\n",
    "        self.num_trials = num_trials\n",
    "        self.model = XGBClassifier()\n",
    "\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def tune(self, X_train, y_train):\n",
    "        y_train = self.label_encoder.fit_transform(y_train)\n",
    "\n",
    "        def objective(\n",
    "            trial,\n",
    "            X,\n",
    "            y,\n",
    "            random_state=22,\n",
    "            n_splits=3,\n",
    "            n_jobs=1,\n",
    "            early_stopping_rounds=100,\n",
    "        ):\n",
    "            # XGBoost parameters\n",
    "            params = {\n",
    "                \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 4, 10_000),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 4, 100),\n",
    "                \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.05),\n",
    "                \"colsample_bytree\": trial.suggest_loguniform(\n",
    "                    \"colsample_bytree\", 0.2, 1\n",
    "                ),\n",
    "                \"subsample\": trial.suggest_loguniform(\"subsample\", 0.00001, 1),\n",
    "                \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-8, 10.0),\n",
    "                \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 10.0),\n",
    "                \"seed\": random_state,\n",
    "                \"n_jobs\": n_jobs,\n",
    "            }\n",
    "\n",
    "            model = XGBClassifier(**params)\n",
    "            pruning_callback = XGBoostPruningCallback(trial, \"validation_0-mlogloss\")\n",
    "            kf = KFold(n_splits=n_splits)\n",
    "            X_values = X.values\n",
    "            y_values = y\n",
    "\n",
    "            scores = []\n",
    "            for train_index, test_index in kf.split(X_values):\n",
    "                X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n",
    "                y_A, y_B = y_values[train_index], y_values[test_index]\n",
    "\n",
    "                model.fit(\n",
    "                    X_A,\n",
    "                    y_A,\n",
    "                    eval_set=[(X_B, y_B)],\n",
    "                    eval_metric=\"mlogloss\",\n",
    "                    verbose=0,\n",
    "                    callbacks=[pruning_callback],\n",
    "                    early_stopping_rounds=early_stopping_rounds,\n",
    "                )\n",
    "                y_pred = model.predict(X_B)\n",
    "                scores.append(f1_score(y_pred, y_B, average=\"macro\"))\n",
    "            return np.mean(scores)\n",
    "\n",
    "        sampler = TPESampler(seed=self.seed)\n",
    "        study = create_study(direction=\"maximize\", sampler=sampler)\n",
    "        study.optimize(\n",
    "            lambda trial: objective(\n",
    "                trial,\n",
    "                X_train,\n",
    "                y_train,\n",
    "                random_state=self.seed,\n",
    "                n_splits=5,\n",
    "                n_jobs=-1,\n",
    "                early_stopping_rounds=100,\n",
    "            ),\n",
    "            n_trials=self.num_trials,\n",
    "            n_jobs=1,\n",
    "        )\n",
    "\n",
    "        self.model = XGBClassifier(**study.best_params, callbacks=[WandbCallback()])\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        y_train = self.label_encoder.fit_transform(y_train)\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.label_encoder.inverse_transform(self.model.predict(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseLineModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/notebooks/04_baselines.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/notebooks/04_baselines.ipynb#ch0000005?line=0'>1</a>\u001b[0m \u001b[39m# |export\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/notebooks/04_baselines.ipynb#ch0000005?line=1'>2</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mXGBRegressionBaseline\u001b[39;00m(BaseLineModel):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/notebooks/04_baselines.ipynb#ch0000005?line=2'>3</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, seed, num_trials\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/gpt3forchem/notebooks/04_baselines.ipynb#ch0000005?line=3'>4</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed \u001b[39m=\u001b[39m seed\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseLineModel' is not defined"
     ]
    }
   ],
   "source": [
    "# |export\n",
    "class XGBRegressionBaseline(BaseLineModel):\n",
    "    def __init__(self, seed, num_trials=100) -> None:\n",
    "        self.seed = seed\n",
    "        self.num_trials = num_trials\n",
    "        self.model = XGBRegressor()\n",
    "\n",
    "    def tune(self, X_train, y_train):\n",
    "        def objective(\n",
    "            trial,\n",
    "            X,\n",
    "            y,\n",
    "            random_state=22,\n",
    "            n_splits=3,\n",
    "            n_jobs=1,\n",
    "            early_stopping_rounds=50,\n",
    "        ):\n",
    "            # XGBoost parameters\n",
    "            params = {\n",
    "                \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n",
    "                \"objective\": \"reg:squarederror\",\n",
    "                \"n_estimators\": 10000,\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12),\n",
    "                \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.005, 0.05),\n",
    "                \"colsample_bytree\": trial.suggest_loguniform(\n",
    "                    \"colsample_bytree\", 0.2, 0.6\n",
    "                ),\n",
    "                \"subsample\": trial.suggest_loguniform(\"subsample\", 0.4, 0.8),\n",
    "                \"alpha\": trial.suggest_loguniform(\"alpha\", 0.01, 10.0),\n",
    "                \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 10.0),\n",
    "                \"gamma\": trial.suggest_loguniform(\"lambda\", 1e-8, 10.0),\n",
    "                \"min_child_weight\": trial.suggest_loguniform(\n",
    "                    \"min_child_weight\", 10, 1000\n",
    "                ),\n",
    "                \"seed\": random_state,\n",
    "                \"n_jobs\": n_jobs,\n",
    "            }\n",
    "\n",
    "            model = XGBRegressor(**params)\n",
    "            pruning_callback = XGBoostPruningCallback(trial, \"validation_0-rmse\")\n",
    "            kf = KFold(n_splits=n_splits)\n",
    "            X_values = X.values\n",
    "            y_values = y.values\n",
    "            scores = []\n",
    "            for train_index, test_index in kf.split(X_values):\n",
    "                X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n",
    "                y_A, y_B = y_values[train_index], y_values[test_index]\n",
    "                model.fit(\n",
    "                    X_A,\n",
    "                    y_A,\n",
    "                    eval_set=[(X_B, y_B)],\n",
    "                    eval_metric=\"rmse\",\n",
    "                    verbose=0,\n",
    "                    callbacks=[pruning_callback],\n",
    "                    early_stopping_rounds=early_stopping_rounds,\n",
    "                )\n",
    "                y_pred = model.predict(X_B)\n",
    "                scores.append(mean_squared_error(y_pred, y_B))\n",
    "            return np.mean(scores)\n",
    "\n",
    "        sampler = TPESampler(seed=self.seed)\n",
    "        study = create_study(direction=\"minimize\", sampler=sampler)\n",
    "        study.optimize(\n",
    "            lambda trial: objective(\n",
    "                trial,\n",
    "                X_train,\n",
    "                y_train,\n",
    "                random_state=self.seed,\n",
    "                n_splits=5,\n",
    "                n_jobs=8,\n",
    "                early_stopping_rounds=100,\n",
    "            ),\n",
    "            n_trials=self.num_trials,\n",
    "            n_jobs=1,\n",
    "        )\n",
    "\n",
    "        self.model = XGBRegressor(**study.best_params)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polymers\n",
    "\n",
    "> Code specific for the polymer test case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpt3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a4fa60962de90e73b5da8d67a44b01d2de04630d82b94b8db1f727a73d31e61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
